{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5c4d70",
   "metadata": {},
   "source": [
    "# Session 7: Logistic Regression & Softmax\n",
    "## From Regression to Classification\n",
    "\n",
    "**Course: Neural Networks for Engineers**  \n",
    "**Duration: 2 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Recap: What We Know So Far](#recap)\n",
    "2. [The Classification Problem](#classification)\n",
    "3. [Sigmoid as Probability: Logistic Regression](#logistic)\n",
    "4. [Why MSE Fails for Classification](#mse-fails)\n",
    "5. [Binary Cross-Entropy Loss](#bce)\n",
    "6. [Implementing Logistic Regression from Scratch](#impl-logistic)\n",
    "7. [Multi-Class Classification: Softmax](#softmax)\n",
    "8. [Categorical Cross-Entropy Loss](#cce)\n",
    "9. [Complete Classification Pipeline](#pipeline)\n",
    "10. [Evaluation Metrics](#metrics)\n",
    "11. [Final Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap: What We Know So Far {#recap}\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "‚úÖ **Loss functions**: MSE measures prediction error  \n",
    "‚úÖ **Gradient descent**: $w \\leftarrow w - \\eta \\nabla L$  \n",
    "‚úÖ **Backpropagation**: Chain rule applied layer by layer  \n",
    "‚úÖ **MLP training**: Automatic weight learning on XOR  \n",
    "‚úÖ **Open question**: MSE + sigmoid for classification ‚Äî is this the best we can do?\n",
    "\n",
    "### ü§î Quick Questions (from Session 6's \"Think About\")\n",
    "\n",
    "**Q1:** Our MLP uses MSE loss for XOR classification. What might be wrong with MSE for classification?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "MSE treats all errors equally ‚Äî an error of 0.1 is penalized the same whether the output is 0.5 or 0.99. But for classification, a confident **wrong** prediction (output 0.99 when target is 0) should be penalized much more harshly than an uncertain one (output 0.5). We need a loss function that understands **probabilities**.\n",
    "</details>\n",
    "\n",
    "**Q2:** If the network outputs 0.99 for a class-0 sample, how should the loss behave?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The loss should be **very large** ‚Äî almost infinite! The network is extremely confident and extremely wrong. MSE gives $(0 - 0.99)^2 = 0.98$, which is just... a number. Cross-entropy gives $-\\log(1 - 0.99) = -\\log(0.01) \\approx 4.6$, which is much steeper. This is what we need.\n",
    "</details>\n",
    "\n",
    "**Q3:** What if we have 5 possible classes instead of 2? How should the output layer look?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "We need **5 output neurons**, one per class. Their outputs should be **probabilities that sum to 1**. This is exactly what the **softmax** function does!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Classification Problem {#classification}\n",
    "\n",
    "### Regression vs Classification\n",
    "\n",
    "So far, we've seen two flavors of prediction:\n",
    "\n",
    "| | Regression | Classification |\n",
    "|---|---|---|\n",
    "| **Output** | Continuous number | Discrete category |\n",
    "| **Examples** | Price, temperature, age | Cat/dog, spam/not, digit 0-9 |\n",
    "| **Output range** | Any real number | Probability per class |\n",
    "| **Loss** | MSE | Cross-entropy (today!) |\n",
    "\n",
    "### Types of Classification\n",
    "\n",
    "| Type | Classes | Output | Example |\n",
    "|---|---|---|---|\n",
    "| **Binary** | 2 | 1 probability | Spam or not spam |\n",
    "| **Multi-class** | $K > 2$ | $K$ probabilities | Digit recognition (0-9) |\n",
    "\n",
    "### What We Need\n",
    "\n",
    "A classification model should output **probabilities**:\n",
    "- Output between 0 and 1\n",
    "- For multi-class: outputs sum to 1\n",
    "- High probability = high confidence\n",
    "\n",
    "The **sigmoid** function (binary) and **softmax** function (multi-class) do exactly this!\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sigmoid as Probability: Logistic Regression {#logistic}\n",
    "\n",
    "### The Model\n",
    "\n",
    "Logistic regression = linear model + sigmoid activation.\n",
    "\n",
    "$$\n",
    "P(y = 1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
    "$$\n",
    "\n",
    "This is just a **single neuron** with sigmoid activation ‚Äî the simplest possible classifier!\n",
    "\n",
    "```\n",
    "Input           Neuron            Output\n",
    "\n",
    " x‚ÇÅ ‚îÄ‚îÄ‚îÄ w‚ÇÅ ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ\n",
    " x‚ÇÇ ‚îÄ‚îÄ‚îÄ w‚ÇÇ ‚îÄ‚îÄ‚îº‚îÄ‚îÄ Œ£ + b ‚îÄ‚îÄ œÉ ‚îÄ‚îÄ P(y=1)\n",
    "              ‚îÇ\n",
    " x‚ÇÉ ‚îÄ‚îÄ‚îÄ w‚ÇÉ ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Why Sigmoid Outputs Are Probabilities\n",
    "\n",
    "The sigmoid function has exactly the right properties:\n",
    "\n",
    "| Property | Sigmoid satisfies? |\n",
    "|---|---|\n",
    "| Output in $[0, 1]$ | ‚úÖ $\\sigma(z) \\in (0, 1)$ for all $z$ |\n",
    "| Output 0.5 when uncertain | ‚úÖ $\\sigma(0) = 0.5$ |\n",
    "| Approaches 1 for large positive $z$ | ‚úÖ Confident class 1 |\n",
    "| Approaches 0 for large negative $z$ | ‚úÖ Confident class 0 |\n",
    "\n",
    "### The Decision Threshold\n",
    "\n",
    "To make a hard prediction, we choose a **threshold** (usually 0.5):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases} 1 & \\text{if } \\sigma(\\mathbf{w}^T\\mathbf{x} + b) \\geq 0.5 \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "### ü§î Think About It\n",
    "\n",
    "**Q:** When is the sigmoid output exactly 0.5?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "When $\\mathbf{w}^T\\mathbf{x} + b = 0$. This equation defines the **decision boundary** ‚Äî the hyperplane that separates the two classes. Points on one side have $P(y=1) > 0.5$, points on the other have $P(y=1) < 0.5$.\n",
    "\n",
    "This is exactly the same linear boundary we saw with the perceptron in Session 2!\n",
    "</details>\n",
    "\n",
    "### üíª Code It: Sigmoid as Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "# Visualize sigmoid as probability\n",
    "z = np.linspace(-6, 6, 200)\n",
    "p = sigmoid(z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(z, p, 'b-', linewidth=3)\n",
    "\n",
    "# Annotate regions\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.fill_between(z, p, 0.5, where=(p >= 0.5), alpha=0.15, color='red', label='Predict class 1')\n",
    "ax.fill_between(z, p, 0.5, where=(p < 0.5), alpha=0.15, color='blue', label='Predict class 0')\n",
    "\n",
    "ax.annotate('Confident: class 0', xy=(-4, 0.05), fontsize=12, color='blue')\n",
    "ax.annotate('Uncertain', xy=(-0.8, 0.55), fontsize=12, color='gray')\n",
    "ax.annotate('Confident: class 1', xy=(2.5, 0.95), fontsize=12, color='red')\n",
    "\n",
    "ax.set_xlabel('$z = w^T x + b$', fontsize=14)\n",
    "ax.set_ylabel('$P(y = 1)$', fontsize=14)\n",
    "ax.set_title('Sigmoid Output as Probability', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c6e11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Why MSE Fails for Classification {#mse-fails}\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Let's compare MSE and cross-entropy (which we'll define next) when the true label is $y = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cabb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True label: y = 1\n",
    "y_hat = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "# MSE loss\n",
    "mse = (1 - y_hat) ** 2\n",
    "\n",
    "# Cross-entropy loss (preview!)\n",
    "bce = -np.log(y_hat)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MSE\n",
    "ax = axes[0]\n",
    "ax.plot(y_hat, mse, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Predicted $\\\\hat{y}$', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "ax.set_title('MSE Loss (target = 1)', fontsize=16)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.annotate('Gradient is small here!\\n(learning is slow)', \n",
    "            xy=(0.05, 0.85), fontsize=11, color='red',\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            xytext=(0.3, 0.6))\n",
    "\n",
    "# Cross-entropy\n",
    "ax = axes[1]\n",
    "ax.plot(y_hat, bce, 'r-', linewidth=2)\n",
    "ax.set_xlabel('Predicted $\\\\hat{y}$', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "ax.set_title('Cross-Entropy Loss (target = 1)', fontsize=16)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 7)\n",
    "ax.annotate('Gradient is huge!\\n(learning is fast)', \n",
    "            xy=(0.05, 5.5), fontsize=11, color='red',\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            xytext=(0.3, 4.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92ce98",
   "metadata": {},
   "source": [
    "### Two Problems with MSE for Classification\n",
    "\n",
    "**Problem 1: Slow gradients when confidently wrong**\n",
    "\n",
    "When the output is near 0 or 1, the sigmoid is **saturated** ‚Äî its derivative is almost zero. Combined with MSE, the gradient becomes tiny:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = 2(\\hat{y} - y) \\cdot \\underbrace{\\sigma'(z)}_{\\approx 0 \\text{ when saturated}}\n",
    "$$\n",
    "\n",
    "The network is very wrong but can barely learn! This is called the **slow learning problem**.\n",
    "\n",
    "**Problem 2: Non-convex loss surface**\n",
    "\n",
    "MSE + sigmoid creates a loss surface with **flat regions**, making optimization harder. Cross-entropy + sigmoid creates a much nicer (convex) surface.\n",
    "\n",
    "### The Solution: Cross-Entropy\n",
    "\n",
    "Cross-entropy loss **cancels out** the sigmoid saturation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "No sigmoid derivative! The gradient is simply the **prediction error**. The more wrong the prediction, the larger the gradient ‚Äî exactly what we want.\n",
    "\n",
    "### ‚úèÔ∏è Exercise 4.1: Gradient Comparison\n",
    "\n",
    "For $y = 1$ and $\\hat{y} = \\sigma(z) = 0.01$ (very wrong prediction, $z \\approx -4.6$):\n",
    "\n",
    "**MSE gradient w.r.t. $z$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = 2(\\hat{y} - y) \\cdot \\sigma'(z) = 2(0.01 - 1) \\cdot 0.01 \\cdot 0.99 = \\text{___}\n",
    "$$\n",
    "\n",
    "**Cross-entropy gradient w.r.t. $z$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\hat{y} - y = 0.01 - 1 = \\text{___}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**MSE:** $2 \\times (-0.99) \\times 0.0099 = -0.0196$ (tiny gradient!)\n",
    "\n",
    "**Cross-entropy:** $0.01 - 1 = -0.99$ (large gradient!)\n",
    "\n",
    "The cross-entropy gradient is **50√ó larger**. This means the network learns much faster from confident wrong predictions.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Binary Cross-Entropy Loss {#bce}\n",
    "\n",
    "### Definition\n",
    "\n",
    "For a single sample with true label $y \\in \\{0, 1\\}$ and predicted probability $\\hat{y} \\in (0, 1)$:\n",
    "\n",
    "$$\n",
    "L_{\\text{BCE}} = -\\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "For a batch of $N$ samples:\n",
    "\n",
    "$$\n",
    "L_{\\text{BCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "### Understanding the Formula\n",
    "\n",
    "The formula has **two cases** (only one is active per sample):\n",
    "\n",
    "| True label $y$ | Active term | Loss | Behavior |\n",
    "|---|---|---|---|\n",
    "| $y = 1$ | $-\\log(\\hat{y})$ | Low when $\\hat{y} \\to 1$ | Penalizes low confidence for class 1 |\n",
    "| $y = 0$ | $-\\log(1 - \\hat{y})$ | Low when $\\hat{y} \\to 0$ | Penalizes high confidence for class 0 |\n",
    "\n",
    "### Why Logarithm?\n",
    "\n",
    "The logarithm provides the right penalty shape:\n",
    "\n",
    "| $\\hat{y}$ (when $y = 1$) | $-\\log(\\hat{y})$ | Interpretation |\n",
    "|---|---|---|\n",
    "| 0.99 | 0.01 | Almost right ‚Üí tiny loss |\n",
    "| 0.9 | 0.11 | Pretty right ‚Üí small loss |\n",
    "| 0.5 | 0.69 | Uncertain ‚Üí moderate loss |\n",
    "| 0.1 | 2.30 | Pretty wrong ‚Üí large loss |\n",
    "| 0.01 | 4.61 | Very wrong ‚Üí **huge** loss |\n",
    "| 0.001 | 6.91 | Extremely wrong ‚Üí **massive** loss |\n",
    "\n",
    "The log penalty grows **without bound** as the prediction approaches the wrong answer ‚Äî exactly the steep penalty we wanted!\n",
    "\n",
    "### ‚úèÔ∏è Exercise 5.1: Compute BCE by Hand\n",
    "\n",
    "Compute the binary cross-entropy loss for these predictions:\n",
    "\n",
    "| Sample | $y$ | $\\hat{y}$ | $-y\\log(\\hat{y})$ | $-(1-y)\\log(1-\\hat{y})$ | Sample Loss |\n",
    "|--------|-----|-----------|-------------------|------------------------|-------------|\n",
    "| 1      | 1   | 0.9       | ___               | ___                    | ___         |\n",
    "| 2      | 0   | 0.2       | ___               | ___                    | ___         |\n",
    "| 3      | 1   | 0.3       | ___               | ___                    | ___         |\n",
    "| 4      | 0   | 0.8       | ___               | ___                    | ___         |\n",
    "\n",
    "**Total BCE =** ___\n",
    "\n",
    "Which sample contributes the **most** loss? Why?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Sample | $y$ | $\\hat{y}$ | $-y\\log(\\hat{y})$ | $-(1-y)\\log(1-\\hat{y})$ | Sample Loss |\n",
    "|--------|-----|-----------|-------------------|------------------------|-------------|\n",
    "| 1      | 1   | 0.9       | $-\\log(0.9) = 0.105$ | 0 | 0.105 |\n",
    "| 2      | 0   | 0.2       | 0 | $-\\log(0.8) = 0.223$ | 0.223 |\n",
    "| 3      | 1   | 0.3       | $-\\log(0.3) = 1.204$ | 0 | 1.204 |\n",
    "| 4      | 0   | 0.8       | 0 | $-\\log(0.2) = 1.609$ | 1.609 |\n",
    "\n",
    "$$\n",
    "\\text{BCE} = \\frac{0.105 + 0.223 + 1.204 + 1.609}{4} = \\frac{3.141}{4} = 0.785\n",
    "$$\n",
    "\n",
    "**Sample 4** contributes the most loss ($1.609$) ‚Äî it predicts 0.8 probability for class 1, but the true label is class 0. It's confidently wrong!\n",
    "</details>\n",
    "\n",
    "### The Gradient: Beautifully Simple\n",
    "\n",
    "For logistic regression ($\\hat{y} = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)$) with BCE loss:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "**This is the same form as linear regression with MSE!** The only difference is that $\\hat{y}$ is now passed through sigmoid.\n",
    "\n",
    "### üíª Code It: Compare MSE vs BCE Loss Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D logistic regression: P(y=1) = sigmoid(w * x + b)\n",
    "# Let's fix b = 0 and vary w to see the loss surface\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y = np.array([0, 0, 0, 1, 1], dtype=float)\n",
    "\n",
    "w_range = np.linspace(-3, 3, 200)\n",
    "\n",
    "mse_losses = []\n",
    "bce_losses = []\n",
    "\n",
    "for w in w_range:\n",
    "    y_hat = sigmoid(w * X)\n",
    "    y_hat_clipped = np.clip(y_hat, 1e-7, 1 - 1e-7)  # Prevent log(0)\n",
    "    \n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    bce = -np.mean(y * np.log(y_hat_clipped) + (1 - y) * np.log(1 - y_hat_clipped))\n",
    "    \n",
    "    mse_losses.append(mse)\n",
    "    bce_losses.append(bce)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].plot(w_range, mse_losses, 'b-', linewidth=2)\n",
    "axes[0].set_title('MSE Loss Surface', fontsize=16)\n",
    "axes[0].set_xlabel('Weight $w$', fontsize=14)\n",
    "axes[0].set_ylabel('Loss', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(w_range, bce_losses, 'r-', linewidth=2)\n",
    "axes[1].set_title('BCE Loss Surface', fontsize=16)\n",
    "axes[1].set_xlabel('Weight $w$', fontsize=14)\n",
    "axes[1].set_ylabel('Loss', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MSE vs Binary Cross-Entropy for Classification', fontsize=18, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7ae76",
   "metadata": {},
   "source": [
    "### ü§î Think About It\n",
    "\n",
    "**Q:** Look at the MSE loss surface. Can you see the flat regions? What would happen to gradient descent in those regions?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "The MSE surface has **plateaus** where the sigmoid is saturated. Gradient descent would slow down dramatically in these regions, even though the weights are far from optimal. The BCE surface is much smoother and steeper everywhere ‚Äî gradient descent converges faster and more reliably.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Implementing Logistic Regression from Scratch {#impl-logistic}\n",
    "\n",
    "### The Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Binary classifier: P(y=1|x) = sigmoid(Wx + b)\n",
    "    Trained with binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features):\n",
    "        self.W = np.zeros((1, n_features))\n",
    "        self.b = np.zeros((1, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X shape: (n_features, N)\n",
    "        Returns probabilities shape: (1, N)\n",
    "        \"\"\"\n",
    "        self.z = self.W @ X + self.b\n",
    "        self.y_hat = sigmoid(self.z)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def compute_loss(self, y_true):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        N = y_true.shape[1]\n",
    "        y_hat_clipped = np.clip(self.y_hat, 1e-7, 1 - 1e-7)\n",
    "        loss = -np.mean(\n",
    "            y_true * np.log(y_hat_clipped) + \n",
    "            (1 - y_true) * np.log(1 - y_hat_clipped)\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, lr):\n",
    "        \"\"\"Gradient descent update\"\"\"\n",
    "        N = X.shape[1]\n",
    "        \n",
    "        # Gradient (the beautiful simple form!)\n",
    "        error = self.y_hat - y_true                # (1, N)\n",
    "        dW = (1 / N) * (error @ X.T)              # (1, n_features)\n",
    "        db = (1 / N) * np.sum(error, keepdims=True)  # (1, 1)\n",
    "        \n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Hard predictions\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return (probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58499b",
   "metadata": {},
   "source": [
    "### üíª Code It: Train on a 2D Dataset\n",
    "\n",
    "**Fill in the training loop:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_data(n_samples=200):\n",
    "    \"\"\"Two Gaussian clouds\"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Class 0: centered at (-1, -1)\n",
    "    X0 = np.random.randn(2, n_samples // 2) * 0.8 + np.array([[-1], [-1]])\n",
    "    # Class 1: centered at (1, 1)\n",
    "    X1 = np.random.randn(2, n_samples // 2) * 0.8 + np.array([[1], [1]])\n",
    "    \n",
    "    X = np.hstack([X0, X1])\n",
    "    y = np.hstack([np.zeros((1, n_samples // 2)), np.ones((1, n_samples // 2))])\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    return X[:, idx], y[:, idx]\n",
    "\n",
    "X_train, y_train = generate_binary_data()\n",
    "\n",
    "# Train\n",
    "model = LogisticRegression(n_features=2)\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(___):  # How many epochs?\n",
    "    # TODO: Forward pass\n",
    "    ___\n",
    "    \n",
    "    # TODO: Compute and record loss\n",
    "    ___\n",
    "    \n",
    "    # TODO: Backward pass\n",
    "    ___\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        acc = np.mean(model.predict(X_train) == y_train) * 100\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss:.4f}, Acc = {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be9b5e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac662c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_features=2)\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.forward(X_train)\n",
    "    loss = model.compute_loss(y_train)\n",
    "    loss_history.append(loss)\n",
    "    model.backward(X_train, y_train, lr=0.5)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        acc = np.mean(model.predict(X_train) == y_train) * 100\n",
    "        print(f\"Epoch {epoch:4d}: Loss = {loss:.4f}, Acc = {acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f44e8",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### üíª Code It: Visualize Decision Boundary and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_result(model, X, y):\n",
    "    \"\"\"Visualize the decision boundary and probability field\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Create grid\n",
    "    x_min, x_max = X[0].min() - 1, X[0].max() + 1\n",
    "    y_min, y_max = X[1].min() - 1, X[1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                          np.linspace(y_min, y_max, 200))\n",
    "    grid = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    probs = model.forward(grid).reshape(xx.shape)\n",
    "    \n",
    "    # Plot 1: Decision boundary\n",
    "    ax = axes[0]\n",
    "    ax.contourf(xx, yy, probs, levels=[0, 0.5, 1],\n",
    "                colors=['#ADD8E6', '#FFCCCB'], alpha=0.4)\n",
    "    ax.contour(xx, yy, probs, levels=[0.5], colors='black', linewidths=2)\n",
    "    ax.scatter(X[0, y[0] == 0], X[1, y[0] == 0], c='blue', alpha=0.6, \n",
    "               edgecolors='black', label='Class 0')\n",
    "    ax.scatter(X[0, y[0] == 1], X[1, y[0] == 1], c='red', alpha=0.6, \n",
    "               edgecolors='black', label='Class 1')\n",
    "    ax.set_xlabel('$x_1$', fontsize=14)\n",
    "    ax.set_ylabel('$x_2$', fontsize=14)\n",
    "    ax.set_title('Decision Boundary', fontsize=16)\n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "    # Plot 2: Probability heatmap\n",
    "    ax = axes[1]\n",
    "    contour = ax.contourf(xx, yy, probs, levels=20, cmap='RdYlBu_r')\n",
    "    plt.colorbar(contour, ax=ax, label='$P(y=1)$')\n",
    "    ax.contour(xx, yy, probs, levels=[0.5], colors='black', linewidths=2)\n",
    "    ax.scatter(X[0, y[0] == 0], X[1, y[0] == 0], c='blue', alpha=0.6, \n",
    "               edgecolors='black', s=20)\n",
    "    ax.scatter(X[0, y[0] == 1], X[1, y[0] == 1], c='red', alpha=0.6, \n",
    "               edgecolors='black', s=20)\n",
    "    ax.set_xlabel('$x_1$', fontsize=14)\n",
    "    ax.set_ylabel('$x_2$', fontsize=14)\n",
    "    ax.set_title('Predicted Probabilities', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_logistic_result(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b20163",
   "metadata": {},
   "source": [
    "### The Decision Threshold\n",
    "\n",
    "We used 0.5, but the threshold is a **tunable parameter**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_experiment(model, X, y):\n",
    "    \"\"\"Show effect of different thresholds\"\"\"\n",
    "    thresholds = [0.3, 0.5, 0.7]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Create grid\n",
    "    x_min, x_max = X[0].min() - 1, X[0].max() + 1\n",
    "    y_min, y_max = X[1].min() - 1, X[1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                          np.linspace(y_min, y_max, 200))\n",
    "    grid = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    probs = model.forward(grid).reshape(xx.shape)\n",
    "    \n",
    "    for ax, t in zip(axes, thresholds):\n",
    "        ax.contourf(xx, yy, probs, levels=[0, t, 1],\n",
    "                    colors=['#ADD8E6', '#FFCCCB'], alpha=0.4)\n",
    "        ax.contour(xx, yy, probs, levels=[t], colors='black', linewidths=2)\n",
    "        ax.scatter(X[0, y[0] == 0], X[1, y[0] == 0], c='blue', alpha=0.6, \n",
    "                   edgecolors='black', s=30)\n",
    "        ax.scatter(X[0, y[0] == 1], X[1, y[0] == 1], c='red', alpha=0.6, \n",
    "                   edgecolors='black', s=30)\n",
    "        \n",
    "        preds = (model.forward(X) >= t).astype(int)\n",
    "        acc = np.mean(preds == y) * 100\n",
    "        ax.set_title(f'Threshold = {t} (Acc: {acc:.1f}%)', fontsize=14)\n",
    "        ax.set_xlabel('$x_1$', fontsize=12)\n",
    "        ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Effect of Decision Threshold', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "threshold_experiment(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223aafb",
   "metadata": {},
   "source": [
    "### ü§î Think About It\n",
    "\n",
    "**Q:** When would you use a threshold other than 0.5?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "When the **cost of mistakes is asymmetric**:\n",
    "\n",
    "- **Medical diagnosis** (cancer detection): Use a low threshold (e.g., 0.3). It's better to have false positives (unnecessary tests) than false negatives (missed cancer).\n",
    "- **Spam filtering**: Use a higher threshold (e.g., 0.7). It's worse to lose a real email (false positive) than to let through some spam (false negative).\n",
    "\n",
    "The threshold lets you trade off between **precision** and **recall** (more on this in Section 10).\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Multi-Class Classification: Softmax {#softmax}\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Binary classification: 1 output neuron + sigmoid ‚Üí $P(y = 1)$.\n",
    "\n",
    "But what about $K$ classes (e.g., digit recognition with 10 classes)?\n",
    "\n",
    "We need:\n",
    "- $K$ output neurons\n",
    "- All outputs between 0 and 1\n",
    "- All outputs **sum to 1** (they're a probability distribution!)\n",
    "\n",
    "### The Softmax Function\n",
    "\n",
    "Given a vector of raw scores (logits) $\\mathbf{z} = [z_1, z_2, \\ldots, z_K]$:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "**In words:** Exponentiate each score, then normalize by the total.\n",
    "\n",
    "### Step-by-Step Example\n",
    "\n",
    "Suppose a 3-class network outputs logits $\\mathbf{z} = [2.0, 1.0, 0.5]$:\n",
    "\n",
    "| Step | $z_1 = 2.0$ | $z_2 = 1.0$ | $z_3 = 0.5$ |\n",
    "|------|------------|------------|------------|\n",
    "| 1. Exponentiate | $e^{2.0} = 7.389$ | $e^{1.0} = 2.718$ | $e^{0.5} = 1.649$ |\n",
    "| 2. Sum | $7.389 + 2.718 + 1.649 = 11.756$ |||\n",
    "| 3. Normalize | $\\frac{7.389}{11.756} = 0.629$ | $\\frac{2.718}{11.756} = 0.231$ | $\\frac{1.649}{11.756} = 0.140$ |\n",
    "\n",
    "**Result:** $P = [0.629, 0.231, 0.140]$ ‚Äî sums to 1.0 ‚úì\n",
    "\n",
    "**Prediction:** Class 1 (highest probability).\n",
    "\n",
    "### ‚úèÔ∏è Exercise 7.1: Compute Softmax by Hand\n",
    "\n",
    "Compute softmax for $\\mathbf{z} = [1.0, 2.0, 3.0]$:\n",
    "\n",
    "| | $z_1 = 1.0$ | $z_2 = 2.0$ | $z_3 = 3.0$ |\n",
    "|---|---|---|---|\n",
    "| $e^{z_i}$ | ___ | ___ | ___ |\n",
    "| $\\sum e^{z_j}$ | ___ | | |\n",
    "| $\\text{softmax}(z_i)$ | ___ | ___ | ___ |\n",
    "\n",
    "Which class is predicted? ___\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| | $z_1 = 1.0$ | $z_2 = 2.0$ | $z_3 = 3.0$ |\n",
    "|---|---|---|---|\n",
    "| $e^{z_i}$ | $2.718$ | $7.389$ | $20.086$ |\n",
    "| $\\sum e^{z_j}$ | $30.193$ | | |\n",
    "| $\\text{softmax}(z_i)$ | $0.090$ | $0.245$ | $0.665$ |\n",
    "\n",
    "**Predicted class: 3** (highest probability at 66.5%)\n",
    "\n",
    "Notice how softmax amplifies differences: the logits differ by 1 each, but the probabilities range from 9% to 66%.\n",
    "</details>\n",
    "\n",
    "### Properties of Softmax\n",
    "\n",
    "| Property | Explanation |\n",
    "|---|---|\n",
    "| All outputs $\\in (0, 1)$ | Exponentials are always positive |\n",
    "| Outputs sum to 1 | Division by total ensures normalization |\n",
    "| Preserves ordering | Largest logit ‚Üí largest probability |\n",
    "| Amplifies differences | Exponential makes large values much larger |\n",
    "| Sensitive to scale | Multiplying all logits by a constant changes the distribution |\n",
    "\n",
    "### The Numerical Stability Trick\n",
    "\n",
    "Computing $e^{z_i}$ for large $z$ can cause **overflow**. The fix:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i - \\max(\\mathbf{z})}}{\\sum_{j} e^{z_j - \\max(\\mathbf{z})}}\n",
    "$$\n",
    "\n",
    "Subtracting the max doesn't change the result (it cancels out) but keeps numbers manageable.\n",
    "\n",
    "### üíª Code It: Softmax Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Numerically stable softmax.\n",
    "    z shape: (K, N) where K = classes, N = samples\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    z_shifted = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "# Test\n",
    "z = np.array([[2.0, 1.0],\n",
    "              [1.0, 3.0],\n",
    "              [0.5, 0.5]])  # 3 classes, 2 samples\n",
    "\n",
    "probs = softmax(z)\n",
    "print(\"Logits:\")\n",
    "print(z)\n",
    "print(\"\\nSoftmax probabilities:\")\n",
    "print(probs)\n",
    "print(\"\\nSum per sample:\", np.sum(probs, axis=0))  # Should be [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd559f9",
   "metadata": {},
   "source": [
    "### üíª Code It: Visualize Softmax Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4160b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_softmax():\n",
    "    \"\"\"Show how softmax transforms logits to probabilities\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Different logit patterns\n",
    "    cases = [\n",
    "        ([2.0, 1.0, 0.5], \"Moderate confidence\"),\n",
    "        ([5.0, 1.0, 0.5], \"High confidence\"),\n",
    "        ([1.0, 1.0, 1.0], \"Uniform (uncertain)\"),\n",
    "    ]\n",
    "    \n",
    "    for ax, (logits, title) in zip(axes, cases):\n",
    "        z = np.array(logits)\n",
    "        probs = softmax(z.reshape(-1, 1)).flatten()\n",
    "        classes = [f'Class {i+1}' for i in range(len(z))]\n",
    "        \n",
    "        # Side-by-side bars\n",
    "        x = np.arange(len(z))\n",
    "        width = 0.35\n",
    "        ax.bar(x - width/2, z, width, label='Logits $z$', color='steelblue', alpha=0.7)\n",
    "        ax.bar(x + width/2, probs, width, label='Softmax $P$', color='coral', alpha=0.7)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(classes)\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Annotate probabilities\n",
    "        for i, p in enumerate(probs):\n",
    "            ax.annotate(f'{p:.1%}', xy=(i + width/2, p + 0.1), \n",
    "                       ha='center', fontsize=11, fontweight='bold', color='red')\n",
    "    \n",
    "    plt.suptitle('Softmax: From Logits to Probabilities', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada5efc",
   "metadata": {},
   "source": [
    "### Softmax vs Sigmoid\n",
    "\n",
    "| | Sigmoid | Softmax |\n",
    "|---|---|---|\n",
    "| **Use** | Binary classification | Multi-class classification |\n",
    "| **Outputs** | 1 value in $(0, 1)$ | $K$ values, sum to 1 |\n",
    "| **Formula** | $\\frac{1}{1+e^{-z}}$ | $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$ |\n",
    "| **Special case** | Softmax with $K = 2$ reduces to sigmoid! | |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Categorical Cross-Entropy Loss {#cce}\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "For multi-class problems, targets are encoded as **one-hot vectors**:\n",
    "\n",
    "| Class | One-hot | Meaning |\n",
    "|---|---|---|\n",
    "| Cat (class 0) | $[1, 0, 0]$ | 100% cat, 0% dog, 0% bird |\n",
    "| Dog (class 1) | $[0, 1, 0]$ | 0% cat, 100% dog, 0% bird |\n",
    "| Bird (class 2) | $[0, 0, 1]$ | 0% cat, 0% dog, 100% bird |\n",
    "\n",
    "### Categorical Cross-Entropy (CCE)\n",
    "\n",
    "For a single sample with one-hot target $\\mathbf{y}$ and predicted probabilities $\\hat{\\mathbf{y}}$:\n",
    "\n",
    "$$\n",
    "L_{\\text{CCE}} = -\\sum_{k=1}^{K} y_k \\log(\\hat{y}_k)\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{y}$ is one-hot (only one element is 1), this simplifies to:\n",
    "\n",
    "$$\n",
    "L_{\\text{CCE}} = -\\log(\\hat{y}_c)\n",
    "$$\n",
    "\n",
    "Where $c$ is the **true class**. We simply take $-\\log$ of the predicted probability for the correct class!\n",
    "\n",
    "### ‚úèÔ∏è Exercise 8.1: Compute CCE by Hand\n",
    "\n",
    "A 3-class network predicts:\n",
    "\n",
    "| Sample | True class | $\\hat{y}_1$ | $\\hat{y}_2$ | $\\hat{y}_3$ | Loss $-\\log(\\hat{y}_c)$ |\n",
    "|--------|-----------|-------------|-------------|-------------|------------------------|\n",
    "| 1      | Class 2   | 0.1         | 0.7         | 0.2         | ___                    |\n",
    "| 2      | Class 1   | 0.8         | 0.1         | 0.1         | ___                    |\n",
    "| 3      | Class 3   | 0.2         | 0.3         | 0.5         | ___                    |\n",
    "\n",
    "**Average CCE =** ___\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Sample | True class | Correct prob $\\hat{y}_c$ | Loss $-\\log(\\hat{y}_c)$ |\n",
    "|--------|-----------|-------------------------|------------------------|\n",
    "| 1      | Class 2   | 0.7                     | $-\\log(0.7) = 0.357$  |\n",
    "| 2      | Class 1   | 0.8                     | $-\\log(0.8) = 0.223$  |\n",
    "| 3      | Class 3   | 0.5                     | $-\\log(0.5) = 0.693$  |\n",
    "\n",
    "$$\n",
    "\\text{Average CCE} = \\frac{0.357 + 0.223 + 0.693}{3} = 0.424\n",
    "$$\n",
    "\n",
    "**Observations:**\n",
    "- Sample 2 has the lowest loss (most confident and correct)\n",
    "- Sample 3 has the highest loss (least confident about the correct class)\n",
    "- Sample 1 is in between (fairly confident and correct)\n",
    "</details>\n",
    "\n",
    "### The Gradient: Softmax + CCE\n",
    "\n",
    "Just like sigmoid + BCE, the combination of softmax + CCE gives a clean gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "**Exactly the same beautiful form!** Predicted minus true, for each class.\n",
    "\n",
    "### üíª Code It: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_hat):\n",
    "    \"\"\"\n",
    "    Compute CCE loss.\n",
    "    y_true: one-hot, shape (K, N)\n",
    "    y_hat: softmax probabilities, shape (K, N)\n",
    "    \"\"\"\n",
    "    N = y_true.shape[1]\n",
    "    y_hat_clipped = np.clip(y_hat, 1e-7, 1 - 1e-7)\n",
    "    loss = -np.sum(y_true * np.log(y_hat_clipped)) / N\n",
    "    return loss\n",
    "\n",
    "# Test\n",
    "y_true = np.array([[0, 1, 0],   # Sample 1: class 2\n",
    "                    [1, 0, 0],   # Sample 2: class 1\n",
    "                    [0, 0, 1]]).T  # Sample 3: class 3\n",
    "# shape: (3 classes, 3 samples)\n",
    "\n",
    "y_hat = np.array([[0.1, 0.8, 0.2],\n",
    "                   [0.7, 0.1, 0.3],\n",
    "                   [0.2, 0.1, 0.5]]).T\n",
    "\n",
    "print(f\"CCE Loss: {categorical_cross_entropy(y_true, y_hat):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b4fe2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Complete Classification Pipeline {#pipeline}\n",
    "\n",
    "### Multi-Class MLP Classifier\n",
    "\n",
    "Let's build a full multi-class classifier using everything we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassMLP:\n",
    "    \"\"\"\n",
    "    MLP for multi-class classification.\n",
    "    Architecture: input ‚Üí hidden (ReLU) ‚Üí output (softmax)\n",
    "    Loss: categorical cross-entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_classes):\n",
    "        np.random.seed(42)\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(n_hidden, n_input) * np.sqrt(2.0 / n_input)\n",
    "        self.b1 = np.zeros((n_hidden, 1))\n",
    "        self.W2 = np.random.randn(n_classes, n_hidden) * np.sqrt(2.0 / n_hidden)\n",
    "        self.b2 = np.zeros((n_classes, 1))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass with ReLU hidden + softmax output\"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        # Hidden layer (ReLU)\n",
    "        self.z1 = self.W1 @ X + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Output layer (softmax)\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true):\n",
    "        \"\"\"Categorical cross-entropy\"\"\"\n",
    "        return categorical_cross_entropy(y_true, self.a2)\n",
    "    \n",
    "    def backward(self, y_true, lr):\n",
    "        \"\"\"Backprop with softmax + CCE (gradient = y_hat - y_true)\"\"\"\n",
    "        N = y_true.shape[1]\n",
    "        \n",
    "        # Output layer: the beautiful gradient\n",
    "        delta2 = (self.a2 - y_true) / N           # (K, N)\n",
    "        dW2 = delta2 @ self.a1.T                   # (K, n_hidden)\n",
    "        db2 = np.sum(delta2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer\n",
    "        delta1 = (self.W2.T @ delta2) * self.relu_derivative(self.z1)\n",
    "        dW1 = delta1 @ self.X.T\n",
    "        db1 = np.sum(delta1, axis=1, keepdims=True)\n",
    "        \n",
    "        # Update\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class indices\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad35b8",
   "metadata": {},
   "source": [
    "### üíª Code It: The Spiral Dataset (3 Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d47854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_per_class=100, n_classes=3, noise=0.3):\n",
    "    \"\"\"\n",
    "    Generate a spiral dataset with K classes.\n",
    "    Classic benchmark for non-linear classifiers.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    N = n_per_class * n_classes\n",
    "    X = np.zeros((2, N))\n",
    "    y = np.zeros(N, dtype=int)\n",
    "    \n",
    "    for k in range(n_classes):\n",
    "        start = k * n_per_class\n",
    "        end = start + n_per_class\n",
    "        \n",
    "        r = np.linspace(0.2, 1.0, n_per_class)\n",
    "        theta = np.linspace(k * 4.0, (k + 1) * 4.0, n_per_class) + np.random.randn(n_per_class) * noise\n",
    "        \n",
    "        X[0, start:end] = r * np.cos(theta)\n",
    "        X[1, start:end] = r * np.sin(theta)\n",
    "        y[start:end] = k\n",
    "    \n",
    "    # One-hot encode\n",
    "    y_onehot = np.zeros((n_classes, N))\n",
    "    y_onehot[y, np.arange(N)] = 1\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(N)\n",
    "    return X[:, idx], y_onehot[:, idx], y[idx]\n",
    "\n",
    "X_spiral, y_spiral_oh, y_spiral = generate_spiral_data()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = ['blue', 'red', 'green']\n",
    "for k in range(3):\n",
    "    mask = y_spiral == k\n",
    "    plt.scatter(X_spiral[0, mask], X_spiral[1, mask], \n",
    "                c=colors[k], alpha=0.6, edgecolors='black', s=30,\n",
    "                label=f'Class {k}')\n",
    "plt.xlabel('$x_1$', fontsize=14)\n",
    "plt.ylabel('$x_2$', fontsize=14)\n",
    "plt.title('Spiral Dataset (3 Classes)', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb58e77",
   "metadata": {},
   "source": [
    "### üíª Code It: Train and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "mlp = MultiClassMLP(n_input=2, n_hidden=50, n_classes=3)\n",
    "loss_history = []\n",
    "\n",
    "n_epochs = 5000\n",
    "lr = 1.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mlp.forward(X_spiral)\n",
    "    loss = mlp.compute_loss(y_spiral_oh)\n",
    "    loss_history.append(loss)\n",
    "    mlp.backward(y_spiral_oh, lr)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        preds = mlp.predict(X_spiral)\n",
    "        acc = np.mean(preds == y_spiral) * 100\n",
    "        print(f\"Epoch {epoch:5d}: Loss = {loss:.4f}, Acc = {acc:.1f}%\")\n",
    "\n",
    "# Final accuracy\n",
    "preds = mlp.predict(X_spiral)\n",
    "print(f\"\\nFinal accuracy: {np.mean(preds == y_spiral) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c09c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision regions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(loss_history, 'b-', linewidth=1)\n",
    "axes[0].set_xlabel('Epoch', fontsize=14)\n",
    "axes[0].set_ylabel('CCE Loss', fontsize=14)\n",
    "axes[0].set_title('Training Loss', fontsize=16)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision regions\n",
    "ax = axes[1]\n",
    "x_min, x_max = X_spiral[0].min() - 0.5, X_spiral[0].max() + 0.5\n",
    "y_min, y_max = X_spiral[1].min() - 0.5, X_spiral[1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                      np.linspace(y_min, y_max, 200))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()])\n",
    "Z = mlp.predict(grid).reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, levels=[-0.5, 0.5, 1.5, 2.5], \n",
    "            colors=['#ADD8E6', '#FFCCCB', '#90EE90'], alpha=0.4)\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for k in range(3):\n",
    "    mask = y_spiral == k\n",
    "    ax.scatter(X_spiral[0, mask], X_spiral[1, mask],\n",
    "               c=colors[k], alpha=0.6, edgecolors='black', s=30,\n",
    "               label=f'Class {k}')\n",
    "\n",
    "ax.set_xlabel('$x_1$', fontsize=14)\n",
    "ax.set_ylabel('$x_2$', fontsize=14)\n",
    "ax.set_title('Learned Decision Regions', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38716bd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Evaluation Metrics {#metrics}\n",
    "\n",
    "### Beyond Accuracy\n",
    "\n",
    "Accuracy = fraction of correct predictions. Simple, but sometimes **misleading**.\n",
    "\n",
    "**Example:** A disease affects 1% of patients. A model that always predicts \"healthy\" gets 99% accuracy ‚Äî but it's completely useless!\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "For binary classification, there are 4 types of outcomes:\n",
    "\n",
    "|  | Predicted Positive | Predicted Negative |\n",
    "|---|---|---|\n",
    "| **Actually Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actually Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Formula | Question it answers |\n",
    "|---|---|---|\n",
    "| **Accuracy** | $\\frac{TP + TN}{TP + TN + FP + FN}$ | Overall, how often are we right? |\n",
    "| **Precision** | $\\frac{TP}{TP + FP}$ | When we predict positive, how often are we right? |\n",
    "| **Recall** | $\\frac{TP}{TP + FN}$ | Of all positives, how many did we catch? |\n",
    "| **F1 Score** | $2 \\cdot \\frac{P \\cdot R}{P + R}$ | Balance between precision and recall |\n",
    "\n",
    "### ‚úèÔ∏è Exercise 10.1: Compute Metrics\n",
    "\n",
    "A spam classifier produces these results on 100 emails:\n",
    "\n",
    "|  | Predicted Spam | Predicted Not Spam |\n",
    "|---|---|---|\n",
    "| **Actually Spam** | 40 | 10 |\n",
    "| **Actually Not Spam** | 5 | 45 |\n",
    "\n",
    "Compute:\n",
    "- Accuracy = ___\n",
    "- Precision = ___\n",
    "- Recall = ___\n",
    "- F1 Score = ___\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "$TP = 40, FP = 5, FN = 10, TN = 45$\n",
    "\n",
    "- **Accuracy** $= \\frac{40 + 45}{100} = 0.85 = 85\\%$\n",
    "\n",
    "- **Precision** $= \\frac{40}{40 + 5} = \\frac{40}{45} = 0.889 = 88.9\\%$\n",
    "\n",
    "- **Recall** $= \\frac{40}{40 + 10} = \\frac{40}{50} = 0.80 = 80\\%$\n",
    "\n",
    "- **F1** $= 2 \\times \\frac{0.889 \\times 0.80}{0.889 + 0.80} = 2 \\times \\frac{0.711}{1.689} = 0.842 = 84.2\\%$\n",
    "\n",
    "**Interpretation:** The classifier is better at precision (when it says \"spam\", it's usually right) than recall (it misses 20% of actual spam).\n",
    "</details>\n",
    "\n",
    "### üíª Code It: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix.\n",
    "    y_true, y_pred: arrays of class indices, shape (N,)\n",
    "    \"\"\"\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        cm[true, pred] += 1\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=None):\n",
    "    \"\"\"Visualize confusion matrix\"\"\"\n",
    "    n = cm.shape[0]\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {i}' for i in range(n)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "            ax.text(j, i, str(cm[i, j]), ha='center', va='center', \n",
    "                    color=color, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels(class_names, fontsize=12)\n",
    "    ax.set_yticklabels(class_names, fontsize=12)\n",
    "    ax.set_xlabel('Predicted', fontsize=14)\n",
    "    ax.set_ylabel('True', fontsize=14)\n",
    "    ax.set_title('Confusion Matrix', fontsize=16)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute and display for spiral dataset\n",
    "preds = mlp.predict(X_spiral)\n",
    "cm = confusion_matrix(y_spiral, preds, n_classes=3)\n",
    "plot_confusion_matrix(cm, class_names=['Class 0', 'Class 1', 'Class 2'])\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nPer-class accuracy:\")\n",
    "for k in range(3):\n",
    "    class_acc = cm[k, k] / cm[k].sum() * 100\n",
    "    print(f\"  Class {k}: {class_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8109fd6",
   "metadata": {},
   "source": [
    "### Multi-Class Metrics\n",
    "\n",
    "For $K > 2$ classes, we compute precision and recall **per class** and then average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d917b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(y_true, y_pred, n_classes):\n",
    "    \"\"\"Compute per-class and overall metrics\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, n_classes)\n",
    "    \n",
    "    print(f\"{'Class':>8} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Support':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    precisions, recalls, f1s, supports = [], [], [], []\n",
    "    \n",
    "    for k in range(n_classes):\n",
    "        tp = cm[k, k]\n",
    "        fp = cm[:, k].sum() - tp\n",
    "        fn = cm[k, :].sum() - tp\n",
    "        support = cm[k, :].sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"{'Class '+str(k):>8} {precision:10.3f} {recall:10.3f} {f1:10.3f} {support:10d}\")\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        supports.append(support)\n",
    "    \n",
    "    # Weighted average\n",
    "    total = sum(supports)\n",
    "    w_prec = sum(p * s for p, s in zip(precisions, supports)) / total\n",
    "    w_rec = sum(r * s for r, s in zip(recalls, supports)) / total\n",
    "    w_f1 = sum(f * s for f, s in zip(f1s, supports)) / total\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Avg':>8} {w_prec:10.3f} {w_rec:10.3f} {w_f1:10.3f} {total:10d}\")\n",
    "\n",
    "classification_report(y_spiral, preds, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62eabed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Final Exercises {#exercises}\n",
    "\n",
    "### üìù Exercise 11.1: Binary Classification from Scratch (Easy)\n",
    "\n",
    "Train a logistic regression model on this dataset and compute the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_binary():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1. Generate a linearly separable binary dataset\n",
    "    2. Train LogisticRegression for 500 epochs\n",
    "    3. Plot the decision boundary\n",
    "    4. Compute accuracy, precision, recall, F1\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    X0 = np.random.randn(2, 50) + np.array([[-2], [0]])\n",
    "    X1 = np.random.randn(2, 50) + np.array([[2], [0]])\n",
    "    X = np.hstack([X0, X1])\n",
    "    y = np.hstack([np.zeros((1, 50)), np.ones((1, 50))])\n",
    "    \n",
    "    # TODO: Train and evaluate\n",
    "    pass\n",
    "\n",
    "# exercise_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0aabdc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f6441",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def exercise_binary():\n",
    "    np.random.seed(0)\n",
    "    X0 = np.random.randn(2, 50) + np.array([[-2], [0]])\n",
    "    X1 = np.random.randn(2, 50) + np.array([[2], [0]])\n",
    "    X = np.hstack([X0, X1])\n",
    "    y = np.hstack([np.zeros((1, 50)), np.ones((1, 50))])\n",
    "    \n",
    "    model = LogisticRegression(n_features=2)\n",
    "    for epoch in range(500):\n",
    "        model.forward(X)\n",
    "        loss = model.compute_loss(y)\n",
    "        model.backward(X, y, lr=0.5)\n",
    "    \n",
    "    preds = model.predict(X)\n",
    "    acc = np.mean(preds == y) * 100\n",
    "    \n",
    "    tp = np.sum((preds == 1) & (y == 1))\n",
    "    fp = np.sum((preds == 1) & (y == 0))\n",
    "    fn = np.sum((preds == 0) & (y == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    print(f\"Accuracy:  {acc:.1f}%\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall:    {recall:.3f}\")\n",
    "    print(f\"F1 Score:  {f1:.3f}\")\n",
    "    \n",
    "    plot_logistic_result(model, X, y)\n",
    "\n",
    "exercise_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ccdf3f",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 11.2: Softmax by Hand (Easy)\n",
    "\n",
    "**a)** Compute softmax for $\\mathbf{z} = [0, 0, 0]$. What do you get?\n",
    "\n",
    "**b)** Compute softmax for $\\mathbf{z} = [10, 0, 0]$. What happens?\n",
    "\n",
    "**c)** Compute softmax for $\\mathbf{z} = [100, 0, 0]$. Why is the numerical stability trick needed?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**a)** $e^0 = 1$ for all, sum = 3, so softmax = $[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$.\n",
    "\n",
    "Equal logits ‚Üí **uniform distribution** (maximum uncertainty).\n",
    "\n",
    "**b)** $e^{10} \\approx 22026$, $e^0 = 1$, sum $\\approx 22028$.\n",
    "\n",
    "Softmax $\\approx [0.9999, 0.00005, 0.00005]$ ‚Äî almost all probability on class 1.\n",
    "\n",
    "**c)** $e^{100} \\approx 2.69 \\times 10^{43}$. This is still representable in float64, but for larger values (e.g., $z = 1000$), we get **overflow** ($e^{1000} = \\infty$).\n",
    "\n",
    "The stability trick: subtract $\\max(\\mathbf{z}) = 100$, so we compute $e^{0}, e^{-100}, e^{-100}$ instead. The result is the same but no overflow occurs.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 11.3: Multi-Class Classifier on Concentric Circles (Medium)\n",
    "\n",
    "Classify points into 3 rings (inner, middle, outer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rings(n_per_class=100):\n",
    "    \"\"\"Three concentric rings\"\"\"\n",
    "    np.random.seed(42)\n",
    "    N = n_per_class * 3\n",
    "    X = np.zeros((2, N))\n",
    "    y = np.zeros(N, dtype=int)\n",
    "    \n",
    "    for k, (r_min, r_max) in enumerate([(0.0, 0.4), (0.5, 0.9), (1.0, 1.4)]):\n",
    "        start = k * n_per_class\n",
    "        end = start + n_per_class\n",
    "        r = np.random.uniform(r_min, r_max, n_per_class)\n",
    "        theta = np.random.uniform(0, 2 * np.pi, n_per_class)\n",
    "        X[0, start:end] = r * np.cos(theta)\n",
    "        X[1, start:end] = r * np.sin(theta)\n",
    "        y[start:end] = k\n",
    "    \n",
    "    y_oh = np.zeros((3, N))\n",
    "    y_oh[y, np.arange(N)] = 1\n",
    "    idx = np.random.permutation(N)\n",
    "    return X[:, idx], y_oh[:, idx], y[idx]\n",
    "\n",
    "# TODO:\n",
    "# 1. Generate the dataset and visualize it\n",
    "# 2. Create a MultiClassMLP (experiment with hidden layer size)\n",
    "# 3. Train and plot loss curve\n",
    "# 4. Visualize decision regions\n",
    "# 5. Print confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04abb1fb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hints</summary>\n",
    "\n",
    "- Concentric rings need a **non-linear** boundary ‚Äî logistic regression won't work!\n",
    "- Try 20-50 hidden neurons\n",
    "- Learning rate around 0.5-1.0\n",
    "- Train for 3000-5000 epochs\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0e0bf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "X_rings, y_rings_oh, y_rings = generate_rings()\n",
    "\n",
    "# Train\n",
    "np.random.seed(42)\n",
    "mlp_rings = MultiClassMLP(n_input=2, n_hidden=30, n_classes=3)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(5000):\n",
    "    mlp_rings.forward(X_rings)\n",
    "    loss = mlp_rings.compute_loss(y_rings_oh)\n",
    "    losses.append(loss)\n",
    "    mlp_rings.backward(y_rings_oh, lr=0.8)\n",
    "\n",
    "preds = mlp_rings.predict(X_rings)\n",
    "print(f\"Accuracy: {np.mean(preds == y_rings) * 100:.1f}%\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('CCE Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()])\n",
    "Z = mlp_rings.predict(grid).reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, levels=[-0.5, 0.5, 1.5, 2.5],\n",
    "            colors=['#ADD8E6', '#FFCCCB', '#90EE90'], alpha=0.4)\n",
    "for k, c in enumerate(['blue', 'red', 'green']):\n",
    "    mask = y_rings == k\n",
    "    ax.scatter(X_rings[0, mask], X_rings[1, mask], c=c, alpha=0.6,\n",
    "               edgecolors='black', s=20, label=f'Ring {k}')\n",
    "ax.set_title('Decision Regions')\n",
    "ax.legend()\n",
    "ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "classification_report(y_rings, preds, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c111cec",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 11.4: Compare MSE vs Cross-Entropy (Hard)\n",
    "\n",
    "Train the **same MLP architecture** on the spiral dataset with two different loss functions and compare convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83344a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_comparison():\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1. Implement an MLP variant that uses MSE loss instead of CCE\n",
    "       (Hint: use sigmoid output instead of softmax, with MSE)\n",
    "    2. Train both versions on the spiral dataset for 5000 epochs\n",
    "    3. Plot both loss curves on the same graph\n",
    "    4. Compare final accuracy\n",
    "    5. Explain the difference\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683f981",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Discussion</summary>\n",
    "\n",
    "You should observe:\n",
    "- **Cross-entropy** converges faster, especially in early epochs\n",
    "- **MSE** can get stuck in flat regions (sigmoid saturation problem)\n",
    "- **Cross-entropy** typically achieves higher final accuracy\n",
    "\n",
    "This demonstrates **why cross-entropy is the standard loss for classification** ‚Äî it provides stronger gradients when the model is wrong, which is exactly when we need them most.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 11.5: Decision Threshold Optimization (Hard)\n",
    "\n",
    "For a binary classifier, find the optimal threshold by evaluating F1 score across a range of thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model, X, y):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1. Get predicted probabilities from the model\n",
    "    2. For thresholds in [0.1, 0.2, ..., 0.9]:\n",
    "       a. Compute predictions at this threshold\n",
    "       b. Compute precision, recall, F1\n",
    "    3. Plot precision, recall, and F1 vs threshold\n",
    "    4. Return the threshold that maximizes F1\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test on the binary dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd9b3a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed46d7c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model, X, y):\n",
    "    probs = model.forward(X)\n",
    "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "    \n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    \n",
    "    for t in thresholds:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        tp = np.sum((preds == 1) & (y == 1))\n",
    "        fp = np.sum((preds == 1) & (y == 0))\n",
    "        fn = np.sum((preds == 0) & (y == 1))\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precisions, 'b-o', label='Precision', linewidth=2)\n",
    "    plt.plot(thresholds, recalls, 'r-o', label='Recall', linewidth=2)\n",
    "    plt.plot(thresholds, f1s, 'g-o', label='F1 Score', linewidth=2)\n",
    "    \n",
    "    best_idx = np.argmax(f1s)\n",
    "    plt.axvline(x=thresholds[best_idx], color='gray', linestyle='--', \n",
    "                label=f'Best F1 at t={thresholds[best_idx]:.2f}')\n",
    "    \n",
    "    plt.xlabel('Threshold', fontsize=14)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.title('Precision, Recall, F1 vs Decision Threshold', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Optimal threshold: {thresholds[best_idx]:.2f}\")\n",
    "    print(f\"Best F1: {f1s[best_idx]:.3f}\")\n",
    "    \n",
    "    return thresholds[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1cb8e",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **Sigmoid for probabilities**: Output of logistic regression is $P(y=1|\\mathbf{x})$  \n",
    "‚úÖ **MSE fails for classification**: Slow gradients when sigmoid is saturated  \n",
    "‚úÖ **Binary cross-entropy**: $-[y\\log\\hat{y} + (1-y)\\log(1-\\hat{y})]$ ‚Äî steep penalty for wrong predictions  \n",
    "‚úÖ **Softmax**: Turns $K$ logits into a probability distribution that sums to 1  \n",
    "‚úÖ **Categorical cross-entropy**: $-\\log(\\hat{y}_c)$ ‚Äî penalizes low probability for the correct class  \n",
    "‚úÖ **Evaluation metrics**: Accuracy, precision, recall, F1, confusion matrix  \n",
    "‚úÖ **Decision threshold**: Tunable trade-off between precision and recall\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Match loss to task:**\n",
    "   - Regression ‚Üí MSE\n",
    "   - Binary classification ‚Üí sigmoid + BCE\n",
    "   - Multi-class classification ‚Üí softmax + CCE\n",
    "\n",
    "2. **The gradient tells the story:**\n",
    "   - MSE + sigmoid: $\\frac{\\partial L}{\\partial z} = 2(\\hat{y} - y) \\cdot \\sigma'(z)$ ‚Äî saturates!\n",
    "   - BCE + sigmoid: $\\frac{\\partial L}{\\partial z} = \\hat{y} - y$ ‚Äî clean and strong\n",
    "   - CCE + softmax: $\\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k$ ‚Äî same beautiful form\n",
    "\n",
    "3. **Accuracy is not enough:**\n",
    "   - Precision matters when false positives are costly\n",
    "   - Recall matters when false negatives are costly\n",
    "   - F1 balances both\n",
    "   - The confusion matrix shows the full picture\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Session 8: Generalization & Regularization**\n",
    "\n",
    "In the next session, we'll learn:\n",
    "- **Train/validation/test splits**: How to evaluate honestly\n",
    "- **Overfitting**: When a model memorizes instead of learning\n",
    "- **Regularization**: L1, L2, dropout ‚Äî tools to prevent overfitting\n",
    "- **Modern optimizers**: Momentum, Adam ‚Äî beyond basic SGD\n",
    "\n",
    "**The goal:** Build models that work on **new, unseen** data, not just the training set!\n",
    "\n",
    "### Before Next Session\n",
    "\n",
    "**Think about:**\n",
    "1. Our spiral classifier gets 95% accuracy on training data. Does that mean it will work well on new spirals?\n",
    "2. What if we increased the hidden layer to 500 neurons? Would that help or hurt?\n",
    "3. How would you know if your model is **too simple** vs **too complex**?\n",
    "\n",
    "**Optional reading:**\n",
    "- Chapter 7 of Goodfellow et al., \"Deep Learning\" (Regularization)\n",
    "- Andrew Ng's \"Advice for applying ML\" lecture\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 7** üéì\n",
    "\n",
    "**You now understand:**\n",
    "- ‚úÖ How to build proper classification models with the right loss functions\n",
    "- ‚úÖ How softmax and cross-entropy work together\n",
    "- ‚úÖ How to evaluate classifiers beyond simple accuracy\n",
    "\n",
    "**Next up:** Making models generalize to new data! üöÄ"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
