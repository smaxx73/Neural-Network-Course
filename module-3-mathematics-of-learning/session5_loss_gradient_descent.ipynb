{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f117851",
   "metadata": {},
   "source": [
    "# Session 5: Loss Functions & Gradient Descent\n",
    "## The Mathematics of Learning\n",
    "\n",
    "**Course: Neural Networks for Engineers**  \n",
    "**Duration: 2 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Recap: What We Know So Far](#recap)\n",
    "2. [Measuring \"How Wrong\": Loss Functions](#loss)\n",
    "3. [Loss Landscapes](#landscape)\n",
    "4. [Derivatives: The Slope Tells You Where to Go](#derivatives)\n",
    "5. [Gradient Descent: Walking Downhill](#gd)\n",
    "6. [Stochastic Gradient Descent (SGD)](#sgd)\n",
    "7. [Putting It Together: Linear Regression](#linreg)\n",
    "8. [From Linear Regression to Neural Networks](#bridge)\n",
    "9. [Final Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap: What We Know So Far {#recap}\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "‚úÖ **Perceptron**: Weighted sum + activation ‚Üí linear decision boundary  \n",
    "‚úÖ **Learning Rule**: `w ‚Üê w + Œ∑(y - ≈∑)x` works for linearly separable data  \n",
    "‚úÖ **Multi-Layer Networks**: Hidden layers create non-linear decision boundaries  \n",
    "‚úÖ **Forward Propagation**: Layer-by-layer computation of outputs  \n",
    "‚úÖ **The Problem**: Manual weight tuning doesn't scale!\n",
    "\n",
    "### ü§î Quick Questions\n",
    "\n",
    "**Q1:** Why was manual weight tuning impractical for the XOR network?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Even with just **9 parameters** (a tiny 2-2-1 network), finding good weights by hand was extremely tedious. Real networks have thousands or millions of parameters ‚Äî we need an **automatic** method.\n",
    "</details>\n",
    "\n",
    "**Q2:** What two things does a network need to learn automatically?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "1. A way to **measure how wrong** the current weights are (‚Üí loss function)\n",
    "2. A way to **adjust weights** in the right direction (‚Üí gradient descent)\n",
    "</details>\n",
    "\n",
    "**Q3:** In forward propagation, what is computed at each layer?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A **weighted sum** $z = Wx + b$ followed by an **activation function** $a = f(z)$.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Measuring \"How Wrong\": Loss Functions {#loss}\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "We need a single number that tells us: **how bad are our current predictions?**\n",
    "\n",
    "This number is called the **loss** (also called cost, error, or objective function).\n",
    "\n",
    "**Properties of a good loss function:**\n",
    "- **Zero** when predictions are perfect\n",
    "- **Larger** when predictions are more wrong\n",
    "- **Smooth** (we'll see why this matters soon)\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "The most common loss function for regression:\n",
    "\n",
    "$$\n",
    "L = \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ = number of samples\n",
    "- $y_i$ = true value for sample $i$\n",
    "- $\\hat{y}_i$ = predicted value for sample $i$\n",
    "\n",
    "### Why Squaring?\n",
    "\n",
    "| What happens if we just use $(y - \\hat{y})$? |\n",
    "|---|\n",
    "| Positive and negative errors cancel out! |\n",
    "| Predicting +5 too high and -5 too low gives average error = 0 |\n",
    "\n",
    "Squaring solves this:\n",
    "- All errors become positive\n",
    "- Large errors are penalized **more** than small ones (quadratic growth)\n",
    "\n",
    "### ‚úèÔ∏è Exercise 2.1: Compute MSE by Hand\n",
    "\n",
    "A network makes these predictions:\n",
    "\n",
    "| Sample | True $y$ | Predicted $\\hat{y}$ | Error $(y - \\hat{y})$ | Squared Error |\n",
    "|--------|----------|---------------------|----------------------|---------------|\n",
    "| 1      | 1.0      | 0.8                 | ___                  | ___           |\n",
    "| 2      | 0.0      | 0.3                 | ___                  | ___           |\n",
    "| 3      | 1.0      | 0.9                 | ___                  | ___           |\n",
    "| 4      | 0.0      | 0.1                 | ___                  | ___           |\n",
    "\n",
    "**MSE =** ___\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Sample | True $y$ | Predicted $\\hat{y}$ | Error $(y - \\hat{y})$ | Squared Error |\n",
    "|--------|----------|---------------------|----------------------|---------------|\n",
    "| 1      | 1.0      | 0.8                 | 0.2                  | 0.04          |\n",
    "| 2      | 0.0      | 0.3                 | -0.3                 | 0.09          |\n",
    "| 3      | 1.0      | 0.9                 | 0.1                  | 0.01          |\n",
    "| 4      | 0.0      | 0.1                 | -0.1                 | 0.01          |\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{0.04 + 0.09 + 0.01 + 0.01}{4} = \\frac{0.15}{4} = 0.0375\n",
    "$$\n",
    "</details>\n",
    "\n",
    "### Visualizing Loss for a Single Weight\n",
    "\n",
    "Imagine a very simple model: $\\hat{y} = w \\cdot x$ (one weight, no bias).\n",
    "\n",
    "If we plot MSE as a function of $w$, we get a **parabola** ‚Äî a smooth curve with a clear minimum!\n",
    "\n",
    "### üíª Code It: Loss as a Function of One Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple dataset: y ‚âà 2x\n",
    "np.random.seed(42)\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y_true = np.array([2.1, 3.9, 6.2, 7.8, 10.1])  # ‚âà 2*x with noise\n",
    "\n",
    "def compute_mse(w, X, y_true):\n",
    "    \"\"\"Compute MSE for y_hat = w * x\"\"\"\n",
    "    y_hat = w * X\n",
    "    return np.mean((y_true - y_hat) ** 2)\n",
    "\n",
    "# Plot MSE for different values of w\n",
    "w_values = np.linspace(0, 4, 200)\n",
    "losses = [compute_mse(w, X, y_true) for w in w_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(w_values, losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Weight $w$', fontsize=14)\n",
    "plt.ylabel('MSE Loss', fontsize=14)\n",
    "plt.title('Loss Function: How MSE Changes with Weight $w$', fontsize=16)\n",
    "\n",
    "# Mark the minimum\n",
    "w_best = w_values[np.argmin(losses)]\n",
    "plt.axvline(x=w_best, color='r', linestyle='--', alpha=0.7, label=f'Best $w$ ‚âà {w_best:.2f}')\n",
    "plt.scatter([w_best], [min(losses)], color='red', s=100, zorder=5)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal weight: w ‚âà {w_best:.2f}\")\n",
    "print(f\"Minimum MSE: {min(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199b6ba",
   "metadata": {},
   "source": [
    "### ü§î Think About It\n",
    "\n",
    "Look at the loss curve above.\n",
    "\n",
    "**Q1:** What shape does the loss curve have?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "It's a **parabola** (U-shape). MSE is always a quadratic function of the weights for linear models, so it has a single, clear minimum.\n",
    "</details>\n",
    "\n",
    "**Q2:** If you're standing at $w = 0.5$ on this curve, which direction should you move?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "**To the right** (increase $w$), because the slope at $w = 0.5$ is negative ‚Äî the loss decreases as $w$ increases. The minimum is around $w = 2$.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Loss Landscapes {#landscape}\n",
    "\n",
    "### From 1D to 2D\n",
    "\n",
    "Real models have many weights. With **two** weights ($w_1$ and $w_2$), the loss becomes a **surface** in 3D space.\n",
    "\n",
    "Think of it as a mountain landscape:\n",
    "- **x-axis**: weight $w_1$\n",
    "- **y-axis**: weight $w_2$  \n",
    "- **z-axis (height)**: loss value\n",
    "- **Goal**: find the lowest valley!\n",
    "\n",
    "### üíª Code It: 3D Loss Surface and Contour Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6932b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: y_hat = w1 * x + w2 (linear regression with weight and bias)\n",
    "np.random.seed(42)\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y_true = np.array([2.8, 5.1, 7.3, 9.0, 11.2])  # ‚âà 2*x + 1\n",
    "\n",
    "def compute_mse_2d(w1, w2, X, y_true):\n",
    "    \"\"\"MSE for y_hat = w1 * x + w2\"\"\"\n",
    "    y_hat = w1 * X + w2\n",
    "    return np.mean((y_true - y_hat) ** 2)\n",
    "\n",
    "# Create grid\n",
    "w1_range = np.linspace(0, 4, 100)\n",
    "w2_range = np.linspace(-2, 4, 100)\n",
    "W1_grid, W2_grid = np.meshgrid(w1_range, w2_range)\n",
    "Loss_grid = np.zeros_like(W1_grid)\n",
    "\n",
    "for i in range(W1_grid.shape[0]):\n",
    "    for j in range(W1_grid.shape[1]):\n",
    "        Loss_grid[i, j] = compute_mse_2d(W1_grid[i, j], W2_grid[i, j], X, y_true)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(W1_grid, W2_grid, Loss_grid, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('$w_1$ (slope)', fontsize=12)\n",
    "ax1.set_ylabel('$w_2$ (bias)', fontsize=12)\n",
    "ax1.set_zlabel('MSE Loss', fontsize=12)\n",
    "ax1.set_title('3D Loss Surface', fontsize=14)\n",
    "ax1.view_init(elev=30, azim=-120)\n",
    "\n",
    "# Contour plot (top-down view)\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W1_grid, W2_grid, Loss_grid, levels=30, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('$w_1$ (slope)', fontsize=12)\n",
    "ax2.set_ylabel('$w_2$ (bias)', fontsize=12)\n",
    "ax2.set_title('Contour Plot (Top-Down View)', fontsize=14)\n",
    "\n",
    "# Mark minimum\n",
    "idx = np.unravel_index(Loss_grid.argmin(), Loss_grid.shape)\n",
    "ax2.scatter(W1_grid[idx], W2_grid[idx], color='red', s=100, zorder=5, label='Minimum')\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26590d88",
   "metadata": {},
   "source": [
    "### ü§î Think About It\n",
    "\n",
    "**Q:** If you're standing somewhere on this surface and can only feel the **slope under your feet**, how would you find the bottom?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "You would **walk downhill** ‚Äî always step in the direction where the slope goes down the steepest. This is exactly the intuition behind **gradient descent**!\n",
    "</details>\n",
    "\n",
    "### A Note on Local vs Global Minima\n",
    "\n",
    "For linear models + MSE, the loss surface is a nice **bowl** (convex) with one minimum.\n",
    "\n",
    "For neural networks, the surface is more complex ‚Äî it can have:\n",
    "- **Local minima**: valleys that aren't the deepest\n",
    "- **Saddle points**: flat spots that aren't minima at all\n",
    "- **Plateaus**: flat regions where progress is slow\n",
    "\n",
    "We'll deal with these later. For now, let's learn the basic algorithm!\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Derivatives: The Slope Tells You Where to Go {#derivatives}\n",
    "\n",
    "### Why Do We Need Derivatives?\n",
    "\n",
    "We want to answer: **\"If I change weight $w$ by a tiny amount, how does the loss change?\"**\n",
    "\n",
    "The derivative gives us exactly this!\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw} \\approx \\frac{L(w + \\epsilon) - L(w)}{\\epsilon} \\quad \\text{(for tiny } \\epsilon \\text{)}\n",
    "$$\n",
    "\n",
    "### Geometric Intuition\n",
    "\n",
    "The derivative is the **slope** of the tangent line:\n",
    "\n",
    "| Derivative Value | Meaning | Action |\n",
    "|---|---|---|\n",
    "| $\\frac{dL}{dw} > 0$ | Loss increases when $w$ increases | **Decrease** $w$ |\n",
    "| $\\frac{dL}{dw} < 0$ | Loss decreases when $w$ increases | **Increase** $w$ |\n",
    "| $\\frac{dL}{dw} = 0$ | At a minimum (or maximum) | **Stop** |\n",
    "\n",
    "**Key insight:** Always move $w$ in the **opposite** direction of the derivative!\n",
    "\n",
    "### Quick Derivative Review\n",
    "\n",
    "Some derivatives you'll need:\n",
    "\n",
    "| Function $f(w)$ | Derivative $\\frac{df}{dw}$ |\n",
    "|---|---|\n",
    "| $w^2$ | $2w$ |\n",
    "| $aw + b$ | $a$ |\n",
    "| $w^n$ | $nw^{n-1}$ |\n",
    "| $(y - w)^2$ | $-2(y - w)$ |\n",
    "\n",
    "### ‚úèÔ∏è Exercise 4.1: Computing Derivatives\n",
    "\n",
    "Compute the derivative of each function with respect to $w$:\n",
    "\n",
    "**a)** $f(w) = 3w^2 + 2w - 1$\n",
    "\n",
    "$\\frac{df}{dw} =$ ___\n",
    "\n",
    "**b)** $f(w) = (5 - 2w)^2$\n",
    "\n",
    "$\\frac{df}{dw} =$ ___\n",
    "\n",
    "**c)** For MSE with one sample: $L(w) = (y - wx)^2$ where $y$ and $x$ are constants.\n",
    "\n",
    "$\\frac{dL}{dw} =$ ___\n",
    "\n",
    "<details>\n",
    "<summary>Solutions</summary>\n",
    "\n",
    "**a)** $\\frac{df}{dw} = 6w + 2$\n",
    "\n",
    "**b)** Using chain rule: let $u = 5 - 2w$, then $f = u^2$\n",
    "\n",
    "$$\n",
    "\\frac{df}{dw} = 2u \\cdot \\frac{du}{dw} = 2(5 - 2w)(-2) = -4(5 - 2w) = 8w - 20\n",
    "$$\n",
    "\n",
    "**c)** Let $u = y - wx$, then $L = u^2$\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw} = 2(y - wx) \\cdot (-x) = -2x(y - wx)\n",
    "$$\n",
    "\n",
    "This is the gradient we'll use for linear regression!\n",
    "</details>\n",
    "\n",
    "### Partial Derivatives: Multiple Weights\n",
    "\n",
    "When the loss depends on **multiple weights**, we compute a **partial derivative** for each one.\n",
    "\n",
    "For $L(w_1, w_2)$:\n",
    "- $\\frac{\\partial L}{\\partial w_1}$: how $L$ changes when we nudge $w_1$ (holding $w_2$ fixed)\n",
    "- $\\frac{\\partial L}{\\partial w_2}$: how $L$ changes when we nudge $w_2$ (holding $w_1$ fixed)\n",
    "\n",
    "### The Gradient Vector\n",
    "\n",
    "The **gradient** collects all partial derivatives into a vector:\n",
    "\n",
    "$$\n",
    "\\nabla L = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} \\\\ \\frac{\\partial L}{\\partial w_2} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Critical property:** The gradient points in the direction of **steepest ascent**.\n",
    "\n",
    "So $-\\nabla L$ points toward the **steepest descent** ‚Äî exactly where we want to go!\n",
    "\n",
    "### ‚úèÔ∏è Exercise 4.2: Partial Derivatives\n",
    "\n",
    "Given: $L(w_1, w_2) = (3 - 2w_1 - w_2)^2$\n",
    "\n",
    "Compute:\n",
    "\n",
    "**a)** $\\frac{\\partial L}{\\partial w_1} =$ ___\n",
    "\n",
    "**b)** $\\frac{\\partial L}{\\partial w_2} =$ ___\n",
    "\n",
    "**c)** Evaluate the gradient at $(w_1, w_2) = (0, 0)$: $\\nabla L =$ ___\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "Let $u = 3 - 2w_1 - w_2$, so $L = u^2$.\n",
    "\n",
    "**a)** $\\frac{\\partial L}{\\partial w_1} = 2u \\cdot \\frac{\\partial u}{\\partial w_1} = 2(3 - 2w_1 - w_2)(-2) = -4(3 - 2w_1 - w_2)$\n",
    "\n",
    "**b)** $\\frac{\\partial L}{\\partial w_2} = 2u \\cdot \\frac{\\partial u}{\\partial w_2} = 2(3 - 2w_1 - w_2)(-1) = -2(3 - 2w_1 - w_2)$\n",
    "\n",
    "**c)** At $(0, 0)$:\n",
    "\n",
    "$$\n",
    "\\nabla L = \\begin{bmatrix} -4(3 - 0 - 0) \\\\ -2(3 - 0 - 0) \\end{bmatrix} = \\begin{bmatrix} -12 \\\\ -6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The gradient is negative ‚Üí loss decreases when we increase the weights ‚Üí we should move in the positive direction!\n",
    "</details>\n",
    "\n",
    "### üíª Code It: Numerical vs Analytical Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, w, epsilon=1e-7):\n",
    "    \"\"\"Compute derivative numerically (finite differences)\"\"\"\n",
    "    return (f(w + epsilon) - f(w - epsilon)) / (2 * epsilon)\n",
    "\n",
    "# Example: f(w) = (3 - 2w)^2\n",
    "def f(w):\n",
    "    return (3 - 2 * w) ** 2\n",
    "\n",
    "def f_derivative_analytical(w):\n",
    "    \"\"\"Analytical derivative: -4(3 - 2w)\"\"\"\n",
    "    return -4 * (3 - 2 * w)\n",
    "\n",
    "# Compare at several points\n",
    "print(\"Comparing numerical vs analytical derivatives:\")\n",
    "print(f\"{'w':>6} | {'Numerical':>12} | {'Analytical':>12} | {'Difference':>12}\")\n",
    "print(\"-\" * 52)\n",
    "for w in [0.0, 0.5, 1.0, 1.5, 2.0]:\n",
    "    num = numerical_derivative(f, w)\n",
    "    ana = f_derivative_analytical(w)\n",
    "    print(f\"{w:6.1f} | {num:12.6f} | {ana:12.6f} | {abs(num - ana):12.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83f145",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```\n",
    "w      |    Numerical |   Analytical |   Difference\n",
    "----------------------------------------------------\n",
    "   0.0 |   -12.000000 |   -12.000000 |     3.97e-10\n",
    "   0.5 |    -8.000000 |    -8.000000 |     2.65e-10\n",
    "   1.0 |    -4.000000 |    -4.000000 |     1.32e-10\n",
    "   1.5 |     0.000000 |     0.000000 |     0.00e+00\n",
    "   2.0 |     4.000000 |     4.000000 |     1.32e-10\n",
    "```\n",
    "\n",
    "**Takeaway:** Numerical derivatives are a powerful tool for **checking** your analytical derivatives! We'll use this technique (called **gradient checking**) when implementing backpropagation in Session 6.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Gradient Descent: Walking Downhill {#gd}\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "Gradient descent is beautifully simple:\n",
    "\n",
    "**Repeat until convergence:**\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ (eta) is the **learning rate** ‚Äî how big a step we take\n",
    "- $\\frac{\\partial L}{\\partial w}$ is the gradient ‚Äî which direction to go\n",
    "\n",
    "For multiple weights:\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla L\n",
    "$$\n",
    "\n",
    "### Step-by-Step Walkthrough (1D)\n",
    "\n",
    "Let's minimize $L(w) = (3 - 2w)^2$ using gradient descent.\n",
    "\n",
    "We know: $\\frac{dL}{dw} = -4(3 - 2w)$\n",
    "\n",
    "**Settings:** $w_0 = 0$, $\\eta = 0.1$\n",
    "\n",
    "| Step | $w$ | $L(w)$ | $\\frac{dL}{dw}$ | Update $w - \\eta \\frac{dL}{dw}$ |\n",
    "|------|-----|---------|-----------------|-------------------------------|\n",
    "| 0    | 0.000 | 9.000 | -12.000 | $0 - 0.1 \\times (-12) = 1.200$ |\n",
    "| 1    | 1.200 | 0.360 | -2.400  | $1.2 - 0.1 \\times (-2.4) = 1.440$ |\n",
    "| 2    | 1.440 | 0.014 | -0.480  | $1.44 - 0.1 \\times (-0.48) = 1.488$ |\n",
    "| 3    | 1.488 | 0.001 | -0.096  | $1.488 - 0.1 \\times (-0.096) = 1.498$ |\n",
    "\n",
    "Notice: $w$ is converging toward **1.5** (the true minimum where $3 - 2w = 0$).\n",
    "\n",
    "### ‚úèÔ∏è Exercise 5.1: Manual Gradient Descent\n",
    "\n",
    "Minimize $L(w) = w^2 - 4w + 5$ using gradient descent.\n",
    "\n",
    "**Given:** $\\frac{dL}{dw} = 2w - 4$, starting point $w_0 = 0$, learning rate $\\eta = 0.3$\n",
    "\n",
    "**Perform 4 steps:**\n",
    "\n",
    "| Step | $w$ | $L(w)$ | $\\frac{dL}{dw}$ | New $w$ |\n",
    "|------|-----|---------|-----------------|---------|\n",
    "| 0    | 0   | ___     | ___             | ___     |\n",
    "| 1    | ___ | ___     | ___             | ___     |\n",
    "| 2    | ___ | ___     | ___             | ___     |\n",
    "| 3    | ___ | ___     | ___             | ___     |\n",
    "\n",
    "**What is the true minimum?** ___\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Step | $w$ | $L(w)$ | $\\frac{dL}{dw}$ | New $w$ |\n",
    "|------|-----|---------|-----------------|---------|\n",
    "| 0    | 0.000 | 5.000 | -4.000 | $0 - 0.3(-4) = 1.200$ |\n",
    "| 1    | 1.200 | 1.640 | -1.600 | $1.2 - 0.3(-1.6) = 1.680$ |\n",
    "| 2    | 1.680 | 1.102 | -0.640 | $1.68 - 0.3(-0.64) = 1.872$ |\n",
    "| 3    | 1.872 | 1.016 | -0.256 | $1.872 - 0.3(-0.256) = 1.949$ |\n",
    "\n",
    "**True minimum:** $\\frac{dL}{dw} = 0 \\Rightarrow 2w - 4 = 0 \\Rightarrow w = 2$\n",
    "\n",
    "$L(2) = 4 - 8 + 5 = 1$ (minimum loss)\n",
    "\n",
    "The algorithm is converging toward $w = 2$ ‚úì\n",
    "</details>\n",
    "\n",
    "### The Learning Rate: Goldilocks Problem\n",
    "\n",
    "The learning rate $\\eta$ controls step size. Getting it right is critical:\n",
    "\n",
    "| $\\eta$ too small | $\\eta$ just right | $\\eta$ too large |\n",
    "|---|---|---|\n",
    "| Takes forever | Converges nicely | Overshoots, diverges! |\n",
    "| Might get stuck | Good balance | Loss goes UP |\n",
    "\n",
    "### üíª Code It: Gradient Descent in 1D with Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eee472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_1d(df, w_init, lr, n_steps):\n",
    "    \"\"\"\n",
    "    Run gradient descent on a 1D function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : callable\n",
    "        Derivative of the loss function\n",
    "    w_init : float\n",
    "        Starting weight\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    n_steps : int\n",
    "        Number of steps\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : list of (w, loss) tuples\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    history = []\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        loss = f(w)          # Compute current loss\n",
    "        grad = df(w)         # Compute gradient\n",
    "        history.append((w, loss))\n",
    "        w = w - lr * grad    # Update!\n",
    "    \n",
    "    history.append((w, f(w)))\n",
    "    return history\n",
    "\n",
    "# Loss function: L(w) = (3 - 2w)^2\n",
    "def f(w):\n",
    "    return (3 - 2 * w) ** 2\n",
    "\n",
    "def df(w):\n",
    "    return -4 * (3 - 2 * w)\n",
    "\n",
    "# Run with different learning rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "learning_rates = [0.01, 0.1, 0.3]\n",
    "titles = ['Œ∑ = 0.01 (Too Small)', 'Œ∑ = 0.1 (Just Right)', 'Œ∑ = 0.3 (Getting Risky)']\n",
    "\n",
    "w_plot = np.linspace(-1, 3, 200)\n",
    "loss_plot = [(3 - 2 * w) ** 2 for w in w_plot]\n",
    "\n",
    "for ax, lr, title in zip(axes, learning_rates, titles):\n",
    "    # Plot loss curve\n",
    "    ax.plot(w_plot, loss_plot, 'b-', linewidth=2, alpha=0.5)\n",
    "    \n",
    "    # Run gradient descent\n",
    "    history = gradient_descent_1d(df, w_init=0.0, lr=lr, n_steps=15)\n",
    "    \n",
    "    # Plot trajectory\n",
    "    ws = [h[0] for h in history]\n",
    "    ls = [h[1] for h in history]\n",
    "    ax.plot(ws, ls, 'ro-', markersize=6, linewidth=1.5, label='GD path')\n",
    "    ax.scatter(ws[0], ls[0], color='green', s=100, zorder=5, label='Start')\n",
    "    ax.scatter(ws[-1], ls[-1], color='red', s=100, zorder=5, label='End')\n",
    "    \n",
    "    ax.set_xlabel('$w$', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-1, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1202af",
   "metadata": {},
   "source": [
    "### üíª Code It: Gradient Descent on 2D Contour Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af20d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2d(X, y_true, w_init, lr, n_steps):\n",
    "    \"\"\"\n",
    "    Gradient descent for y_hat = w1 * x + w2\n",
    "    \n",
    "    Returns history of (w1, w2, loss) tuples.\n",
    "    \"\"\"\n",
    "    w1, w2 = w_init\n",
    "    history = []\n",
    "    N = len(X)\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Forward pass\n",
    "        y_hat = w1 * X + w2\n",
    "        loss = np.mean((y_true - y_hat) ** 2)\n",
    "        history.append((w1, w2, loss))\n",
    "        \n",
    "        # Compute gradients\n",
    "        # TODO: Fill in the gradient formulas!\n",
    "        error = y_hat - y_true                          # (N,)\n",
    "        dL_dw1 = (2 / N) * np.sum(error * ___)         # Fill in!\n",
    "        dL_dw2 = (2 / N) * np.sum(error * ___)         # Fill in!\n",
    "        \n",
    "        # Update weights\n",
    "        w1 = w1 - lr * dL_dw1\n",
    "        w2 = w2 - lr * dL_dw2\n",
    "    \n",
    "    history.append((w1, w2, np.mean((y_true - w1 * X - w2) ** 2)))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbfa537",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution for blanks</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62375d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dw1 = (2 / N) * np.sum(error * X)     # derivative w.r.t. w1 (slope)\n",
    "dL_dw2 = (2 / N) * np.sum(error * 1)     # derivative w.r.t. w2 (bias) ‚Üí simplifies to mean(error)*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0ff55",
   "metadata": {},
   "source": [
    "**Derivation:**\n",
    "\n",
    "For $L = \\frac{1}{N}\\sum(y_i - w_1 x_i - w_2)^2$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1} = \\frac{2}{N}\\sum(\\hat{y}_i - y_i) \\cdot x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_2} = \\frac{2}{N}\\sum(\\hat{y}_i - y_i) \\cdot 1\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GD trajectory on contour plot\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y_true = np.array([2.8, 5.1, 7.3, 9.0, 11.2])\n",
    "\n",
    "# Create loss landscape\n",
    "w1_range = np.linspace(0, 4, 100)\n",
    "w2_range = np.linspace(-2, 4, 100)\n",
    "W1_grid, W2_grid = np.meshgrid(w1_range, w2_range)\n",
    "Loss_grid = np.zeros_like(W1_grid)\n",
    "\n",
    "for i in range(W1_grid.shape[0]):\n",
    "    for j in range(W1_grid.shape[1]):\n",
    "        y_hat = W1_grid[i, j] * X + W2_grid[i, j]\n",
    "        Loss_grid[i, j] = np.mean((y_true - y_hat) ** 2)\n",
    "\n",
    "# Run GD with different learning rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "lrs = [0.005, 0.02, 0.05]\n",
    "titles = ['Œ∑ = 0.005 (Slow)', 'Œ∑ = 0.02 (Good)', 'Œ∑ = 0.05 (Fast)']\n",
    "\n",
    "for ax, lr, title in zip(axes, lrs, titles):\n",
    "    ax.contour(W1_grid, W2_grid, Loss_grid, levels=30, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    history = gradient_descent_2d(X, y_true, w_init=(0.5, -1.0), lr=lr, n_steps=50)\n",
    "    w1s = [h[0] for h in history]\n",
    "    w2s = [h[1] for h in history]\n",
    "    \n",
    "    ax.plot(w1s, w2s, 'ro-', markersize=3, linewidth=1, alpha=0.8)\n",
    "    ax.scatter(w1s[0], w2s[0], color='green', s=100, zorder=5, label='Start')\n",
    "    ax.scatter(w1s[-1], w2s[-1], color='red', s=100, zorder=5, label='End')\n",
    "    \n",
    "    ax.set_xlabel('$w_1$ (slope)', fontsize=12)\n",
    "    ax.set_ylabel('$w_2$ (bias)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037664e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Stochastic Gradient Descent (SGD) {#sgd}\n",
    "\n",
    "### The Problem with Full-Batch Gradient Descent\n",
    "\n",
    "In the algorithm above, we compute the gradient using **all** samples:\n",
    "\n",
    "$$\n",
    "\\nabla L = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla L_i\n",
    "$$\n",
    "\n",
    "**Problem:** If $N$ = 1,000,000 images, computing one gradient update is very expensive!\n",
    "\n",
    "### The Stochastic Solution\n",
    "\n",
    "Instead of using all data, use a **random subset** (mini-batch):\n",
    "\n",
    "| Variant | Batch Size | Pros | Cons |\n",
    "|---|---|---|---|\n",
    "| **Full-Batch GD** | All $N$ samples | Stable, exact gradient | Slow per update |\n",
    "| **Stochastic GD** | 1 sample | Very fast per update | Very noisy |\n",
    "| **Mini-Batch SGD** | $B$ samples (e.g., 32) | Good balance | Most common in practice |\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "```\n",
    "for each epoch:\n",
    "    shuffle the dataset\n",
    "    for each mini-batch of size B:\n",
    "        1. Forward pass on mini-batch\n",
    "        2. Compute loss on mini-batch\n",
    "        3. Compute gradient on mini-batch\n",
    "        4. Update weights: w ‚Üê w - Œ∑ * gradient\n",
    "```\n",
    "\n",
    "**Key term ‚Äî Epoch:** one complete pass through the entire dataset.\n",
    "\n",
    "### ü§î Think About It\n",
    "\n",
    "**Q:** Why might the noise in SGD actually be **helpful**?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "The noise acts as a form of **implicit regularization** ‚Äî it can help escape local minima and saddle points. A perfectly smooth gradient might settle into a sharp local minimum, while SGD's noise can \"bounce\" out and find a better, flatter minimum that generalizes better.\n",
    "</details>\n",
    "\n",
    "### üíª Code It: Full-Batch vs Mini-Batch vs Stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_variants(X, y_true, w_init, lr, n_epochs, batch_size):\n",
    "    \"\"\"\n",
    "    SGD with configurable batch size.\n",
    "    \n",
    "    batch_size = len(X)  ‚Üí Full-batch GD\n",
    "    batch_size = 1       ‚Üí Stochastic GD\n",
    "    batch_size = k       ‚Üí Mini-batch SGD\n",
    "    \"\"\"\n",
    "    w1, w2 = w_init\n",
    "    N = len(X)\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(N)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y_true[indices]\n",
    "        \n",
    "        # Process mini-batches\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(start + batch_size, N)\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_hat = w1 * X_batch + w2\n",
    "            \n",
    "            # Gradients\n",
    "            error = y_hat - y_batch\n",
    "            B = len(X_batch)\n",
    "            dL_dw1 = (2 / B) * np.sum(error * X_batch)\n",
    "            dL_dw2 = (2 / B) * np.sum(error)\n",
    "            \n",
    "            # Update\n",
    "            w1 -= lr * dL_dw1\n",
    "            w2 -= lr * dL_dw2\n",
    "        \n",
    "        # Record loss on full dataset (for monitoring)\n",
    "        full_loss = np.mean((y_true - w1 * X - w2) ** 2)\n",
    "        loss_history.append(full_loss)\n",
    "    \n",
    "    return w1, w2, loss_history\n",
    "\n",
    "# Compare variants\n",
    "np.random.seed(42)\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y_true = np.array([2.8, 5.1, 7.3, 9.0, 11.2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for batch_size, label, color in [(len(X), 'Full-Batch', 'blue'), \n",
    "                                   (2, 'Mini-Batch (B=2)', 'green'),\n",
    "                                   (1, 'Stochastic (B=1)', 'red')]:\n",
    "    _, _, losses = sgd_variants(X, y_true, w_init=(0.5, -1.0), \n",
    "                                 lr=0.01, n_epochs=50, batch_size=batch_size)\n",
    "    ax.plot(losses, label=label, color=color, linewidth=2, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('MSE Loss', fontsize=14)\n",
    "ax.set_title('Comparing SGD Variants', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dc54c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Putting It Together: Linear Regression {#linreg}\n",
    "\n",
    "### Linear Regression as a \"Neural Network\"\n",
    "\n",
    "Linear regression is the **simplest possible neural network**:\n",
    "- 1 neuron\n",
    "- No activation function (or identity activation)\n",
    "- MSE loss\n",
    "\n",
    "```\n",
    "Input       \"Neuron\"        Output\n",
    "\n",
    " x‚ÇÅ ‚îÄ‚îÄ‚îÄ w‚ÇÅ ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ\n",
    " x‚ÇÇ ‚îÄ‚îÄ‚îÄ w‚ÇÇ ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ Œ£ + b ‚îÄ‚îÄ‚îÄ‚îÄ ≈∑\n",
    "              ‚îÇ\n",
    " x‚ÇÉ ‚îÄ‚îÄ‚îÄ w‚ÇÉ ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_1 x_1 + w_2 x_2 + w_3 x_3 + b = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "### Deriving the Gradient (Single Feature)\n",
    "\n",
    "For the simple case $\\hat{y} = wx + b$ with MSE loss:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - wx_i - b)^2\n",
    "$$\n",
    "\n",
    "**Gradient with respect to $w$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - wx_i - b) \\cdot x_i\n",
    "$$\n",
    "\n",
    "**Gradient with respect to $b$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - wx_i - b)\n",
    "$$\n",
    "\n",
    "### Closed-Form vs Gradient Descent\n",
    "\n",
    "Linear regression actually has a **closed-form** (exact) solution:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "So why bother with gradient descent?\n",
    "\n",
    "| Closed-Form | Gradient Descent |\n",
    "|---|---|\n",
    "| Exact solution | Approximate (iterative) |\n",
    "| Requires matrix inversion ($O(n^3)$) | Scales to large datasets |\n",
    "| Only works for linear + MSE | Works for **any** differentiable loss |\n",
    "| ‚ùå Can't do neural networks | ‚úÖ Can do neural networks! |\n",
    "\n",
    "We learn GD on linear regression because it's simple, but the real payoff is that **the same algorithm works for neural networks**.\n",
    "\n",
    "### üíª Code It: Full Linear Regression with Gradient Descent\n",
    "\n",
    "**Fill in the blanks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gd(X, y, lr=0.01, n_epochs=100):\n",
    "    \"\"\"\n",
    "    Train linear regression y = wx + b using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array, shape (N,)\n",
    "        Input features\n",
    "    y : array, shape (N,)\n",
    "        Target values\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    n_epochs : int\n",
    "        Number of epochs\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w, b : float\n",
    "        Learned parameters\n",
    "    loss_history : list\n",
    "        MSE at each epoch\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    w = np.random.randn() * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Forward pass\n",
    "        y_hat = ___ * X + ___  # Fill in!\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = np.mean((___ - ___) ** 2)  # Fill in!\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Compute gradients\n",
    "        error = y_hat - y\n",
    "        dw = (2 / N) * np.sum(error * ___)  # Fill in!\n",
    "        db = (2 / N) * np.sum(error * ___)  # Fill in!\n",
    "        \n",
    "        # Update weights\n",
    "        w = w - ___ * ___  # Fill in!\n",
    "        b = b - ___ * ___  # Fill in!\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12df96",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution for blanks</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cc9b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "y_hat = w * X + b\n",
    "\n",
    "# Compute loss\n",
    "loss = np.mean((y - y_hat) ** 2)\n",
    "\n",
    "# Compute gradients\n",
    "error = y_hat - y\n",
    "dw = (2 / N) * np.sum(error * X)\n",
    "db = (2 / N) * np.sum(error * 1)  # or just np.sum(error)\n",
    "\n",
    "# Update weights\n",
    "w = w - lr * dw\n",
    "b = b - lr * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0b502",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "np.random.seed(42)\n",
    "X_train = np.linspace(0, 10, 50)\n",
    "y_train = 2.5 * X_train + 1.3 + np.random.randn(50) * 1.5  # y = 2.5x + 1.3 + noise\n",
    "\n",
    "# Train\n",
    "w, b, loss_history = linear_regression_gd(X_train, y_train, lr=0.005, n_epochs=200)\n",
    "\n",
    "print(f\"\\nFinal: w = {w:.4f} (true: 2.5), b = {b:.4f} (true: 1.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3dcf54",
   "metadata": {},
   "source": [
    "### üíª Code It: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "ax = axes[0]\n",
    "ax.plot(loss_history, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('MSE Loss', fontsize=14)\n",
    "ax.set_title('Training Loss Over Time', fontsize=16)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final fit\n",
    "ax = axes[1]\n",
    "ax.scatter(X_train, y_train, color='blue', alpha=0.6, label='Data')\n",
    "\n",
    "# True line\n",
    "x_line = np.linspace(0, 10, 100)\n",
    "ax.plot(x_line, 2.5 * x_line + 1.3, 'g--', linewidth=2, label='True: y = 2.5x + 1.3')\n",
    "\n",
    "# Learned line\n",
    "ax.plot(x_line, w * x_line + b, 'r-', linewidth=2, \n",
    "        label=f'Learned: y = {w:.2f}x + {b:.2f}')\n",
    "\n",
    "ax.set_xlabel('$x$', fontsize=14)\n",
    "ax.set_ylabel('$y$', fontsize=14)\n",
    "ax.set_title('Linear Regression Fit', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e0371",
   "metadata": {},
   "source": [
    "### üíª Bonus: Watch the Line Evolve During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_animate(X, y, lr=0.005, n_epochs=200, plot_every=20):\n",
    "    \"\"\"Train and show snapshots of the regression line at different epochs\"\"\"\n",
    "    N = len(X)\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.scatter(X, y, color='blue', alpha=0.5, label='Data', zorder=5)\n",
    "    \n",
    "    x_line = np.linspace(X.min() - 1, X.max() + 1, 100)\n",
    "    colors = plt.cm.Reds(np.linspace(0.3, 1.0, n_epochs // plot_every + 1))\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        y_hat = w * X + b\n",
    "        error = y_hat - y\n",
    "        dw = (2 / N) * np.sum(error * X)\n",
    "        db = (2 / N) * np.sum(error)\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "        \n",
    "        if epoch % plot_every == 0:\n",
    "            idx = epoch // plot_every\n",
    "            loss = np.mean((y - w * X - b) ** 2)\n",
    "            ax.plot(x_line, w * x_line + b, color=colors[idx], \n",
    "                    linewidth=1.5, alpha=0.7,\n",
    "                    label=f'Epoch {epoch} (L={loss:.2f})')\n",
    "    \n",
    "    # Final line\n",
    "    ax.plot(x_line, w * x_line + b, 'r-', linewidth=3, label=f'Final (w={w:.2f}, b={b:.2f})')\n",
    "    \n",
    "    ax.set_xlabel('$x$', fontsize=14)\n",
    "    ax.set_ylabel('$y$', fontsize=14)\n",
    "    ax.set_title('Regression Line Evolution During Training', fontsize=16)\n",
    "    ax.legend(fontsize=9, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "train_and_animate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a29ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. From Linear Regression to Neural Networks {#bridge}\n",
    "\n",
    "### What We Just Did\n",
    "\n",
    "We trained a **single neuron** (linear regression) using:\n",
    "1. A **loss function** (MSE) to measure error\n",
    "2. **Gradients** to know which direction to adjust weights\n",
    "3. **Gradient descent** to iteratively improve weights\n",
    "\n",
    "### The Same Recipe for Neural Networks\n",
    "\n",
    "The training recipe is **identical** for MLPs:\n",
    "\n",
    "| Step | Linear Regression | Neural Network |\n",
    "|---|---|---|\n",
    "| 1. Forward pass | $\\hat{y} = wx + b$ | Layer-by-layer computation |\n",
    "| 2. Compute loss | $L = \\text{MSE}(y, \\hat{y})$ | Same! |\n",
    "| 3. Compute gradients | Simple derivatives | **Backpropagation** (chain rule) |\n",
    "| 4. Update weights | $w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}$ | Same, for ALL weights |\n",
    "\n",
    "The **only** difference is Step 3: computing gradients for hidden layers requires the **chain rule**, which we'll learn in Session 6.\n",
    "\n",
    "### ü§î Think About It\n",
    "\n",
    "**Q:** In our MLP from Session 4, we had weights $W^{(1)}$ (input‚Üíhidden) and $W^{(2)}$ (hidden‚Üíoutput). We can easily compute $\\frac{\\partial L}{\\partial W^{(2)}}$ (the output layer gradient). But why is $\\frac{\\partial L}{\\partial W^{(1)}}$ harder to compute?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "Because changing $W^{(1)}$ affects the **hidden layer activations** $h$, which in turn affect the **output**. The loss doesn't depend on $W^{(1)}$ directly ‚Äî it depends on it **through** the hidden layer. We need the **chain rule** to \"propagate\" the error backward through the network.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W^{(1)}}\n",
    "$$\n",
    "\n",
    "This is **backpropagation** ‚Äî the topic of Session 6!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Final Exercises {#exercises}\n",
    "\n",
    "### üìù Exercise 9.1: Loss and Gradient by Hand (Easy)\n",
    "\n",
    "A simple model $\\hat{y} = 3x$ is evaluated on these data points:\n",
    "\n",
    "| $x$ | $y$ (true) | $\\hat{y} = 3x$ | Error $(y - \\hat{y})$ |\n",
    "|-----|-----------|----------------|----------------------|\n",
    "| 1   | 2         | ___            | ___                  |\n",
    "| 2   | 5         | ___            | ___                  |\n",
    "| 3   | 7         | ___            | ___                  |\n",
    "\n",
    "**a)** Compute the MSE loss.\n",
    "\n",
    "**b)** Compute $\\frac{\\partial L}{\\partial w}$ where $\\hat{y} = wx$ and $w = 3$.\n",
    "\n",
    "**c)** With $\\eta = 0.01$, what is the new value of $w$ after one gradient descent step?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Fill in the table:**\n",
    "\n",
    "| $x$ | $y$ (true) | $\\hat{y} = 3x$ | Error $(y - \\hat{y})$ |\n",
    "|-----|-----------|----------------|----------------------|\n",
    "| 1   | 2         | 3              | -1                   |\n",
    "| 2   | 5         | 6              | -1                   |\n",
    "| 3   | 7         | 9              | -2                   |\n",
    "\n",
    "**a)** $\\text{MSE} = \\frac{(-1)^2 + (-1)^2 + (-2)^2}{3} = \\frac{1 + 1 + 4}{3} = 2.0$\n",
    "\n",
    "**b)** $\\frac{\\partial L}{\\partial w} = \\frac{-2}{N} \\sum (y_i - wx_i) x_i = \\frac{-2}{3}[(-1)(1) + (-1)(2) + (-2)(3)]$\n",
    "\n",
    "$$\n",
    "= \\frac{-2}{3}(-1 - 2 - 6) = \\frac{-2}{3}(-9) = 6.0\n",
    "$$\n",
    "\n",
    "**c)** $w_{\\text{new}} = 3 - 0.01 \\times 6.0 = 2.94$\n",
    "\n",
    "The weight decreased ‚Äî the model is correcting its overprediction ‚úì\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 9.2: Gradient Descent on a Quadratic (Medium)\n",
    "\n",
    "Implement gradient descent to minimize $L(w) = (w - 3)^2 + 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a738e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_quadratic():\n",
    "    \"\"\"\n",
    "    Minimize L(w) = (w - 3)^2 + 1\n",
    "    \n",
    "    TODO:\n",
    "    1. Write the derivative dL/dw\n",
    "    2. Implement gradient descent with w_init = -2, lr = 0.1, 30 steps\n",
    "    3. Plot the loss curve and the GD trajectory\n",
    "    \"\"\"\n",
    "    # Analytical derivative\n",
    "    def dL_dw(w):\n",
    "        return ___  # TODO!\n",
    "    \n",
    "    # Gradient descent\n",
    "    w = -2.0\n",
    "    lr = 0.1\n",
    "    history = []\n",
    "    \n",
    "    for step in range(30):\n",
    "        loss = (w - 3) ** 2 + 1\n",
    "        history.append((w, loss))\n",
    "        grad = dL_dw(w)\n",
    "        w = ___  # TODO: update rule!\n",
    "    \n",
    "    history.append((w, (w - 3) ** 2 + 1))\n",
    "    \n",
    "    # Plotting\n",
    "    w_range = np.linspace(-3, 7, 200)\n",
    "    loss_range = (w_range - 3) ** 2 + 1\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(w_range, loss_range, 'b-', linewidth=2, label='$L(w) = (w-3)^2 + 1$')\n",
    "    \n",
    "    ws = [h[0] for h in history]\n",
    "    ls = [h[1] for h in history]\n",
    "    plt.plot(ws, ls, 'ro-', markersize=5, linewidth=1, label='GD trajectory')\n",
    "    plt.scatter(ws[0], ls[0], color='green', s=150, zorder=5, label='Start')\n",
    "    plt.scatter(ws[-1], ls[-1], color='red', s=150, zorder=5, label='End')\n",
    "    \n",
    "    plt.xlabel('$w$', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.title('Gradient Descent on $(w-3)^2 + 1$', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final w = {ws[-1]:.4f} (should be close to 3.0)\")\n",
    "    print(f\"Final loss = {ls[-1]:.4f} (should be close to 1.0)\")\n",
    "\n",
    "# minimize_quadratic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5143b2f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175eb477",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def dL_dw(w):\n",
    "    return 2 * (w - 3)\n",
    "\n",
    "# Update rule:\n",
    "w = w - lr * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0466d23",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 9.3: Learning Rate Exploration (Medium)\n",
    "\n",
    "Test linear regression with different learning rates and observe what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e27539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_experiment():\n",
    "    \"\"\"\n",
    "    Train linear regression with lr = [0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "    TODO:\n",
    "    1. Generate data: y = 2x + 1 + noise (50 points, x in [0, 5])\n",
    "    2. Train with each learning rate for 500 epochs\n",
    "    3. Plot the loss curves on the same graph\n",
    "    4. Observe: which converges fastest? Does any diverge?\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = np.linspace(0, 5, 50)\n",
    "    y = 2 * X + 1 + np.random.randn(50) * 0.5\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
    "        _, _, losses = linear_regression_gd(X, y, lr=lr, n_epochs=500)\n",
    "        ax.plot(losses, label=f'Œ∑ = {lr}', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch', fontsize=14)\n",
    "    ax.set_ylabel('MSE Loss', fontsize=14)\n",
    "    ax.set_title('Learning Rate Comparison', fontsize=16)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_yscale('log')  # Log scale to see all curves\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# learning_rate_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df065832",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "1. Which learning rate converges the fastest?\n",
    "2. What happens with $\\eta = 0.1$?\n",
    "3. Can you find the largest learning rate that still converges?\n",
    "\n",
    "<details>\n",
    "<summary>Discussion</summary>\n",
    "\n",
    "1. $\\eta = 0.01$ typically converges fastest for this dataset\n",
    "2. $\\eta = 0.1$ likely **diverges** ‚Äî the loss explodes! This is because the steps overshoot the minimum.\n",
    "3. The maximum stable learning rate depends on the data. For this dataset, something around $\\eta = 0.05$ is near the boundary. A good rule of thumb: start with 0.01 and adjust.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 9.4: 2D Linear Regression (Hard)\n",
    "\n",
    "Extend linear regression to **two input features**: $\\hat{y} = w_1 x_1 + w_2 x_2 + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_2d(X1, X2, y, lr=0.001, n_epochs=500):\n",
    "    \"\"\"\n",
    "    Train y = w1*x1 + w2*x2 + b using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X1, X2 : arrays, shape (N,)\n",
    "        Input features\n",
    "    y : array, shape (N,)\n",
    "        Targets\n",
    "    \n",
    "    TODO:\n",
    "    1. Initialize w1, w2, b\n",
    "    2. Forward pass: y_hat = w1*X1 + w2*X2 + b\n",
    "    3. Compute gradients for w1, w2, and b\n",
    "    4. Update all three parameters\n",
    "    5. Record loss history\n",
    "    \"\"\"\n",
    "    N = len(X1)\n",
    "    w1, w2, b = 0.0, 0.0, 0.0\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # TODO: Implement!\n",
    "        pass\n",
    "    \n",
    "    return w1, w2, b, loss_history\n",
    "\n",
    "# Generate 2D data: y = 3*x1 - 2*x2 + 5 + noise\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "X1 = np.random.randn(N)\n",
    "X2 = np.random.randn(N)\n",
    "y = 3 * X1 - 2 * X2 + 5 + np.random.randn(N) * 0.5\n",
    "\n",
    "# Train and verify\n",
    "w1, w2, b, losses = linear_regression_2d(X1, X2, y, lr=0.01, n_epochs=500)\n",
    "print(f\"Learned: w1={w1:.2f} (true: 3), w2={w2:.2f} (true: -2), b={b:.2f} (true: 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637cddc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b32e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def linear_regression_2d(X1, X2, y, lr=0.001, n_epochs=500):\n",
    "    N = len(X1)\n",
    "    w1, w2, b = 0.0, 0.0, 0.0\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Forward pass\n",
    "        y_hat = w1 * X1 + w2 * X2 + b\n",
    "        \n",
    "        # Loss\n",
    "        loss = np.mean((y - y_hat) ** 2)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Gradients\n",
    "        error = y_hat - y\n",
    "        dw1 = (2 / N) * np.sum(error * X1)\n",
    "        dw2 = (2 / N) * np.sum(error * X2)\n",
    "        db  = (2 / N) * np.sum(error)\n",
    "        \n",
    "        # Update\n",
    "        w1 -= lr * dw1\n",
    "        w2 -= lr * dw2\n",
    "        b  -= lr * db\n",
    "    \n",
    "    return w1, w2, b, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbcdd8d",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **Loss Functions**: Measure \"how wrong\" predictions are (MSE)  \n",
    "‚úÖ **Loss Landscapes**: Visualize loss as a surface over weight space  \n",
    "‚úÖ **Derivatives**: The slope tells us which direction to move  \n",
    "‚úÖ **Gradient**: Vector of partial derivatives ‚Äî points toward steepest ascent  \n",
    "‚úÖ **Gradient Descent**: Walk downhill by updating $w \\leftarrow w - \\eta \\nabla L$  \n",
    "‚úÖ **SGD**: Use mini-batches for efficiency and implicit regularization  \n",
    "‚úÖ **Linear Regression**: Trained with GD ‚Äî same recipe used for neural networks!\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Loss + Gradient = Learning:**\n",
    "   - The loss function measures the error\n",
    "   - The gradient tells us how to reduce it\n",
    "   - Gradient descent applies the correction iteratively\n",
    "\n",
    "2. **Learning rate matters:**\n",
    "   - Too small ‚Üí slow convergence\n",
    "   - Too large ‚Üí divergence\n",
    "   - Finding the right value is part of training\n",
    "\n",
    "3. **The GD recipe is universal:**\n",
    "   - Works for any differentiable model + loss\n",
    "   - Same algorithm trains linear regression and deep neural networks\n",
    "   - The only hard part for MLPs: computing gradients for hidden layers\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Session 6: Backpropagation**\n",
    "\n",
    "In the next session, we'll learn:\n",
    "- **The chain rule**: How to compute gradients through multiple layers\n",
    "- **Backpropagation**: The algorithm that makes neural network training possible\n",
    "- **Training an MLP**: Finally train our XOR network automatically!\n",
    "\n",
    "**The goal:** Compute $\\frac{\\partial L}{\\partial W^{(l)}}$ for **any** layer, so we can train deep networks!\n",
    "\n",
    "### Before Next Session\n",
    "\n",
    "**Think about:**\n",
    "1. In a chain of functions $f(g(x))$, how would you compute $\\frac{df}{dx}$?\n",
    "2. If changing a hidden neuron's output by +0.1 increases the loss by +0.3, what is $\\frac{\\partial L}{\\partial h}$?\n",
    "3. How would you \"pass\" the error from the output layer back to the hidden layer?\n",
    "\n",
    "**Optional reading:**\n",
    "- 3Blue1Brown: \"Gradient descent, how neural networks learn\" (YouTube)\n",
    "- Chapter 6.5 of Goodfellow et al., \"Deep Learning\"\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 5** üéì\n",
    "\n",
    "**You now understand:**\n",
    "- ‚úÖ How to measure error with loss functions\n",
    "- ‚úÖ How derivatives guide optimization\n",
    "- ‚úÖ How gradient descent trains models automatically\n",
    "\n",
    "**Next up:** Backpropagation ‚Äî training deep networks! üöÄ"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
