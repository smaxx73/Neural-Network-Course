{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: The Perceptron Learning Rule\n",
    "## Learning Without Calculus\n",
    "\n",
    "**Course: Neural Networks for Engineers**  \n",
    "**Duration: 2 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The Problem: Manual Weights Don't Scale](#problem)\n",
    "2. [The Big Idea: Learn From Mistakes](#idea)\n",
    "3. [Rosenblatt's Learning Algorithm](#algorithm)\n",
    "4. [Manual Example: Step by Step](#manual)\n",
    "5. [Geometric Intuition](#geometric)\n",
    "6. [Implementation From Scratch](#implementation)\n",
    "7. [Visualizing Learning](#visualization)\n",
    "8. [Learning Rate Effects](#learning-rate)\n",
    "9. [Convergence: When Does It Work?](#convergence)\n",
    "10. [Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Problem: Manual Weights Don't Scale {#problem}\n",
    "\n",
    "### What We've Done So Far\n",
    "\n",
    "In the previous session, we manually chose weights:\n",
    "- AND gate: $w_1 = 1, w_2 = 1, b = -1.5$\n",
    "- OR gate: $w_1 = 1, w_2 = 1, b = -0.5$\n",
    "- NOT gate: $w = -1, b = 0.5$\n",
    "\n",
    "This worked because:\n",
    "- ‚úÖ Simple problems (2 inputs)\n",
    "- ‚úÖ We knew the solution\n",
    "- ‚úÖ Trial and error was fast\n",
    "\n",
    "### The Real World Problem\n",
    "\n",
    "Imagine classifying emails as spam/not spam:\n",
    "- **Input features**: 1000+ words\n",
    "- **Possible weights**: 1000+ numbers to choose\n",
    "- **Manual tuning**: Impossible! üò±\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Email features: [2, 0, 5, 1, 0, 3, ...]  # word counts\n",
    "                 ‚Üì  ‚Üì  ‚Üì  ‚Üì  ‚Üì  ‚Üì\n",
    "Weights:        [?, ?, ?, ?, ?, ?]  # What should these be?\n",
    "```\n",
    "\n",
    "### We Need Automatic Learning!\n",
    "\n",
    "**Question:** Can we teach the perceptron to find its own weights?\n",
    "\n",
    "**Answer:** YES! Frank Rosenblatt (1957) discovered a simple algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Big Idea: Learn From Mistakes {#idea}\n",
    "\n",
    "### Human Learning Analogy\n",
    "\n",
    "Think about learning to throw a basketball:\n",
    "1. **Try**: Throw the ball\n",
    "2. **Observe**: Did it go in?\n",
    "3. **Adjust**: \n",
    "   - If too short ‚Üí throw harder next time\n",
    "   - If too long ‚Üí throw softer next time\n",
    "4. **Repeat**: Keep adjusting until you succeed\n",
    "\n",
    "### Perceptron Learning\n",
    "\n",
    "Same idea!\n",
    "1. **Try**: Make a prediction with current weights\n",
    "2. **Observe**: Is the prediction correct?\n",
    "3. **Adjust**:\n",
    "   - If correct ‚Üí do nothing! ‚úÖ\n",
    "   - If wrong ‚Üí adjust weights in the right direction ‚ùå\n",
    "4. **Repeat**: Keep adjusting until all predictions are correct\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**When we make a mistake, we know which direction to adjust!**\n",
    "\n",
    "Example:\n",
    "- **Prediction**: Class 0 (below the line)\n",
    "- **Actual**: Class 1 (should be above the line)\n",
    "- **Fix**: Move the line toward this point!\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Rosenblatt's Learning Algorithm {#algorithm}\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "For each training example $(\\mathbf{x}, y)$:\n",
    "\n",
    "1. **Predict**: $\\hat{y} = \\text{sign}(\\mathbf{w}^T \\mathbf{x} + b)$\n",
    "\n",
    "2. **Calculate Error**: $\\text{error} = y - \\hat{y}$\n",
    "\n",
    "3. **Update Weights**:\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\text{error} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "4. **Update Bias**:\n",
    "$$\n",
    "b \\leftarrow b + \\eta \\cdot \\text{error}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ (eta) = **learning rate** (typically 0.01 to 1.0)\n",
    "- error = how wrong we were (+2, 0, or -2 with sign function)\n",
    "\n",
    "### Understanding the Update\n",
    "\n",
    "Let's decode this formula: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\text{error} \\cdot \\mathbf{x}$\n",
    "\n",
    "**Case 1: Correct Prediction**\n",
    "- $y = \\hat{y}$ ‚Üí error = 0\n",
    "- Update: $\\mathbf{w} \\leftarrow \\mathbf{w} + 0 = \\mathbf{w}$\n",
    "- **No change!** ‚úÖ\n",
    "\n",
    "**Case 2: Should be +1, predicted -1**\n",
    "- $y = +1, \\hat{y} = -1$ ‚Üí error = +2\n",
    "- Update: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot (+2) \\cdot \\mathbf{x}$\n",
    "- **Move w toward x** (increase dot product)\n",
    "\n",
    "**Case 3: Should be -1, predicted +1**\n",
    "- $y = -1, \\hat{y} = +1$ ‚Üí error = -2\n",
    "- Update: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot (-2) \\cdot \\mathbf{x}$\n",
    "- **Move w away from x** (decrease dot product)\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "```\n",
    "Initialize w = [0, 0, ...], b = 0\n",
    "\n",
    "For each epoch (pass through data):\n",
    "    For each training example (x, y):\n",
    "        # Predict\n",
    "        z = w ¬∑ x + b\n",
    "        ≈∑ = sign(z)\n",
    "        \n",
    "        # Update if wrong\n",
    "        if ≈∑ ‚â† y:\n",
    "            error = y - ≈∑\n",
    "            w = w + Œ∑ * error * x\n",
    "            b = b + Œ∑ * error\n",
    "    \n",
    "    # Check if all correct\n",
    "    if no mistakes:\n",
    "        DONE! ‚úì\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Manual Example: Step by Step {#manual}\n",
    "\n",
    "Let's learn the OR gate from data!\n",
    "\n",
    "### Training Data\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ (target) |\n",
    "|-------|-------|--------------|\n",
    "| 0     | 0     | 0            |\n",
    "| 0     | 1     | 1            |\n",
    "| 1     | 0     | 1            |\n",
    "| 1     | 1     | 1            |\n",
    "\n",
    "### Initialization\n",
    "\n",
    "- $\\mathbf{w} = [0, 0]^T$ (start with zeros)\n",
    "- $b = 0$\n",
    "- $\\eta = 1$ (learning rate)\n",
    "\n",
    "### Epoch 1\n",
    "\n",
    "#### Example 1: $(x_1, x_2) = (0, 0), y = 0$\n",
    "\n",
    "**Predict:**\n",
    "$$\n",
    "z = w_1 \\cdot 0 + w_2 \\cdot 0 + b = 0 + 0 + 0 = 0\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(0) = 1 \\text{ (by convention, sign(0) = 1)}\n",
    "$$\n",
    "\n",
    "**Error:**\n",
    "$$\n",
    "\\text{error} = y - \\hat{y} = 0 - 1 = -1\n",
    "$$\n",
    "\n",
    "**Update:**\n",
    "$$\n",
    "w_1 \\leftarrow 0 + 1 \\cdot (-1) \\cdot 0 = 0\n",
    "$$\n",
    "$$\n",
    "w_2 \\leftarrow 0 + 1 \\cdot (-1) \\cdot 0 = 0\n",
    "$$\n",
    "$$\n",
    "b \\leftarrow 0 + 1 \\cdot (-1) = -1\n",
    "$$\n",
    "\n",
    "**New weights:** $\\mathbf{w} = [0, 0]^T, b = -1$\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 2: $(x_1, x_2) = (0, 1), y = 1$\n",
    "\n",
    "**Predict:**\n",
    "$$\n",
    "z = 0 \\cdot 0 + 0 \\cdot 1 + (-1) = -1\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(-1) = -1\n",
    "$$\n",
    "\n",
    "**Error:**\n",
    "$$\n",
    "\\text{error} = 1 - (-1) = 2\n",
    "$$\n",
    "\n",
    "**Update:**\n",
    "$$\n",
    "w_1 \\leftarrow 0 + 1 \\cdot 2 \\cdot 0 = 0\n",
    "$$\n",
    "$$\n",
    "w_2 \\leftarrow 0 + 1 \\cdot 2 \\cdot 1 = 2\n",
    "$$\n",
    "$$\n",
    "b \\leftarrow -1 + 1 \\cdot 2 = 1\n",
    "$$\n",
    "\n",
    "**New weights:** $\\mathbf{w} = [0, 2]^T, b = 1$\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 3: $(x_1, x_2) = (1, 0), y = 1$\n",
    "\n",
    "**Predict:**\n",
    "$$\n",
    "z = 0 \\cdot 1 + 2 \\cdot 0 + 1 = 1\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(1) = 1\n",
    "$$\n",
    "\n",
    "**Error:** $\\text{error} = 1 - 1 = 0$ ‚úÖ **Correct!**\n",
    "\n",
    "**Update:** No change\n",
    "\n",
    "**Weights stay:** $\\mathbf{w} = [0, 2]^T, b = 1$\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 4: $(x_1, x_2) = (1, 1), y = 1$\n",
    "\n",
    "**Predict:**\n",
    "$$\n",
    "z = 0 \\cdot 1 + 2 \\cdot 1 + 1 = 3\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(3) = 1\n",
    "$$\n",
    "\n",
    "**Error:** $\\text{error} = 1 - 1 = 0$ ‚úÖ **Correct!**\n",
    "\n",
    "**Weights stay:** $\\mathbf{w} = [0, 2]^T, b = 1$\n",
    "\n",
    "---\n",
    "\n",
    "### End of Epoch 1\n",
    "\n",
    "**Mistakes in epoch 1:** 2 (examples 1 and 2)\n",
    "\n",
    "Let's verify with final weights on all data:\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ | $z = 0 \\cdot x_1 + 2 \\cdot x_2 + 1$ | $\\hat{y}$ | Correct? |\n",
    "|-------|-------|-----|--------------------------------------|-----------|----------|\n",
    "| 0     | 0     | 0   | 1                                    | 1         | ‚ùå        |\n",
    "| 0     | 1     | 1   | 3                                    | 1         | ‚úÖ        |\n",
    "| 1     | 0     | 1   | 1                                    | 1         | ‚úÖ        |\n",
    "| 1     | 1     | 1   | 3                                    | 1         | ‚úÖ        |\n",
    "\n",
    "Still 1 mistake! Need another epoch.\n",
    "\n",
    "### Epoch 2\n",
    "\n",
    "#### Example 1: $(0, 0), y = 0$\n",
    "\n",
    "**Predict:**\n",
    "$$\n",
    "z = 0 \\cdot 0 + 2 \\cdot 0 + 1 = 1\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(1) = 1\n",
    "$$\n",
    "\n",
    "**Error:** $-1$\n",
    "\n",
    "**Update:**\n",
    "$$\n",
    "\\mathbf{w} = [0, 2] + 1 \\cdot (-1) \\cdot [0, 0] = [0, 2]\n",
    "$$\n",
    "$$\n",
    "b = 1 + 1 \\cdot (-1) = 0\n",
    "$$\n",
    "\n",
    "**New weights:** $\\mathbf{w} = [0, 2]^T, b = 0$\n",
    "\n",
    "Rest of examples are correct...\n",
    "\n",
    "### Final Verification\n",
    "\n",
    "After a few epochs, we converge to:\n",
    "$$\n",
    "\\mathbf{w} = [0, 2]^T, b = 0 \\text{ (or similar)}\n",
    "$$\n",
    "\n",
    "**Decision boundary:** $0 \\cdot x_1 + 2 \\cdot x_2 + 0 = 0$ ‚Üí $x_2 = 0$\n",
    "\n",
    "This correctly separates OR gate data!\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Geometric Intuition {#geometric}\n",
    "\n",
    "### What's Really Happening?\n",
    "\n",
    "The learning rule **moves the decision boundary toward misclassified points**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the learning process geometrically\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Training data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 1])\n",
    "\n",
    "# Three stages of learning\n",
    "stages = [\n",
    "    ([0, 0], 0, \"Initial (random)\"),\n",
    "    ([0, 2], 1, \"After 2 updates\"),\n",
    "    ([1, 1], -0.5, \"Converged\")\n",
    "]\n",
    "\n",
    "for idx, (w, b, title) in enumerate(stages):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot points\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "               s=200, c='blue', marker='o', \n",
    "               edgecolors='black', linewidth=2, label='Class 0')\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "               s=200, c='red', marker='s', \n",
    "               edgecolors='black', linewidth=2, label='Class 1')\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    if w[1] != 0:\n",
    "        x1_line = np.linspace(-0.5, 1.5, 100)\n",
    "        x2_line = -(w[0] * x1_line + b) / w[1]\n",
    "        ax.plot(x1_line, x2_line, 'g-', linewidth=3, \n",
    "                label='Decision Boundary')\n",
    "    \n",
    "    # Highlight misclassified points\n",
    "    w_arr = np.array(w)\n",
    "    predictions = np.sign(X @ w_arr + b)\n",
    "    predictions[predictions == 0] = 1  # convention\n",
    "    y_binary = y.copy()\n",
    "    y_binary[y_binary == 0] = -1\n",
    "    \n",
    "    misclassified = predictions != y_binary\n",
    "    if np.any(misclassified):\n",
    "        ax.scatter(X[misclassified, 0], X[misclassified, 1],\n",
    "                   s=400, facecolors='none', edgecolors='orange',\n",
    "                   linewidth=3, label='Misclassified')\n",
    "    \n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Magic\n",
    "\n",
    "- **Misclassified point**: \"I'm on the wrong side!\"\n",
    "- **Update rule**: \"Let me pull the line toward you!\"\n",
    "- **Result**: Line rotates/shifts to fix the mistake\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Implementation From Scratch {#implementation}\n",
    "\n",
    "### Complete Perceptron Class with Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PerceptronLearner:\n",
    "    \"\"\"\n",
    "    Perceptron with learning capability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, max_epochs=100, random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            Learning rate (eta)\n",
    "        max_epochs : int\n",
    "            Maximum number of training epochs\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.random_state = random_state\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_history = []\n",
    "        self.weight_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the perceptron on data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values (must be 0 or 1, or -1 or 1)\n",
    "        \"\"\"\n",
    "        # Initialize random generator\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        # Convert labels to -1/+1 if they are 0/1\n",
    "        y_train = np.where(y == 0, -1, y)\n",
    "        \n",
    "        # Initialize weights to small random values\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = rgen.normal(loc=0.0, scale=0.01, size=n_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Store initial weights\n",
    "        self.weight_history.append((self.weights.copy(), self.bias))\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.max_epochs):\n",
    "            errors = 0\n",
    "            \n",
    "            # Loop through each training example\n",
    "            for xi, yi in zip(X, y_train):\n",
    "                # Make prediction\n",
    "                linear_output = np.dot(xi, self.weights) + self.bias\n",
    "                y_pred = np.sign(linear_output)\n",
    "                \n",
    "                # Handle sign(0) = 1 by convention\n",
    "                if y_pred == 0:\n",
    "                    y_pred = 1\n",
    "                \n",
    "                # Calculate error\n",
    "                error = yi - y_pred\n",
    "                \n",
    "                # Update weights if wrong\n",
    "                if error != 0:\n",
    "                    update = self.learning_rate * error\n",
    "                    self.weights += update * xi\n",
    "                    self.bias += update\n",
    "                    errors += 1\n",
    "            \n",
    "            # Store error count and weights for this epoch\n",
    "            self.errors_history.append(errors)\n",
    "            self.weight_history.append((self.weights.copy(), self.bias))\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.max_epochs} - Errors: {errors}\")\n",
    "            \n",
    "            # Check convergence\n",
    "            if errors == 0:\n",
    "                print(f\"‚úì Converged after {epoch + 1} epochs!\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        predictions = np.sign(linear_output)\n",
    "        predictions[predictions == 0] = 1\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy\n",
    "        \"\"\"\n",
    "        y_train = np.where(y == 0, -1, y)\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y_train)\n",
    "\n",
    "print(\"PerceptronLearner class ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Learning OR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR gate data\n",
    "X_or = np.array([[0, 0],\n",
    "                 [0, 1],\n",
    "                 [1, 0],\n",
    "                 [1, 1]])\n",
    "\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "# Create and train perceptron\n",
    "perceptron = PerceptronLearner(learning_rate=0.1, max_epochs=20, random_state=42)\n",
    "perceptron.fit(X_or, y_or)\n",
    "\n",
    "# Test\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Final Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Weights: {perceptron.weights}\")\n",
    "print(f\"Bias: {perceptron.bias}\")\n",
    "print(f\"Accuracy: {perceptron.score(X_or, y_or) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for i, (xi, yi) in enumerate(zip(X_or, y_or)):\n",
    "    pred = perceptron.predict(xi.reshape(1, -1))[0]\n",
    "    pred_label = 1 if pred == 1 else 0\n",
    "    print(f\"  {xi} ‚Üí Actual: {yi}, Predicted: {pred_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "```\n",
    "Epoch 1/20 - Errors: 2\n",
    "Epoch 10/20 - Errors: 0\n",
    "‚úì Converged after 5 epochs!\n",
    "\n",
    "==================================================\n",
    "Final Results:\n",
    "==================================================\n",
    "Weights: [0.2  0.3]\n",
    "Bias: -0.1\n",
    "Accuracy: 100.0%\n",
    "\n",
    "Predictions:\n",
    "  [0 0] ‚Üí Actual: 0, Predicted: 0\n",
    "  [0 1] ‚Üí Actual: 1, Predicted: 1\n",
    "  [1 0] ‚Üí Actual: 1, Predicted: 1\n",
    "  [1 1] ‚Üí Actual: 1, Predicted: 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Visualizing Learning {#visualization}\n",
    "\n",
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot errors over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(perceptron.errors_history) + 1), \n",
    "         perceptron.errors_history, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Number of Errors', fontsize=14)\n",
    "plt.title('Perceptron Learning Curve (OR Gate)', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='r', linestyle='--', label='Perfect Classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_evolution(perceptron, X, y, epochs_to_show):\n",
    "    \"\"\"\n",
    "    Show how decision boundary evolves during training\n",
    "    \"\"\"\n",
    "    n_plots = len(epochs_to_show)\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 4))\n",
    "    \n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, epoch in enumerate(epochs_to_show):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get weights at this epoch\n",
    "        w, b = perceptron.weight_history[epoch]\n",
    "        \n",
    "        # Plot data points\n",
    "        ax.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "                   s=200, c='blue', marker='o', \n",
    "                   edgecolors='black', linewidth=2, label='Class 0')\n",
    "        ax.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "                   s=200, c='red', marker='s', \n",
    "                   edgecolors='black', linewidth=2, label='Class 1')\n",
    "        \n",
    "        # Plot decision boundary\n",
    "        if w[1] != 0:\n",
    "            x1_line = np.linspace(-0.5, 1.5, 100)\n",
    "            x2_line = -(w[0] * x1_line + b) / w[1]\n",
    "            ax.plot(x1_line, x2_line, 'g-', linewidth=3)\n",
    "        \n",
    "        # Check for errors\n",
    "        y_binary = np.where(y == 0, -1, 1)\n",
    "        z = X @ w + b\n",
    "        predictions = np.sign(z)\n",
    "        predictions[predictions == 0] = 1\n",
    "        errors = np.sum(predictions != y_binary)\n",
    "        \n",
    "        ax.set_xlim(-0.5, 1.5)\n",
    "        ax.set_ylim(-0.5, 1.5)\n",
    "        ax.set_xlabel('$x_1$', fontsize=12)\n",
    "        ax.set_ylabel('$x_2$', fontsize=12)\n",
    "        ax.set_title(f'Epoch {epoch}\\n({errors} errors)', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if idx == 0:\n",
    "            ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show evolution at epochs 0, 1, 2, final\n",
    "epochs_to_show = [0, 1, 2, len(perceptron.weight_history)-1]\n",
    "plot_decision_boundary_evolution(perceptron, X_or, y_or, epochs_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated Learning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learning_animation(perceptron, X, y, interval=500):\n",
    "    \"\"\"\n",
    "    Create an animation of the learning process\n",
    "    \"\"\"\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        \n",
    "        # Get weights at this epoch\n",
    "        if frame < len(perceptron.weight_history):\n",
    "            w, b = perceptron.weight_history[frame]\n",
    "        else:\n",
    "            w, b = perceptron.weight_history[-1]\n",
    "        \n",
    "        # Plot data\n",
    "        ax.scatter(X[y == 0, 0], X[y == 0, 1], \n",
    "                   s=200, c='blue', marker='o', \n",
    "                   edgecolors='black', linewidth=2, label='Class 0')\n",
    "        ax.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "                   s=200, c='red', marker='s', \n",
    "                   edgecolors='black', linewidth=2, label='Class 1')\n",
    "        \n",
    "        # Plot boundary\n",
    "        if w[1] != 0:\n",
    "            x1_line = np.linspace(-0.5, 1.5, 100)\n",
    "            x2_line = -(w[0] * x1_line + b) / w[1]\n",
    "            ax.plot(x1_line, x2_line, 'g-', linewidth=3)\n",
    "        \n",
    "        # Calculate errors\n",
    "        y_binary = np.where(y == 0, -1, 1)\n",
    "        predictions = np.sign(X @ w + b)\n",
    "        predictions[predictions == 0] = 1\n",
    "        errors = np.sum(predictions != y_binary)\n",
    "        \n",
    "        ax.set_xlim(-0.5, 1.5)\n",
    "        ax.set_ylim(-0.5, 1.5)\n",
    "        ax.set_xlabel('$x_1$', fontsize=14)\n",
    "        ax.set_ylabel('$x_2$', fontsize=14)\n",
    "        ax.set_title(f'Epoch {frame}\\nErrors: {errors}', fontsize=16)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, \n",
    "                        frames=len(perceptron.weight_history),\n",
    "                        interval=interval, repeat=True)\n",
    "    plt.close()\n",
    "    \n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "# To view animation (in Jupyter):\n",
    "# visualize_learning_animation(perceptron, X_or, y_or)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Learning Rate Effects {#learning-rate}\n",
    "\n",
    "### What is the Learning Rate?\n",
    "\n",
    "The learning rate $\\eta$ controls **how big** each update step is.\n",
    "\n",
    "- **Too small**: Learning is slow üêå\n",
    "- **Too large**: Learning is unstable, might not converge üé¢\n",
    "- **Just right**: Fast and stable convergence üéØ\n",
    "\n",
    "### Experiment: Different Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    # Train perceptron\n",
    "    perc = PerceptronLearner(learning_rate=lr, max_epochs=50, random_state=42)\n",
    "    perc.fit(X_or, y_or)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    ax = axes[idx]\n",
    "    epochs = range(1, len(perc.errors_history) + 1)\n",
    "    ax.plot(epochs, perc.errors_history, 'bo-', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Number of Errors', fontsize=12)\n",
    "    ax.set_title(f'Learning Rate Œ∑ = {lr}', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.5, max(perc.errors_history) + 0.5)\n",
    "    \n",
    "    # Add convergence info\n",
    "    converged = 0 in perc.errors_history\n",
    "    if converged:\n",
    "        conv_epoch = perc.errors_history.index(0) + 1\n",
    "        ax.text(0.6, 0.9, f'Converged at epoch {conv_epoch}',\n",
    "                transform=ax.transAxes, fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules of Thumb\n",
    "\n",
    "**Learning Rate Guidelines:**\n",
    "- Start with $\\eta = 0.1$\n",
    "- If not converging: decrease $\\eta$\n",
    "- If too slow: increase $\\eta$ (carefully!)\n",
    "- Typical range: $[0.001, 1.0]$\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Convergence: When Does It Work? {#convergence}\n",
    "\n",
    "### The Perceptron Convergence Theorem\n",
    "\n",
    "**Theorem (Rosenblatt, 1962):**  \n",
    "If the training data is **linearly separable**, the perceptron algorithm will find a solution in a **finite** number of steps.\n",
    "\n",
    "### What Does \"Linearly Separable\" Mean?\n",
    "\n",
    "Data is linearly separable if you can draw a straight line (in 2D) or hyperplane (in higher dimensions) that perfectly separates the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of linearly separable vs not\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Example 1: Linearly Separable (AND gate)\n",
    "ax = axes[0]\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "ax.scatter(X_and[y_and == 0, 0], X_and[y_and == 0, 1], \n",
    "           s=200, c='blue', marker='o', edgecolors='black', linewidth=2)\n",
    "ax.scatter(X_and[y_and == 1, 0], X_and[y_and == 1, 1], \n",
    "           s=200, c='red', marker='s', edgecolors='black', linewidth=2)\n",
    "# Draw separating line\n",
    "x_line = np.linspace(-0.2, 1.2, 100)\n",
    "y_line = -x_line + 1.2\n",
    "ax.plot(x_line, y_line, 'g-', linewidth=3)\n",
    "ax.set_xlim(-0.3, 1.3)\n",
    "ax.set_ylim(-0.3, 1.3)\n",
    "ax.set_title('Linearly Separable\\n(Perceptron WILL converge)', fontsize=12, color='green')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Example 2: Also Linearly Separable (Random points)\n",
    "np.random.seed(42)\n",
    "X_sep = np.vstack([\n",
    "    np.random.randn(20, 2) + [2, 2],\n",
    "    np.random.randn(20, 2) + [-2, -2]\n",
    "])\n",
    "y_sep = np.array([1]*20 + [0]*20)\n",
    "ax = axes[1]\n",
    "ax.scatter(X_sep[y_sep == 0, 0], X_sep[y_sep == 0, 1], \n",
    "           s=100, c='blue', marker='o', edgecolors='black', linewidth=1, alpha=0.6)\n",
    "ax.scatter(X_sep[y_sep == 1, 0], X_sep[y_sep == 1, 1], \n",
    "           s=100, c='red', marker='s', edgecolors='black', linewidth=1, alpha=0.6)\n",
    "ax.plot([-5, 5], [0, 0], 'g-', linewidth=3)\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_title('Linearly Separable\\n(Perceptron WILL converge)', fontsize=12, color='green')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Example 3: NOT Linearly Separable (XOR)\n",
    "ax = axes[2]\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "ax.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], \n",
    "           s=200, c='blue', marker='o', edgecolors='black', linewidth=2)\n",
    "ax.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], \n",
    "           s=200, c='red', marker='s', edgecolors='black', linewidth=2)\n",
    "# Try to draw a line (impossible!)\n",
    "ax.plot([-0.3, 1.3], [0.5, 0.5], 'r--', linewidth=2, alpha=0.5, label='No line works!')\n",
    "ax.set_xlim(-0.3, 1.3)\n",
    "ax.set_ylim(-0.3, 1.3)\n",
    "ax.set_title('NOT Linearly Separable\\n(Perceptron will NOT converge)', fontsize=12, color='red')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on XOR (Will Fail!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to learn XOR (this will fail!)\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "print(\"Attempting to learn XOR...\")\n",
    "print(\"(This will NOT converge!)\\n\")\n",
    "\n",
    "perceptron_xor = PerceptronLearner(learning_rate=0.1, max_epochs=100, random_state=42)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "print(f\"\\nFinal accuracy: {perceptron_xor.score(X_xor, y_xor) * 100:.1f}%\")\n",
    "print(\"(Will never reach 100% because XOR is not linearly separable)\")\n",
    "\n",
    "# Plot learning curve showing no convergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(perceptron_xor.errors_history) + 1), \n",
    "         perceptron_xor.errors_history, 'ro-', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Number of Errors', fontsize=14)\n",
    "plt.title('XOR: Perceptron Cannot Converge', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='g', linestyle='--', alpha=0.5, label='Perfect (impossible)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Perceptron Learning?\n",
    "\n",
    "‚úÖ **Use when:**\n",
    "- Data is (approximately) linearly separable\n",
    "- You need a simple, interpretable model\n",
    "- Quick baseline for binary classification\n",
    "\n",
    "‚ùå **Don't use when:**\n",
    "- Data is not linearly separable (like XOR)\n",
    "- You need probabilistic outputs\n",
    "- Problem is highly non-linear\n",
    "\n",
    "**Solution for non-linear problems:** Multi-layer networks! (Next session!)\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Exercises {#exercises}\n",
    "\n",
    "### üìù Exercise 10.1: Manual Learning (Easy)\n",
    "\n",
    "Given training data:\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|-------|-------|-----|\n",
    "| 1     | 2     | +1  |\n",
    "| 2     | 1     | +1  |\n",
    "| 2     | 3     | -1  |\n",
    "\n",
    "Starting with $\\mathbf{w} = [0, 0]^T, b = 0, \\eta = 1$:\n",
    "\n",
    "**Tasks:**\n",
    "1. Manually compute the first 3 updates (show all calculations)\n",
    "2. What are the weights after these 3 examples?\n",
    "3. Predict the class for new point $(1.5, 2.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù Exercise 10.2: Learning Rate Experiment (Easy)\n",
    "\n",
    "Train a perceptron on the AND gate with learning rates: $\\eta \\in \\{0.01, 0.1, 1.0, 10.0\\}$\n",
    "\n",
    "**Tasks:**\n",
    "1. How many epochs does each take to converge?\n",
    "2. What happens with $\\eta = 10.0$? Why?\n",
    "3. Plot all learning curves on the same graph\n",
    "4. Which learning rate is best? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù Exercise 10.3: Visualization Challenge (Medium)\n",
    "\n",
    "Create an animated visualization showing:\n",
    "1. The current point being processed (highlighted)\n",
    "2. The decision boundary before update\n",
    "3. The decision boundary after update\n",
    "4. A trail showing how the boundary moved\n",
    "\n",
    "**Hint:** Use matplotlib animation or create a sequence of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù Exercise 10.4: Linearly Separable Data (Medium)\n",
    "\n",
    "Generate random linearly separable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X_class0 = np.random.randn(50, 2) + [-2, -2]\n",
    "X_class1 = np.random.randn(50, 2) + [2, 2]\n",
    "X = np.vstack([X_class0, X_class1])\n",
    "y = np.array([0]*50 + [1]*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "1. Plot the data\n",
    "2. Train a perceptron with $\\eta = 0.1$\n",
    "3. Plot the final decision boundary\n",
    "4. Calculate and report accuracy\n",
    "5. How many epochs did it take?\n",
    "6. What happens if you add noise? Try adding 5 points that cross the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù Exercise 10.5: Convergence Analysis (Hard)\n",
    "\n",
    "**Theoretical question:**\n",
    "\n",
    "The convergence theorem says the number of mistakes is bounded by:\n",
    "$$\n",
    "\\text{mistakes} \\leq \\left(\\frac{R}{\\gamma}\\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $R = \\max_i \\|\\mathbf{x}^{(i)}\\|$ (maximum distance from origin)\n",
    "- $\\gamma$ = margin (minimum distance to decision boundary)\n",
    "\n",
    "**Tasks:**\n",
    "1. For the OR gate data, calculate $R$\n",
    "2. Estimate $\\gamma$ (distance from closest point to your learned boundary)\n",
    "3. Calculate the theoretical bound on mistakes\n",
    "4. Compare with actual number of mistakes from your training\n",
    "5. Repeat with data that has:\n",
    "   - Large margin (well-separated)\n",
    "   - Small margin (barely separable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "### üìù Exercise 10.6: Early Stopping (Hard)\n",
    "\n",
    "The perceptron can overfit on noisy data!\n",
    "\n",
    "**Scenario:** Add label noise to OR gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip 1 label\n",
    "X_or_noisy = X_or.copy()\n",
    "y_or_noisy = y_or.copy()\n",
    "y_or_noisy[0] = 1  # Flip (0,0) to class 1 (wrong!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "1. Train perceptron for 1000 epochs on noisy data\n",
    "2. Plot training error vs epoch\n",
    "3. What happens? Does it converge?\n",
    "4. Implement \"early stopping\": Stop when no improvement for 10 epochs\n",
    "5. Compare solutions with and without early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù Exercise 10.7: Perceptron vs Pocket Algorithm (Very Hard)\n",
    "\n",
    "The **Pocket Algorithm** keeps track of the best weights seen so far.\n",
    "\n",
    "**Pseudocode:**\n",
    "```\n",
    "best_weights = w\n",
    "best_errors = infinity\n",
    "\n",
    "For each epoch:\n",
    "    For each example:\n",
    "        Update w using perceptron rule\n",
    "    \n",
    "    # Count errors with current weights\n",
    "    current_errors = count_errors(w)\n",
    "    \n",
    "    # Keep best weights\n",
    "    if current_errors < best_errors:\n",
    "        best_weights = w\n",
    "        best_errors = current_errors\n",
    "\n",
    "Return best_weights\n",
    "```\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement the Pocket Algorithm\n",
    "2. Test on XOR data\n",
    "3. Compare with regular perceptron\n",
    "4. What's the best accuracy you can achieve on XOR?\n",
    "5. Why does Pocket work better on non-separable data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "class PocketPerceptron:\n",
    "    # Implement here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **The Perceptron Learning Rule**\n",
    "- Update only when wrong: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot \\text{error} \\cdot \\mathbf{x}$\n",
    "- Geometric interpretation: move boundary toward misclassified points\n",
    "- No calculus needed!\n",
    "\n",
    "‚úÖ **Implementation**\n",
    "- Training loop\n",
    "- Error tracking\n",
    "- Convergence detection\n",
    "\n",
    "‚úÖ **Visualization**\n",
    "- Learning curves\n",
    "- Decision boundary evolution\n",
    "- Understanding the learning process\n",
    "\n",
    "‚úÖ **Learning Rate Effects**\n",
    "- Too small = slow\n",
    "- Too large = unstable\n",
    "- Need to tune!\n",
    "\n",
    "‚úÖ **Convergence Properties**\n",
    "- Works perfectly for linearly separable data\n",
    "- Fails on XOR and other non-linear problems\n",
    "- Finite convergence guarantee\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Learning is adjustment**: We learn by correcting mistakes\n",
    "2. **Geometry matters**: The algorithm moves the line toward misclassified points\n",
    "3. **Simple but limited**: Works great for linear problems, but that's it\n",
    "4. **Motivation for next step**: We need something better for non-linear problems!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Session 4**, we'll see how to solve non-linear problems like XOR using:\n",
    "- **Hidden layers** (multiple perceptrons working together)\n",
    "- **Forward propagation** (computing outputs through layers)\n",
    "- **Manual experimentation** (before we learn gradient descent)\n",
    "\n",
    "This will set us up to understand WHY we need gradient descent and backpropagation in later sessions!\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 3** üéì\n",
    "\n",
    "**Homework:**\n",
    "- Complete exercises 10.1 - 10.4\n",
    "- (Optional challenge): Exercises 10.5 - 10.7\n",
    "- Think about: \"How can multiple perceptrons solve XOR?\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
