{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: Multi-Layer Networks (Intuitive)\n",
    "## Building Neural Networks Without Calculus\n",
    "\n",
    "**Course: Neural Networks for Engineers**  \n",
    "**Duration: 2 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Recap: What We Know So Far](#recap)\n",
    "2. [The XOR Problem: A Perceptron's Nemesis](#xor)\n",
    "3. [The Solution: Hidden Layers](#solution)\n",
    "4. [Multi-Layer Perceptron Architecture](#mlp)\n",
    "5. [Forward Propagation: Computing Outputs](#forward)\n",
    "6. [Understanding What Hidden Neurons Learn](#understanding)\n",
    "7. [Manual Weight Tuning Challenge](#manual)\n",
    "8. [Why We Need Something Better](#motivation)\n",
    "9. [Final Exercises](#exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap: What We Know So Far {#recap}\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "‚úÖ **Perceptron basics**: Weighted sum + activation  \n",
    "‚úÖ **Learning rule**: `w ‚Üê w + Œ∑(y - ≈∑)x`  \n",
    "‚úÖ **Convergence**: Works perfectly on linearly separable data  \n",
    "‚úÖ **Limitation**: Fails on XOR and other non-linear problems\n",
    "\n",
    "### ü§î Quick Question\n",
    "\n",
    "Before we continue, let's check your understanding:\n",
    "\n",
    "**Q1:** A single perceptron creates a __________ decision boundary in 2D space.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "A **line** (or more generally, a hyperplane in higher dimensions)\n",
    "</details>\n",
    "\n",
    "**Q2:** The perceptron learning rule will converge if and only if the data is __________ separable.\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "**Linearly** separable\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The XOR Problem: A Perceptron's Nemesis {#xor}\n",
    "\n",
    "### What is XOR?\n",
    "\n",
    "XOR (eXclusive OR) returns true when inputs are **different**.\n",
    "\n",
    "**Truth Table:**\n",
    "\n",
    "| $x_1$ | $x_2$ | XOR |\n",
    "|-------|-------|-----|\n",
    "| 0     | 0     | 0   |\n",
    "| 0     | 1     | 1   |\n",
    "| 1     | 0     | 1   |\n",
    "| 1     | 1     | 0   |\n",
    "\n",
    "### Why can't a Perceptron solve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XOR data\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], \n",
    "            s=300, c='blue', marker='o', \n",
    "            edgecolors='black', linewidth=3, label='Class 0')\n",
    "plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], \n",
    "            s=300, c='red', marker='s', \n",
    "            edgecolors='black', linewidth=3, label='Class 1')\n",
    "\n",
    "# Try to draw a line\n",
    "# Can you separate red squares from blue circles with ONE straight line?\n",
    "plt.xlabel('$x_1$', fontsize=14)\n",
    "plt.ylabel('$x_2$', fontsize=14)\n",
    "plt.title('XOR Problem: Can You Draw ONE Line to Separate Them?', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Exercise 2.1: Try to separate XOR\n",
    "\n",
    "**Task:** Draw (on paper or in your mind) a single straight line that separates:\n",
    "- Blue circles (0,0) and (1,1) on one side\n",
    "- Red squares (0,1) and (1,0) on the other side\n",
    "\n",
    "**Can you do it?** _______\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "**NO!** It's impossible with a single straight line. The data points are arranged diagonally - you'd need a more complex boundary (like an X or two lines).\n",
    "</details>\n",
    "\n",
    "### ü§î Think About It\n",
    "\n",
    "**Question:** If we can't use ONE line, what if we used TWO lines?\n",
    "\n",
    "Think about this before moving forward...\n",
    "\n",
    "### The Geometric Insight\n",
    "\n",
    "XOR needs a **non-linear** decision boundary. Look at this visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what boundary we actually need\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Failed linear attempt\n",
    "ax = axes[0]\n",
    "ax.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], \n",
    "           s=300, c='blue', marker='o', edgecolors='black', linewidth=3)\n",
    "ax.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], \n",
    "           s=300, c='red', marker='s', edgecolors='black', linewidth=3)\n",
    "\n",
    "# Try a line (will fail)\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "ax.plot([0.5, 0.5], [-0.5, 1.5], 'g--', linewidth=2, label='Line 1 (separates left/right)')\n",
    "ax.plot([-0.5, 1.5], [0.5, 0.5], 'purple', linestyle='--', linewidth=2, label='Line 2 (separates top/bottom)')\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Need TWO Lines!', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: What we actually need\n",
    "ax = axes[1]\n",
    "ax.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], \n",
    "           s=300, c='blue', marker='o', edgecolors='black', linewidth=3)\n",
    "ax.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], \n",
    "           s=300, c='red', marker='s', edgecolors='black', linewidth=3)\n",
    "\n",
    "# Show the regions\n",
    "from matplotlib.patches import Rectangle\n",
    "# Blue region (corners)\n",
    "ax.add_patch(Rectangle((-0.5, -0.5), 0.8, 0.8, alpha=0.2, color='blue'))\n",
    "ax.add_patch(Rectangle((0.7, 0.7), 0.8, 0.8, alpha=0.2, color='blue'))\n",
    "# Red region (sides)\n",
    "ax.add_patch(Rectangle((-0.5, 0.7), 0.8, 0.8, alpha=0.2, color='red'))\n",
    "ax.add_patch(Rectangle((0.7, -0.5), 0.8, 0.8, alpha=0.2, color='red'))\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Non-Linear Boundary Needed', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** We need to **combine** multiple linear boundaries to create non-linear boundaries!\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Solution: Hidden Layers {#solution}\n",
    "\n",
    "### The Big Idea\n",
    "\n",
    "What if we:\n",
    "1. Use **multiple perceptrons** in parallel (hidden layer)\n",
    "2. Each creates its own line\n",
    "3. **Combine** their outputs with another perceptron (output layer)\n",
    "\n",
    "This is a **Multi-Layer Perceptron (MLP)**!\n",
    "\n",
    "### Architecture for XOR\n",
    "\n",
    "```\n",
    "Input Layer    Hidden Layer    Output Layer\n",
    "   (2)            (2)              (1)\n",
    "\n",
    "    x‚ÇÅ ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ h‚ÇÅ ‚îÄ‚îÄ‚îê\n",
    "         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ≈∑\n",
    "    x‚ÇÇ ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ h‚ÇÇ ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Breaking Down the Solution\n",
    "\n",
    "The hidden layer neurons can learn:\n",
    "- **h‚ÇÅ**: \"Is the input in the top-left or bottom-right?\" ‚Üí detects one diagonal\n",
    "- **h‚ÇÇ**: \"Is the input in the top-right or bottom-left?\" ‚Üí detects other diagonal\n",
    "- **Output**: Combines h‚ÇÅ and h‚ÇÇ to make final decision\n",
    "\n",
    "### üíª Code It: Visualize the Architecture\n",
    "\n",
    "**Fill in the blanks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mlp_architecture():\n",
    "    \"\"\"Visualize a 2-2-1 MLP for XOR\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Layer positions\n",
    "    input_x = 1\n",
    "    hidden_x = 3\n",
    "    output_x = 5\n",
    "    \n",
    "    # Neuron positions (y-coordinates)\n",
    "    input_positions = [2, 1]  # x1 and x2\n",
    "    hidden_positions = [2, 1]  # h1 and h2\n",
    "    output_position = 1.5      # y_hat\n",
    "    \n",
    "    # Draw neurons\n",
    "    for i, y in enumerate(input_positions):\n",
    "        circle = plt.Circle((input_x, y), 0.2, color='lightblue', ec='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(input_x, y, f'$x_{i+1}$', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for i, y in enumerate(hidden_positions):\n",
    "        circle = plt.Circle((hidden_x, y), 0.2, color='lightgreen', ec='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(hidden_x, y, f'$h_{i+1}$', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    circle = plt.Circle((output_x, output_position), 0.2, color='lightcoral', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(output_x, output_position, r'$\\hat{y}$', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw connections (complete this!)\n",
    "    # TODO: Draw lines from input to hidden layer\n",
    "    for i_y in input_positions:\n",
    "        for h_y in hidden_positions:\n",
    "            ax.plot([input_x + 0.2, hidden_x - 0.2], [i_y, h_y], \n",
    "                   'gray', linewidth=1.5, alpha=0.6)\n",
    "    \n",
    "    # TODO: Draw lines from hidden to output layer\n",
    "    for h_y in hidden_positions:\n",
    "        ax.plot([___ + 0.2, ___ - 0.2], [h_y, output_position],  # Fill in the blanks!\n",
    "               'gray', linewidth=1.5, alpha=0.6)\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(input_x, 3, 'Input Layer', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(hidden_x, 3, 'Hidden Layer', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(output_x, 3, 'Output Layer', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, 6)\n",
    "    ax.set_ylim(0, 3.5)\n",
    "    ax.axis('off')\n",
    "    plt.title('Multi-Layer Perceptron: 2-2-1 Architecture', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# visualize_mlp_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Multi-Layer Perceptron Architecture {#mlp}\n",
    "\n",
    "### General MLP Structure\n",
    "\n",
    "An MLP consists of:\n",
    "1. **Input layer**: Receives features (not \"neurons\", just input values)\n",
    "2. **Hidden layer(s)**: One or more layers of neurons\n",
    "3. **Output layer**: Produces final prediction\n",
    "\n",
    "### Notation\n",
    "\n",
    "For a network with:\n",
    "- **L** layers (numbered 0 to L, where 0 is input)\n",
    "- Layer **l** has **n^(l)** neurons\n",
    "\n",
    "**Weights:**\n",
    "- $W^{(l)}$: weight matrix connecting layer $l-1$ to layer $l$\n",
    "- $W^{(l)}_{ij}$: weight from neuron $j$ in layer $l-1$ to neuron $i$ in layer $l$\n",
    "\n",
    "**Activations:**\n",
    "- $a^{(l)}_i$: activation (output) of neuron $i$ in layer $l$\n",
    "- $a^{(0)} = \\mathbf{x}$: input layer activations are just the input features\n",
    "\n",
    "### ‚úèÔ∏è Exercise 4.1: Architecture Counting\n",
    "\n",
    "Given this architecture: **4-8-8-3**\n",
    "\n",
    "**Answer these questions:**\n",
    "\n",
    "1. How many input features? _______\n",
    "2. How many hidden layers? _______\n",
    "3. How many neurons in the first hidden layer? _______\n",
    "4. How many output classes? _______\n",
    "5. How many total neurons (excluding input)? _______\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. **4** input features\n",
    "2. **2** hidden layers (8 neurons each)\n",
    "3. **8** neurons in first hidden layer\n",
    "4. **3** output classes\n",
    "5. **8 + 8 + 3 = 19** total neurons\n",
    "</details>\n",
    "\n",
    "### ü§î Think About It\n",
    "\n",
    "**Q:** How many weights are in the connection between:\n",
    "- Input (4 neurons) ‚Üí First hidden layer (8 neurons)?\n",
    "\n",
    "Remember: Each hidden neuron connects to ALL input neurons!\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "$4 \\times 8 = 32$ weights (plus 8 biases for the hidden neurons)\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Forward Propagation: Computing Outputs {#forward}\n",
    "\n",
    "### The Process\n",
    "\n",
    "**Forward propagation** = computing the output layer by layer.\n",
    "\n",
    "For each layer $l$:\n",
    "\n",
    "1. **Compute weighted sum** for each neuron:\n",
    "   $$\n",
    "   z^{(l)}_i = \\sum_{j} W^{(l)}_{ij} a^{(l-1)}_j + b^{(l)}_i\n",
    "   $$\n",
    "\n",
    "2. **Apply activation function**:\n",
    "   $$\n",
    "   a^{(l)}_i = f(z^{(l)}_i)\n",
    "   $$\n",
    "\n",
    "3. **Repeat** for next layer\n",
    "\n",
    "### Manual Example: XOR with 2-2-1 Network\n",
    "\n",
    "Let's solve XOR step by step with **given weights**!\n",
    "\n",
    "#### Network Architecture\n",
    "- Input: $x_1, x_2$\n",
    "- Hidden layer: $h_1, h_2$ (using sigmoid activation)\n",
    "- Output: $\\hat{y}$ (using sigmoid activation)\n",
    "\n",
    "#### Given Weights\n",
    "\n",
    "**Layer 1 (Input ‚Üí Hidden):**\n",
    "```\n",
    "W1 = [[20,  20],    # weights to h1\n",
    "      [20,  20]]    # weights to h2\n",
    "\n",
    "b1 = [-10, -30]     # biases for h1, h2\n",
    "```\n",
    "\n",
    "**Layer 2 (Hidden ‚Üí Output):**\n",
    "```\n",
    "W2 = [[20],         # weight from h1\n",
    "      [-20]]        # weight from h2\n",
    "\n",
    "b2 = [-10]          # bias for output\n",
    "```\n",
    "\n",
    "### Step-by-Step Calculation: Input (0, 1)\n",
    "\n",
    "Let's calculate the output for $(x_1, x_2) = (0, 1)$ (should give output ‚âà 1)\n",
    "\n",
    "#### Step 1: Input Layer\n",
    "$$\n",
    "a^{(0)} = [x_1, x_2] = [0, 1]\n",
    "$$\n",
    "\n",
    "#### Step 2: Hidden Layer - Weighted Sums\n",
    "\n",
    "**For h‚ÇÅ:**\n",
    "$$\n",
    "z^{(1)}_1 = W^{(1)}_{11} x_1 + W^{(1)}_{12} x_2 + b^{(1)}_1\n",
    "$$\n",
    "\n",
    "üíª **Calculate this:** (fill in the values)\n",
    "\n",
    "$$\n",
    "z^{(1)}_1 = 20 \\times ___ + 20 \\times ___ + (___) = ___\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "$$\n",
    "z^{(1)}_1 = 20 \\times 0 + 20 \\times 1 + (-10) = 10\n",
    "$$\n",
    "</details>\n",
    "\n",
    "**For h‚ÇÇ:**\n",
    "$$\n",
    "z^{(1)}_2 = 20 \\times 0 + 20 \\times 1 + (-30) = -10\n",
    "$$\n",
    "\n",
    "#### Step 3: Hidden Layer - Activations\n",
    "\n",
    "We use the **sigmoid** activation:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**For h‚ÇÅ:**\n",
    "$$\n",
    "a^{(1)}_1 = \\sigma(10) = \\frac{1}{1 + e^{-10}} \\approx 0.9999\n",
    "$$\n",
    "\n",
    "**For h‚ÇÇ:**\n",
    "$$\n",
    "a^{(1)}_2 = \\sigma(-10) = \\frac{1}{1 + e^{10}} \\approx 0.0000\n",
    "$$\n",
    "\n",
    "**Interpretation:** \n",
    "- h‚ÇÅ is **strongly activated** (‚âà1)\n",
    "- h‚ÇÇ is **not activated** (‚âà0)\n",
    "\n",
    "#### Step 4: Output Layer - Weighted Sum\n",
    "\n",
    "$$\n",
    "z^{(2)} = W^{(2)}_{11} a^{(1)}_1 + W^{(2)}_{21} a^{(1)}_2 + b^{(2)}\n",
    "$$\n",
    "\n",
    "üíª **Your turn:** Calculate this!\n",
    "\n",
    "$$\n",
    "z^{(2)} = 20 \\times ___ + (-20) \\times ___ + (___) = ___\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "$$\n",
    "z^{(2)} = 20 \\times 0.9999 + (-20) \\times 0.0000 + (-10) \\approx 10\n",
    "$$\n",
    "</details>\n",
    "\n",
    "#### Step 5: Output Layer - Activation\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(10) \\approx 0.9999 \\approx 1\n",
    "$$\n",
    "\n",
    "**Result:** Input (0,1) ‚Üí Output ‚âà 1 ‚úÖ (Correct for XOR!)\n",
    "\n",
    "### üíª Code It: Complete Forward Propagation\n",
    "\n",
    "Now let's implement this in code. **Fill in the missing parts:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation_xor(x1, x2, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward propagation for 2-2-1 network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x1, x2 : float\n",
    "        Input values\n",
    "    W1 : array, shape (2, 2)\n",
    "        Weights from input to hidden layer\n",
    "    b1 : array, shape (2,)\n",
    "        Biases for hidden layer\n",
    "    W2 : array, shape (2, 1)\n",
    "        Weights from hidden to output layer\n",
    "    b2 : float\n",
    "        Bias for output layer\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : float\n",
    "        Network output\n",
    "    h : array, shape (2,)\n",
    "        Hidden layer activations\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    x = np.array([x1, x2])\n",
    "    \n",
    "    # Hidden layer\n",
    "    # TODO: Calculate z1 (weighted sum for hidden layer)\n",
    "    z1 = W1 @ ___ + ___  # Fill in the blanks!\n",
    "    \n",
    "    # TODO: Apply sigmoid activation\n",
    "    h = ___(___)  # Fill in!\n",
    "    \n",
    "    # Output layer\n",
    "    # TODO: Calculate z2 (weighted sum for output)\n",
    "    z2 = W2.T @ ___ + ___  # Fill in the blanks!\n",
    "    \n",
    "    # TODO: Apply sigmoid activation\n",
    "    output = sigmoid(___)  # Fill in!\n",
    "    \n",
    "    return output, h\n",
    "\n",
    "# Define weights (given)\n",
    "W1 = np.array([[20, 20],\n",
    "               [20, 20]])\n",
    "b1 = np.array([-10, -30])\n",
    "\n",
    "W2 = np.array([[20],\n",
    "               [-20]])\n",
    "b2 = np.array([-10])\n",
    "\n",
    "# Test on all XOR inputs\n",
    "print(\"XOR Network Test:\")\n",
    "print(\"=\"*50)\n",
    "for x1, x2, y_true in [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0)]:\n",
    "    output, h = forward_propagation_xor(x1, x2, W1, b1, W2, b2)\n",
    "    predicted = 1 if output > 0.5 else 0\n",
    "    status = \"‚úì\" if predicted == y_true else \"‚úó\"\n",
    "    print(f\"{status} Input: ({x1}, {x2}) ‚Üí Output: {output:.4f} ‚Üí Predicted: {predicted} (True: {y_true})\")\n",
    "    print(f\"   Hidden: h1={h[0]:.4f}, h2={h[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution for blanks</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Hidden layer\n",
    "z1 = W1 @ x + b1\n",
    "h = sigmoid(z1)\n",
    "\n",
    "# Output layer\n",
    "z2 = W2.T @ h + b2\n",
    "output = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### ‚úèÔ∏è Exercise 5.1: Manual Calculation Practice\n",
    "\n",
    "Calculate the output for input **(1, 1)** manually (show all steps):\n",
    "\n",
    "1. Calculate $z^{(1)}_1$ and $z^{(1)}_2$\n",
    "2. Calculate $a^{(1)}_1$ and $a^{(1)}_2$\n",
    "3. Calculate $z^{(2)}$\n",
    "4. Calculate final output $\\hat{y}$\n",
    "5. Is this correct for XOR?\n",
    "\n",
    "**Work it out on paper first!**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Step 1: Hidden layer weighted sums**\n",
    "- $z^{(1)}_1 = 20(1) + 20(1) - 10 = 30$\n",
    "- $z^{(1)}_2 = 20(1) + 20(1) - 30 = 10$\n",
    "\n",
    "**Step 2: Hidden layer activations**\n",
    "- $a^{(1)}_1 = \\sigma(30) \\approx 1.0$\n",
    "- $a^{(1)}_2 = \\sigma(10) \\approx 1.0$\n",
    "\n",
    "**Step 3: Output weighted sum**\n",
    "- $z^{(2)} = 20(1) + (-20)(1) + (-10) = -10$\n",
    "\n",
    "**Step 4: Output activation**\n",
    "- $\\hat{y} = \\sigma(-10) \\approx 0.0$\n",
    "\n",
    "**Step 5: Verification**\n",
    "- XOR(1,1) should be 0 ‚úì **CORRECT!**\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Understanding What Hidden Neurons Learn {#understanding}\n",
    "\n",
    "### Visualizing Hidden Layer Activations\n",
    "\n",
    "Let's see what each hidden neuron \"detects\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hidden_activations(W1, b1):\n",
    "    \"\"\"\n",
    "    Visualize what each hidden neuron activates on\n",
    "    \"\"\"\n",
    "    # Create a grid of points\n",
    "    x1_range = np.linspace(-0.5, 1.5, 100)\n",
    "    x2_range = np.linspace(-0.5, 1.5, 100)\n",
    "    X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "    \n",
    "    # Calculate activations for each hidden neuron\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for idx in range(2):  # For h1 and h2\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Calculate activation for this hidden neuron on entire grid\n",
    "        Z = np.zeros_like(X1_grid)\n",
    "        for i in range(X1_grid.shape[0]):\n",
    "            for j in range(X1_grid.shape[1]):\n",
    "                x = np.array([X1_grid[i, j], X2_grid[i, j]])\n",
    "                z = W1[idx] @ x + b1[idx]\n",
    "                Z[i, j] = sigmoid(z)\n",
    "        \n",
    "        # Plot\n",
    "        contour = ax.contourf(X1_grid, X2_grid, Z, levels=20, cmap='RdYlBu_r')\n",
    "        plt.colorbar(contour, ax=ax)\n",
    "        \n",
    "        # Overlay XOR points\n",
    "        ax.scatter([0, 1], [0, 1], s=200, c='blue', marker='o', \n",
    "                   edgecolors='black', linewidth=2, label='XOR = 0')\n",
    "        ax.scatter([0, 1], [1, 0], s=200, c='red', marker='s', \n",
    "                   edgecolors='black', linewidth=2, label='XOR = 1')\n",
    "        \n",
    "        ax.set_xlabel('$x_1$', fontsize=12)\n",
    "        ax.set_ylabel('$x_2$', fontsize=12)\n",
    "        ax.set_title(f'Hidden Neuron h{idx+1} Activation', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.set_xlim(-0.5, 1.5)\n",
    "        ax.set_ylim(-0.5, 1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run with our XOR weights\n",
    "visualize_hidden_activations(W1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Interpretation Question\n",
    "\n",
    "Look at the activation maps above. \n",
    "\n",
    "**Q1:** Which regions does h‚ÇÅ activate on (show high values)?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "h‚ÇÅ activates on the **top-right region** (where $x_1 + x_2$ is large)\n",
    "</details>\n",
    "\n",
    "**Q2:** Which regions does h‚ÇÇ activate on?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "h‚ÇÇ activates on the **very top-right region** (where $x_1 + x_2$ is even larger, due to bias=-30)\n",
    "</details>\n",
    "\n",
    "**Q3:** How does the output layer combine these to solve XOR?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "Output = 20¬∑h‚ÇÅ - 20¬∑h‚ÇÇ - 10\n",
    "\n",
    "- When only h‚ÇÅ is active (e.g., at (0,1) or (1,0)): Output is positive ‚Üí 1\n",
    "- When both are active (at (1,1)): They cancel out ‚Üí 0\n",
    "- When neither is active (at (0,0)): Output is negative ‚Üí 0\n",
    "</details>\n",
    "\n",
    "### The Power of Hidden Layers\n",
    "\n",
    "**Key Insight:** Each hidden neuron creates its own linear boundary. The output layer **combines** these boundaries to create complex, non-linear decision regions!\n",
    "\n",
    "### üíª Code It: Decision Boundary Visualization\n",
    "\n",
    "Complete this code to visualize the final decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(W1, b1, W2, b2):\n",
    "    \"\"\"Visualize the final decision boundary\"\"\"\n",
    "    # Create grid\n",
    "    x1_range = np.linspace(-0.5, 1.5, 200)\n",
    "    x2_range = np.linspace(-0.5, 1.5, 200)\n",
    "    X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "    \n",
    "    # Calculate output for each point\n",
    "    Z = np.zeros_like(X1_grid)\n",
    "    for i in range(X1_grid.shape[0]):\n",
    "        for j in range(X1_grid.shape[1]):\n",
    "            output, _ = forward_propagation_xor(\n",
    "                X1_grid[i, j], X2_grid[i, j], W1, b1, W2, b2\n",
    "            )\n",
    "            Z[i, j] = output\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # TODO: Fill in - create a contour plot with the decision boundary at 0.5\n",
    "    plt.contourf(X1_grid, X2_grid, Z, levels=[0, 0.5, 1], \n",
    "                colors=['___', '___'], alpha=0.3)  # Fill in colors!\n",
    "    \n",
    "    # Draw decision boundary\n",
    "    plt.contour(X1_grid, X2_grid, Z, levels=[___],  # What level?\n",
    "               colors='green', linewidths=3)\n",
    "    \n",
    "    # Plot XOR points\n",
    "    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y_xor = np.array([0, 1, 1, 0])\n",
    "    plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1],\n",
    "               s=300, c='blue', marker='o', edgecolors='black', linewidth=3)\n",
    "    plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],\n",
    "               s=300, c='red', marker='s', edgecolors='black', linewidth=3)\n",
    "    \n",
    "    plt.xlabel('$x_1$', fontsize=14)\n",
    "    plt.ylabel('$x_2$', fontsize=14)\n",
    "    plt.title('XOR Solution: Non-Linear Decision Boundary', fontsize=16)\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.show()\n",
    "\n",
    "# visualize_decision_boundary(W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution for blanks</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.contourf(X1_grid, X2_grid, Z, levels=[0, 0.5, 1], \n",
    "            colors=['blue', 'red'], alpha=0.3)\n",
    "\n",
    "plt.contour(X1_grid, X2_grid, Z, levels=[0.5],\n",
    "           colors='green', linewidths=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Manual Weight Tuning Challenge {#manual}\n",
    "\n",
    "### The Problem\n",
    "\n",
    "We just saw that with the **right** weights, a 2-2-1 network can solve XOR perfectly.\n",
    "\n",
    "But how do we **find** those weights?\n",
    "\n",
    "### Challenge: Manual Weight Search\n",
    "\n",
    "Let's try to find good weights ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def interactive_xor_network():\n",
    "    \"\"\"\n",
    "    Interactive widget to manually tune weights\n",
    "    \"\"\"\n",
    "    # Create sliders for weights\n",
    "    w11_slider = widgets.FloatSlider(value=1, min=-30, max=30, step=1, \n",
    "                                     description='W1[0,0]:')\n",
    "    w12_slider = widgets.FloatSlider(value=1, min=-30, max=30, step=1,\n",
    "                                     description='W1[0,1]:')\n",
    "    w21_slider = widgets.FloatSlider(value=1, min=-30, max=30, step=1,\n",
    "                                     description='W1[1,0]:')\n",
    "    w22_slider = widgets.FloatSlider(value=1, min=-30, max=30, step=1,\n",
    "                                     description='W1[1,1]:')\n",
    "    b1_slider = widgets.FloatSlider(value=0, min=-30, max=30, step=1,\n",
    "                                    description='b1[0]:')\n",
    "    b2_slider = widgets.FloatSlider(value=0, min=-30, max=30, step=1,\n",
    "                                    description='b1[1]:')\n",
    "    \n",
    "    w_out1_slider = widgets.FloatSlider(value=1, min=-30, max=30, step=1,\n",
    "                                       description='W2[0]:')\n",
    "    w_out2_slider = widgets.FloatSlider(value=1, min=-30, max=30, step=1,\n",
    "                                       description='W2[1]:')\n",
    "    b_out_slider = widgets.FloatSlider(value=0, min=-30, max=30, step=1,\n",
    "                                      description='b2:')\n",
    "    \n",
    "    def update_plot(w11, w12, w21, w22, b1, b2, w_out1, w_out2, b_out):\n",
    "        # Create weight matrices\n",
    "        W1_manual = np.array([[w11, w12], [w21, w22]])\n",
    "        b1_manual = np.array([b1, b2])\n",
    "        W2_manual = np.array([[w_out1], [w_out2]])\n",
    "        b2_manual = np.array([b_out])\n",
    "        \n",
    "        # Test on XOR\n",
    "        print(\"Testing on XOR:\")\n",
    "        print(\"=\"*50)\n",
    "        correct = 0\n",
    "        for x1, x2, y_true in [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0)]:\n",
    "            output, h = forward_propagation_xor(x1, x2, W1_manual, b1_manual, \n",
    "                                               W2_manual, b2_manual)\n",
    "            predicted = 1 if output > 0.5 else 0\n",
    "            status = \"‚úì\" if predicted == y_true else \"‚úó\"\n",
    "            if predicted == y_true:\n",
    "                correct += 1\n",
    "            print(f\"{status} ({x1},{x2}) ‚Üí {output:.3f} ‚Üí {predicted} (true: {y_true})\")\n",
    "        \n",
    "        print(f\"\\nAccuracy: {correct}/4 = {correct/4*100:.0f}%\")\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_decision_boundary(W1_manual, b1_manual, W2_manual, b2_manual)\n",
    "    \n",
    "    # Create interactive widget\n",
    "    widgets.interactive(update_plot, \n",
    "                       w11=w11_slider, w12=w12_slider,\n",
    "                       w21=w21_slider, w22=w22_slider,\n",
    "                       b1=b1_slider, b2=b2_slider,\n",
    "                       w_out1=w_out1_slider, w_out2=w_out2_slider,\n",
    "                       b_out=b_out_slider)\n",
    "\n",
    "# Uncomment to try (works in Jupyter):\n",
    "# interactive_xor_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Exercise 7.1: Manual Tuning Reflection\n",
    "\n",
    "Try to manually find weights that solve XOR. After 5 minutes, answer:\n",
    "\n",
    "**Q1:** Were you able to find a perfect solution? _______\n",
    "\n",
    "**Q2:** How long did it take? _______\n",
    "\n",
    "**Q3:** How many parameters (weights + biases) do you need to tune? _______\n",
    "\n",
    "**Q4:** For this simple 2-2-1 network, there are 9 parameters. How would you feel about tuning a 784-100-10 network (79,510 parameters)? \n",
    "\n",
    "<details>\n",
    "<summary>Answer for Q3</summary>\n",
    "**9 parameters total:**\n",
    "- Layer 1: 4 weights + 2 biases = 6\n",
    "- Layer 2: 2 weights + 1 bias = 3\n",
    "</details>\n",
    "\n",
    "### The Problem with Manual Tuning\n",
    "\n",
    "**Issues:**\n",
    "1. ‚ùå **Time-consuming**: Even for tiny networks\n",
    "2. ‚ùå **Not systematic**: Trial and error\n",
    "3. ‚ùå **Doesn't scale**: Impossible for large networks\n",
    "4. ‚ùå **Not optimal**: Hard to find the best solution\n",
    "\n",
    "**We need an AUTOMATIC way to find good weights!**\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Why We Need Something Better {#motivation}\n",
    "\n",
    "### What We Can Do Now\n",
    "\n",
    "‚úÖ **Design** network architectures  \n",
    "‚úÖ **Compute** outputs (forward propagation)  \n",
    "‚úÖ **Understand** what hidden layers do  \n",
    "\n",
    "### What We Can't Do Yet\n",
    "\n",
    "‚ùå **Find** good weights automatically  \n",
    "‚ùå **Train** on large datasets efficiently  \n",
    "‚ùå **Optimize** systematically  \n",
    "\n",
    "### The Missing Piece: Gradient Descent\n",
    "\n",
    "To train networks automatically, we need:\n",
    "\n",
    "1. **A way to measure \"how wrong\" we are** ‚Üí **Loss functions** (Session 5)\n",
    "2. **A direction to adjust weights** ‚Üí **Gradients** (Session 5)\n",
    "3. **A way to calculate gradients efficiently** ‚Üí **Backpropagation** (Session 6)\n",
    "\n",
    "### ü§î Final Reflection\n",
    "\n",
    "**Before moving to the next module, think about:**\n",
    "\n",
    "1. We can solve XOR with the right weights\n",
    "2. Finding those weights manually is impractical\n",
    "3. We need an algorithm that:\n",
    "   - Measures how wrong the current weights are\n",
    "   - Adjusts weights to make them better\n",
    "   - Repeats until weights are good\n",
    "\n",
    "**This is exactly what gradient descent + backpropagation does!**\n",
    "\n",
    "### The Journey Ahead\n",
    "\n",
    "```\n",
    "Where we are now:         Where we're going:\n",
    "                         \n",
    " Random weights    ‚Üí     Measure error      ‚Üí    Calculate gradients\n",
    "      ‚îÇ                        ‚îÇ                         ‚îÇ\n",
    "      ‚ñº                        ‚ñº                         ‚ñº\n",
    " Forward prop       ‚Üí     Loss function     ‚Üí    Backpropagation\n",
    "      ‚îÇ                        ‚îÇ                         ‚îÇ\n",
    "      ‚ñº                        ‚ñº                         ‚ñº\n",
    " Manual tuning     ‚Üí     Gradient descent  ‚Üí    Automatic training\n",
    "   (slow! üò´)                                      (fast! üòä)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Final Exercises {#exercises}\n",
    "\n",
    "### üìù Exercise 9.1: Architecture Design (Easy)\n",
    "\n",
    "Design an MLP architecture for each task:\n",
    "\n",
    "**a) Binary classification with 10 input features**\n",
    "- Input layer: ___\n",
    "- Hidden layer(s): ___\n",
    "- Output layer: ___\n",
    "\n",
    "**b) Multi-class classification (5 classes) with 20 input features**\n",
    "- Input layer: ___\n",
    "- Hidden layer(s): ___\n",
    "- Output layer: ___\n",
    "\n",
    "**c) For task (a), how many parameters (weights + biases) if you use one hidden layer with 8 neurons?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**a)** 10-8-1 (or any reasonable hidden layer size)\n",
    "- Input: 10 features\n",
    "- Hidden: 8 neurons (example)\n",
    "- Output: 1 neuron (binary ‚Üí single output with sigmoid)\n",
    "\n",
    "**b)** 20-16-5 (or any reasonable hidden layer size)\n",
    "- Input: 20 features\n",
    "- Hidden: 16 neurons (example)\n",
    "- Output: 5 neurons (one per class, with softmax)\n",
    "\n",
    "**c)** For 10-8-1:\n",
    "- Layer 1: $(10 \\times 8) + 8 = 88$ parameters\n",
    "- Layer 2: $(8 \\times 1) + 1 = 9$ parameters\n",
    "- **Total: 97 parameters**\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 9.2: Forward Propagation Practice (Medium)\n",
    "\n",
    "Given this network:\n",
    "- Input: $[2, 3]$\n",
    "- Hidden layer (2 neurons, ReLU): \n",
    "  - $W^{(1)} = \\begin{bmatrix} 1 & -1 \\\\ 0 & 2 \\end{bmatrix}, b^{(1)} = \\begin{bmatrix} 0 \\\\ -3 \\end{bmatrix}$\n",
    "- Output layer (1 neuron, sigmoid):\n",
    "  - $W^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, b^{(2)} = -1$\n",
    "\n",
    "**Calculate:**\n",
    "1. Hidden layer outputs (after ReLU)\n",
    "2. Final output (after sigmoid)\n",
    "\n",
    "**Show all steps!**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Step 1: Hidden layer weighted sums**\n",
    "$$\n",
    "z^{(1)} = W^{(1)} x + b^{(1)} = \\begin{bmatrix} 1 & -1 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ -3 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "z^{(1)} = \\begin{bmatrix} 2-3 \\\\ 0+6 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2: Apply ReLU**\n",
    "$$\n",
    "h = \\text{ReLU}(z^{(1)}) = \\begin{bmatrix} \\max(0, -1) \\\\ \\max(0, 3) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 3: Output weighted sum**\n",
    "$$\n",
    "z^{(2)} = W^{(2)T} h + b^{(2)} = \\begin{bmatrix} 2 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix} + (-1)\n",
    "$$\n",
    "$$\n",
    "z^{(2)} = 0 + 3 - 1 = 2\n",
    "$$\n",
    "\n",
    "**Step 4: Apply sigmoid**\n",
    "$$\n",
    "\\hat{y} = \\sigma(2) = \\frac{1}{1+e^{-2}} \\approx 0.88\n",
    "$$\n",
    "\n",
    "**Final answer:** $\\hat{y} \\approx 0.88$\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 9.3: Implementing a 3-Layer Network (Hard)\n",
    "\n",
    "Implement a 2-3-1 network (2 inputs ‚Üí 3 hidden neurons ‚Üí 1 output) with:\n",
    "- ReLU activation in hidden layer\n",
    "- Sigmoid activation in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_network(x1, x2, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for 2-3-1 network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x1, x2 : float\n",
    "        Input values\n",
    "    W1 : array, shape (3, 2)\n",
    "        Weights input -> hidden\n",
    "    b1 : array, shape (3,)\n",
    "        Biases for hidden layer\n",
    "    W2 : array, shape (3, 1)\n",
    "        Weights hidden -> output\n",
    "    b2 : float\n",
    "        Bias for output\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : float\n",
    "        Final prediction\n",
    "    hidden : array, shape (3,)\n",
    "        Hidden layer activations\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Create input vector\n",
    "    # 2. Calculate hidden layer (with ReLU)\n",
    "    # 3. Calculate output (with sigmoid)\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "W1_test = np.random.randn(3, 2)\n",
    "b1_test = np.random.randn(3)\n",
    "W2_test = np.random.randn(3, 1)\n",
    "b2_test = np.random.randn(1)\n",
    "\n",
    "output, hidden = three_layer_network(0.5, 0.5, W1_test, b1_test, W2_test, b2_test)\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Hidden: {hidden}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def three_layer_network(x1, x2, W1, b1, W2, b2):\n",
    "    # Input\n",
    "    x = np.array([x1, x2])\n",
    "    \n",
    "    # Hidden layer\n",
    "    z1 = W1 @ x + b1\n",
    "    hidden = np.maximum(0, z1)  # ReLU\n",
    "    \n",
    "    # Output layer\n",
    "    z2 = W2.T @ hidden + b2\n",
    "    output = sigmoid(z2[0])\n",
    "    \n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 9.4: Activation Function Comparison (Medium)\n",
    "\n",
    "Compare different activation functions for the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations_on_xor():\n",
    "    \"\"\"\n",
    "    Test XOR with different activation functions\n",
    "    \"\"\"\n",
    "    # Same architecture, different activations\n",
    "    activations = {\n",
    "        'sigmoid': sigmoid,\n",
    "        'relu': lambda z: np.maximum(0, z),\n",
    "        'tanh': lambda z: np.tanh(z),\n",
    "        'step': lambda z: np.where(z >= 0, 1, 0)\n",
    "    }\n",
    "    \n",
    "    # Test each activation\n",
    "    # TODO: Implement and compare!\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "1. Which activation works best for XOR?\n",
    "2. Why might step function fail?\n",
    "3. What's the advantage of ReLU over sigmoid?\n",
    "\n",
    "<details>\n",
    "<summary>Discussion</summary>\n",
    "\n",
    "1. **Sigmoid and tanh** work well for XOR (smooth, differentiable)\n",
    "2. **Step function** is problematic because:\n",
    "   - Not differentiable\n",
    "   - Can't use gradient-based learning (coming in Session 5!)\n",
    "3. **ReLU advantages:**\n",
    "   - Computationally faster (just max(0,z))\n",
    "   - Avoids vanishing gradient problem\n",
    "   - Works well in practice for deep networks\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Exercise 9.5: The Universal Approximation Intuition (Hard)\n",
    "\n",
    "**Theoretical question:**\n",
    "\n",
    "The Universal Approximation Theorem states: \"A neural network with one hidden layer can approximate any continuous function to arbitrary accuracy (given enough hidden neurons).\"\n",
    "\n",
    "**Thought experiment:**\n",
    "\n",
    "1. Draw a complex wavy function $f(x)$ on paper\n",
    "2. How could you approximate it with:\n",
    "   - Many small step functions?\n",
    "   - Combining many ReLU neurons?\n",
    "\n",
    "3. For a 2D input function $f(x_1, x_2)$, how does adding more hidden neurons help?\n",
    "\n",
    "**Hint:** Think about how each hidden neuron creates a \"basis function\" that the output layer combines.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **XOR Problem**: Single perceptrons can't solve it (not linearly separable)  \n",
    "‚úÖ **Hidden Layers**: Multiple perceptrons working together can solve non-linear problems  \n",
    "‚úÖ **MLP Architecture**: Input ‚Üí Hidden(s) ‚Üí Output  \n",
    "‚úÖ **Forward Propagation**: Computing outputs layer by layer  \n",
    "‚úÖ **Hidden Layer Role**: Each neuron detects different features  \n",
    "‚úÖ **Manual Tuning**: Impractical and doesn't scale  \n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Non-linearity comes from:**\n",
    "   - Multiple linear boundaries (hidden neurons)\n",
    "   - Non-linear activations (sigmoid, ReLU)\n",
    "   - Combination at output layer\n",
    "\n",
    "2. **Forward propagation is simple:**\n",
    "   - Just weighted sums + activations\n",
    "   - Repeat for each layer\n",
    "   - Easy to implement!\n",
    "\n",
    "3. **The hard part:**\n",
    "   - Finding good weights\n",
    "   - Need automatic training\n",
    "   - ‚Üí Motivation for gradient descent!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 3: The Mathematics of Learning**\n",
    "\n",
    "In the next session, we'll learn:\n",
    "- **Loss functions**: How to measure \"how wrong\" we are\n",
    "- **Gradient descent**: How to adjust weights systematically\n",
    "- **Derivatives**: The math we need (don't worry, we'll build it up!)\n",
    "\n",
    "**The goal:** Automatically train our networks instead of guessing weights!\n",
    "\n",
    "### Before Next Session\n",
    "\n",
    "**Think about:**\n",
    "1. How would you measure if your weights are \"good\" or \"bad\"?\n",
    "2. If you could see which direction to move each weight, how would you use that information?\n",
    "3. What does \"learning\" mean mathematically?\n",
    "\n",
    "**Optional reading:**\n",
    "- 3Blue1Brown: \"But what is a neural network?\" (YouTube)\n",
    "- Visualization: https://playground.tensorflow.org/ (play with the interactive network!)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 4** üéì\n",
    "\n",
    "**You now understand:**\n",
    "- ‚úÖ Multi-layer networks solve non-linear problems\n",
    "- ‚úÖ Forward propagation computes predictions\n",
    "- ‚úÖ We need automatic training methods\n",
    "\n",
    "**Next up:** Making networks learn by themselves! üöÄ"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
