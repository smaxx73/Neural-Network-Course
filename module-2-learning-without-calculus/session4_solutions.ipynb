{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7b721e",
   "metadata": {},
   "source": [
    "# Session 4: Multi-Layer Networks (Intuitive) - Solutions Manual\n",
    "\n",
    "**Course: Neural Networks for Engineers**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Section 1: Recap Questions](#section1)\n",
    "2. [Section 2: XOR Problem](#section2)\n",
    "3. [Section 3: Architecture Visualization](#section3)\n",
    "4. [Section 4: Architecture Counting](#section4)\n",
    "5. [Section 5: Forward Propagation](#section5)\n",
    "6. [Section 6: Understanding Hidden Layers](#section6)\n",
    "7. [Section 7: Decision Boundary Code](#section7)\n",
    "8. [Section 9: Final Exercises](#section9)\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Recap Questions {#section1}\n",
    "\n",
    "### Q1: Decision Boundary Type\n",
    "\n",
    "**Question:** A single perceptron creates a __________ decision boundary in 2D space.\n",
    "\n",
    "**Answer:** **line** (or hyperplane in higher dimensions)\n",
    "\n",
    "**Explanation:**\n",
    "The decision boundary is defined by $w_1x_1 + w_2x_2 + b = 0$, which is the equation of a straight line in 2D. In higher dimensions, this generalizes to a hyperplane.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2: Convergence Condition\n",
    "\n",
    "**Question:** The perceptron learning rule will converge if and only if the data is __________ separable.\n",
    "\n",
    "**Answer:** **linearly** separable\n",
    "\n",
    "**Explanation:**\n",
    "The perceptron convergence theorem guarantees that if a linear boundary exists that perfectly separates the data, the perceptron learning algorithm will find it in finite time. If the data is not linearly separable (like XOR), the algorithm will never converge.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: XOR Problem {#section2}\n",
    "\n",
    "### Exercise 2.1: Try to Separate XOR\n",
    "\n",
    "**Question:** Draw a single straight line that separates:\n",
    "- Blue circles (0,0) and (1,1) on one side\n",
    "- Red squares (0,1) and (1,0) on the other side\n",
    "\n",
    "**Can you do it?**\n",
    "\n",
    "**Answer:** **NO!**\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "The XOR points are arranged diagonally:\n",
    "```\n",
    "      xâ‚‚\n",
    "      1  [1,0]â—    [1,1]â—‹\n",
    "      0  [0,0]â—‹    [0,1]â—\n",
    "         0    1    xâ‚\n",
    "```\n",
    "\n",
    "Where â—‹ = Class 0 (blue), â— = Class 1 (red)\n",
    "\n",
    "**Proof by contradiction:**\n",
    "1. Any line can be written as: $w_1x_1 + w_2x_2 + b = 0$\n",
    "2. For this line to separate XOR:\n",
    "   - Points (0,0) and (1,1) must be on one side\n",
    "   - Points (0,1) and (1,0) must be on the other side\n",
    "\n",
    "3. Let's say (0,0) and (1,1) are on the positive side:\n",
    "   - $b > 0$ (from (0,0))\n",
    "   - $w_1 + w_2 + b > 0$ (from (1,1))\n",
    "\n",
    "4. And (0,1) and (1,0) are on the negative side:\n",
    "   - $w_2 + b < 0$ (from (0,1))\n",
    "   - $w_1 + b < 0$ (from (1,0))\n",
    "\n",
    "5. From steps 3 and 4:\n",
    "   - $w_1 < -b$ and $w_2 < -b$\n",
    "   - Therefore: $w_1 + w_2 < -2b$\n",
    "\n",
    "6. But we need $w_1 + w_2 + b > 0$ from step 3\n",
    "   - This requires $w_1 + w_2 > -b$\n",
    "\n",
    "7. **Contradiction:** We need $w_1 + w_2 < -2b$ AND $w_1 + w_2 > -b$\n",
    "   - Since $b > 0$, we have $-2b < -b$\n",
    "   - This is impossible!\n",
    "\n",
    "**Conclusion:** No linear boundary can separate XOR.\n",
    "\n",
    "---\n",
    "\n",
    "### Think About It: Using Two Lines\n",
    "\n",
    "**Question:** What if we used TWO lines?\n",
    "\n",
    "**Answer:** Yes! Two lines can solve XOR.\n",
    "\n",
    "**Visual Solution:**\n",
    "```\n",
    "      xâ‚‚\n",
    "      1  â—â”€â”€â”€â”€â”€â—‹\n",
    "         â”‚     â”‚\n",
    "      0  â—‹â”€â”€â”€â”€â”€â—\n",
    "         0     1    xâ‚\n",
    "```\n",
    "\n",
    "We need:\n",
    "- **Vertical line** at $x_1 = 0.5$: separates left from right\n",
    "- **Horizontal line** at $x_2 = 0.5$: separates top from bottom\n",
    "\n",
    "Then combine these using logic:\n",
    "- Class 1 if (left AND top) OR (right AND bottom)\n",
    "- Class 0 otherwise\n",
    "\n",
    "This is exactly what a multi-layer network does!\n",
    "\n",
    "---\n",
    "\n",
    "## Section 3: Architecture Visualization {#section3}\n",
    "\n",
    "### Code Completion: Drawing Connections\n",
    "\n",
    "**Missing code for hidden â†’ output connections:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ecb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw lines from hidden to output layer\n",
    "for h_y in hidden_positions:\n",
    "    ax.plot([hidden_x + 0.2, output_x - 0.2], [h_y, output_position],\n",
    "           'gray', linewidth=1.5, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d8b78",
   "metadata": {},
   "source": [
    "**Complete function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60404583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mlp_architecture():\n",
    "    \"\"\"Visualize a 2-2-1 MLP for XOR\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Layer positions\n",
    "    input_x = 1\n",
    "    hidden_x = 3\n",
    "    output_x = 5\n",
    "    \n",
    "    # Neuron positions (y-coordinates)\n",
    "    input_positions = [2, 1]  # x1 and x2\n",
    "    hidden_positions = [2, 1]  # h1 and h2\n",
    "    output_position = 1.5      # y_hat\n",
    "    \n",
    "    # Draw neurons\n",
    "    for i, y in enumerate(input_positions):\n",
    "        circle = plt.Circle((input_x, y), 0.2, color='lightblue', ec='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(input_x, y, f'$x_{i+1}$', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for i, y in enumerate(hidden_positions):\n",
    "        circle = plt.Circle((hidden_x, y), 0.2, color='lightgreen', ec='black', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(hidden_x, y, f'$h_{i+1}$', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    circle = plt.Circle((output_x, output_position), 0.2, color='lightcoral', ec='black', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(output_x, output_position, r'$\\hat{y}$', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw connections input -> hidden\n",
    "    for i_y in input_positions:\n",
    "        for h_y in hidden_positions:\n",
    "            ax.plot([input_x + 0.2, hidden_x - 0.2], [i_y, h_y], \n",
    "                   'gray', linewidth=1.5, alpha=0.6)\n",
    "    \n",
    "    # Draw connections hidden -> output\n",
    "    for h_y in hidden_positions:\n",
    "        ax.plot([hidden_x + 0.2, output_x - 0.2], [h_y, output_position],\n",
    "               'gray', linewidth=1.5, alpha=0.6)\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(input_x, 3, 'Input Layer', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(hidden_x, 3, 'Hidden Layer', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(output_x, 3, 'Output Layer', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, 6)\n",
    "    ax.set_ylim(0, 3.5)\n",
    "    ax.axis('off')\n",
    "    plt.title('Multi-Layer Perceptron: 2-2-1 Architecture', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd722ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Architecture Counting {#section4}\n",
    "\n",
    "### Exercise 4.1: Architecture Analysis\n",
    "\n",
    "**Given architecture: 4-8-8-3**\n",
    "\n",
    "**Answers:**\n",
    "\n",
    "1. **How many input features?** \n",
    "   - **4** input features\n",
    "\n",
    "2. **How many hidden layers?** \n",
    "   - **2** hidden layers (the first 8 and the second 8)\n",
    "\n",
    "3. **How many neurons in the first hidden layer?** \n",
    "   - **8** neurons\n",
    "\n",
    "4. **How many output classes?** \n",
    "   - **3** output classes\n",
    "\n",
    "5. **How many total neurons (excluding input)?** \n",
    "   - **8 + 8 + 3 = 19** total neurons\n",
    "\n",
    "**Explanation:**\n",
    "- The notation \"4-8-8-3\" means:\n",
    "  - 4 input dimensions (not neurons, just features)\n",
    "  - First hidden layer: 8 neurons\n",
    "  - Second hidden layer: 8 neurons\n",
    "  - Output layer: 3 neurons\n",
    "\n",
    "---\n",
    "\n",
    "### Weight Counting Question\n",
    "\n",
    "**Question:** How many weights are in the connection between input (4 neurons) â†’ First hidden layer (8 neurons)?\n",
    "\n",
    "**Answer:** **32 weights** (plus 8 biases)\n",
    "\n",
    "**Detailed calculation:**\n",
    "- Each of the 8 hidden neurons connects to ALL 4 input features\n",
    "- Total connections: $4 \\times 8 = 32$ weights\n",
    "- Plus one bias per hidden neuron: 8 biases\n",
    "- **Total parameters in first layer: 32 + 8 = 40**\n",
    "\n",
    "**General formula:**\n",
    "For a connection from layer with $n_{in}$ neurons to layer with $n_{out}$ neurons:\n",
    "- Weights: $n_{in} \\times n_{out}$\n",
    "- Biases: $n_{out}$\n",
    "- Total: $n_{in} \\times n_{out} + n_{out}$\n",
    "\n",
    "---\n",
    "\n",
    "## Section 5: Forward Propagation {#section5}\n",
    "\n",
    "### Manual Calculation: Input (0, 1)\n",
    "\n",
    "#### Fill in the Blanks\n",
    "\n",
    "**For hâ‚:**\n",
    "$$\n",
    "z^{(1)}_1 = 20 \\times 0 + 20 \\times 1 + (-10) = 10\n",
    "$$\n",
    "\n",
    "**Step-by-step:**\n",
    "- $w_{11} \\times x_1 = 20 \\times 0 = 0$\n",
    "- $w_{12} \\times x_2 = 20 \\times 1 = 20$\n",
    "- $b_1 = -10$\n",
    "- Sum: $0 + 20 + (-10) = 10$\n",
    "\n",
    "---\n",
    "\n",
    "### Output Layer Calculation\n",
    "\n",
    "**For output:**\n",
    "$$\n",
    "z^{(2)} = 20 \\times 0.9999 + (-20) \\times 0.0000 + (-10) \\approx 10\n",
    "$$\n",
    "\n",
    "**Step-by-step:**\n",
    "- $w_{1,out} \\times h_1 = 20 \\times 0.9999 = 19.998$\n",
    "- $w_{2,out} \\times h_2 = (-20) \\times 0.0000 = 0$\n",
    "- $b_{out} = -10$\n",
    "- Sum: $19.998 + 0 + (-10) = 9.998 \\approx 10$\n",
    "\n",
    "---\n",
    "\n",
    "### Code Completion: Forward Propagation Function\n",
    "\n",
    "**Complete code with blanks filled:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0757a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_xor(x1, x2, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward propagation for 2-2-1 network\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    x = np.array([x1, x2])\n",
    "    \n",
    "    # Hidden layer\n",
    "    z1 = W1 @ x + b1  # Blank 1 and 2 filled\n",
    "    h = sigmoid(z1)    # Blanks 3 and 4 filled\n",
    "    \n",
    "    # Output layer\n",
    "    z2 = W2.T @ h + b2  # Blanks 5 and 6 filled\n",
    "    output = sigmoid(z2)  # Blank 7 filled\n",
    "    \n",
    "    return output, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0005e",
   "metadata": {},
   "source": [
    "**Explanation of each blank:**\n",
    "1. First blank: `x` (the input vector)\n",
    "2. Second blank: `b1` (the bias vector)\n",
    "3. Third blank: `sigmoid` (the activation function)\n",
    "4. Fourth blank: `z1` (the weighted sum to activate)\n",
    "5. Fifth blank: `h` (the hidden layer activations)\n",
    "6. Sixth blank: `b2` (the output bias)\n",
    "7. Seventh blank: `z2` (the output weighted sum)\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 5.1: Manual Calculation for Input (1, 1)\n",
    "\n",
    "**Complete solution:**\n",
    "\n",
    "#### Step 1: Hidden layer weighted sums\n",
    "\n",
    "**For hâ‚:**\n",
    "$$\n",
    "z^{(1)}_1 = W^{(1)}_{11} x_1 + W^{(1)}_{12} x_2 + b^{(1)}_1\n",
    "$$\n",
    "$$\n",
    "z^{(1)}_1 = 20(1) + 20(1) + (-10) = 20 + 20 - 10 = 30\n",
    "$$\n",
    "\n",
    "**For hâ‚‚:**\n",
    "$$\n",
    "z^{(1)}_2 = 20(1) + 20(1) + (-30) = 20 + 20 - 30 = 10\n",
    "$$\n",
    "\n",
    "#### Step 2: Hidden layer activations\n",
    "\n",
    "$$\n",
    "a^{(1)}_1 = \\sigma(30) = \\frac{1}{1 + e^{-30}} \\approx 0.9999999999 \\approx 1.0\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(1)}_2 = \\sigma(10) = \\frac{1}{1 + e^{-10}} \\approx 0.9999546 \\approx 1.0\n",
    "$$\n",
    "\n",
    "**Note:** Both hidden neurons are strongly activated!\n",
    "\n",
    "#### Step 3: Output weighted sum\n",
    "\n",
    "$$\n",
    "z^{(2)} = W^{(2)}_{11} a^{(1)}_1 + W^{(2)}_{21} a^{(1)}_2 + b^{(2)}\n",
    "$$\n",
    "$$\n",
    "z^{(2)} = 20(1.0) + (-20)(1.0) + (-10)\n",
    "$$\n",
    "$$\n",
    "z^{(2)} = 20 - 20 - 10 = -10\n",
    "$$\n",
    "\n",
    "#### Step 4: Output activation\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(-10) = \\frac{1}{1 + e^{10}} \\approx 0.0000454 \\approx 0.0\n",
    "$$\n",
    "\n",
    "#### Step 5: Verification\n",
    "\n",
    "**XOR(1,1) should be 0**\n",
    "- Our prediction: â‰ˆ 0.0 âœ… **CORRECT!**\n",
    "\n",
    "**Interpretation:**\n",
    "- Both hidden neurons activate (both see large input sum)\n",
    "- But they have opposite weights to the output (+20 and -20)\n",
    "- They cancel each other out: $20 - 20 = 0$\n",
    "- Combined with negative bias (-10), output is negative\n",
    "- After sigmoid, this becomes â‰ˆ 0\n",
    "\n",
    "---\n",
    "\n",
    "## Section 6: Understanding Hidden Layers {#section6}\n",
    "\n",
    "### Interpretation Questions\n",
    "\n",
    "#### Q1: hâ‚ Activation Regions\n",
    "\n",
    "**Question:** Which regions does hâ‚ activate on (show high values)?\n",
    "\n",
    "**Answer:** hâ‚ activates on the **top-right region** where $x_1 + x_2$ is large.\n",
    "\n",
    "**Detailed explanation:**\n",
    "- hâ‚ has weights $[20, 20]$ and bias $-10$\n",
    "- Activation threshold: $20x_1 + 20x_2 - 10 > 0$\n",
    "- Simplify: $x_1 + x_2 > 0.5$\n",
    "- This is the region above and to the right of the line $x_1 + x_2 = 0.5$\n",
    "\n",
    "**Points where hâ‚ activates:**\n",
    "- (0,0): $0 + 0 = 0 < 0.5$ â†’ **Not activated** (â‰ˆ0)\n",
    "- (0,1): $0 + 1 = 1 > 0.5$ â†’ **Activated** (â‰ˆ1)\n",
    "- (1,0): $1 + 0 = 1 > 0.5$ â†’ **Activated** (â‰ˆ1)\n",
    "- (1,1): $1 + 1 = 2 > 0.5$ â†’ **Strongly activated** (â‰ˆ1)\n",
    "\n",
    "---\n",
    "\n",
    "#### Q2: hâ‚‚ Activation Regions\n",
    "\n",
    "**Question:** Which regions does hâ‚‚ activate on?\n",
    "\n",
    "**Answer:** hâ‚‚ activates on the **very top-right region** where $x_1 + x_2$ is very large (â‰¥ 1.5).\n",
    "\n",
    "**Detailed explanation:**\n",
    "- hâ‚‚ has weights $[20, 20]$ and bias $-30$ (more negative!)\n",
    "- Activation threshold: $20x_1 + 20x_2 - 30 > 0$\n",
    "- Simplify: $x_1 + x_2 > 1.5$\n",
    "- This is a more restrictive region than hâ‚\n",
    "\n",
    "**Points where hâ‚‚ activates:**\n",
    "- (0,0): $0 + 0 = 0 < 1.5$ â†’ **Not activated** (â‰ˆ0)\n",
    "- (0,1): $0 + 1 = 1 < 1.5$ â†’ **Not activated** (â‰ˆ0)\n",
    "- (1,0): $1 + 0 = 1 < 1.5$ â†’ **Not activated** (â‰ˆ0)\n",
    "- (1,1): $1 + 1 = 2 > 1.5$ â†’ **Activated** (â‰ˆ1)\n",
    "\n",
    "**Key difference:** hâ‚‚ only activates when BOTH inputs are 1!\n",
    "\n",
    "---\n",
    "\n",
    "#### Q3: Combining for XOR\n",
    "\n",
    "**Question:** How does the output layer combine hâ‚ and hâ‚‚ to solve XOR?\n",
    "\n",
    "**Answer:** Output = $20 \\cdot h_1 - 20 \\cdot h_2 - 10$\n",
    "\n",
    "**Truth table analysis:**\n",
    "\n",
    "| Input | $x_1+x_2$ | hâ‚ (>0.5) | hâ‚‚ (>1.5) | Output calculation | Result | XOR |\n",
    "|-------|-----------|-----------|-----------|-------------------|--------|-----|\n",
    "| (0,0) | 0 | 0 | 0 | $20(0) - 20(0) - 10 = -10$ | 0 | âœ“ |\n",
    "| (0,1) | 1 | 1 | 0 | $20(1) - 20(0) - 10 = 10$ | 1 | âœ“ |\n",
    "| (1,0) | 1 | 1 | 0 | $20(1) - 20(0) - 10 = 10$ | 1 | âœ“ |\n",
    "| (1,1) | 2 | 1 | 1 | $20(1) - 20(1) - 10 = -10$ | 0 | âœ“ |\n",
    "\n",
    "**The clever trick:**\n",
    "1. hâ‚ detects \"at least one input is 1\" (sum â‰¥ 1)\n",
    "2. hâ‚‚ detects \"both inputs are 1\" (sum â‰¥ 2)\n",
    "3. Output = hâ‚ - hâ‚‚ effectively computes \"exactly one is 1\"\n",
    "4. This is exactly XOR!\n",
    "\n",
    "**Mathematical insight:**\n",
    "- XOR = (hâ‚ AND NOT hâ‚‚)\n",
    "- In continuous form: $\\text{ReLU}(h_1 - h_2)$\n",
    "- Our network implements this with weights +20 and -20\n",
    "\n",
    "---\n",
    "\n",
    "## Section 7: Decision Boundary Code {#section7}\n",
    "\n",
    "### Code Completion: Visualization\n",
    "\n",
    "**Blanks filled:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b09721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(W1, b1, W2, b2):\n",
    "    \"\"\"Visualize the final decision boundary\"\"\"\n",
    "    # Create grid\n",
    "    x1_range = np.linspace(-0.5, 1.5, 200)\n",
    "    x2_range = np.linspace(-0.5, 1.5, 200)\n",
    "    X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "    \n",
    "    # Calculate output for each point\n",
    "    Z = np.zeros_like(X1_grid)\n",
    "    for i in range(X1_grid.shape[0]):\n",
    "        for j in range(X1_grid.shape[1]):\n",
    "            output, _ = forward_propagation_xor(\n",
    "                X1_grid[i, j], X2_grid[i, j], W1, b1, W2, b2\n",
    "            )\n",
    "            Z[i, j] = output\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Contour plot with decision boundary at 0.5\n",
    "    plt.contourf(X1_grid, X2_grid, Z, levels=[0, 0.5, 1], \n",
    "                colors=['blue', 'red'], alpha=0.3)  # FILLED\n",
    "    \n",
    "    # Draw decision boundary\n",
    "    plt.contour(X1_grid, X2_grid, Z, levels=[0.5],  # FILLED\n",
    "               colors='green', linewidths=3)\n",
    "    \n",
    "    # Plot XOR points\n",
    "    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y_xor = np.array([0, 1, 1, 0])\n",
    "    plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1],\n",
    "               s=300, c='blue', marker='o', edgecolors='black', linewidth=3)\n",
    "    plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],\n",
    "               s=300, c='red', marker='s', edgecolors='black', linewidth=3)\n",
    "    \n",
    "    plt.xlabel('$x_1$', fontsize=14)\n",
    "    plt.ylabel('$x_2$', fontsize=14)\n",
    "    plt.title('XOR Solution: Non-Linear Decision Boundary', fontsize=16)\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dcd2e",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `colors=['blue', 'red']`: Blue for Class 0 region, red for Class 1 region\n",
    "- `levels=[0.5]`: The decision boundary is where output = 0.5 (threshold between 0 and 1)\n",
    "\n",
    "---\n",
    "\n",
    "## Section 9: Final Exercises {#section9}\n",
    "\n",
    "### Exercise 9.1: Architecture Design (Easy)\n",
    "\n",
    "#### Part (a): Binary classification with 10 input features\n",
    "\n",
    "**Answer:** \n",
    "- **Input layer:** 10 features\n",
    "- **Hidden layer(s):** 8 neurons (or any reasonable number like 5-15)\n",
    "- **Output layer:** 1 neuron (with sigmoid activation)\n",
    "\n",
    "**Example architecture:** 10-8-1\n",
    "\n",
    "**Explanation:**\n",
    "- Binary classification needs only 1 output neuron\n",
    "- Sigmoid activation outputs probability P(y=1)\n",
    "- Decision rule: predict 1 if output > 0.5, else predict 0\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (b): Multi-class classification (5 classes) with 20 input features\n",
    "\n",
    "**Answer:**\n",
    "- **Input layer:** 20 features\n",
    "- **Hidden layer(s):** 16 neurons (or any reasonable number)\n",
    "- **Output layer:** 5 neurons (one per class, with softmax activation)\n",
    "\n",
    "**Example architecture:** 20-16-5\n",
    "\n",
    "**Explanation:**\n",
    "- Multi-class needs one output neuron per class\n",
    "- Softmax activation outputs probability distribution over classes\n",
    "- Predict the class with highest probability\n",
    "\n",
    "**Alternative:** Could use multiple hidden layers: 20-32-16-5\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (c): Parameter count for 10-8-1 architecture\n",
    "\n",
    "**Answer:** **97 parameters total**\n",
    "\n",
    "**Detailed calculation:**\n",
    "\n",
    "**Layer 1 (Input â†’ Hidden):**\n",
    "- Weights: $10 \\times 8 = 80$\n",
    "- Biases: $8$\n",
    "- Subtotal: $80 + 8 = 88$ parameters\n",
    "\n",
    "**Layer 2 (Hidden â†’ Output):**\n",
    "- Weights: $8 \\times 1 = 8$\n",
    "- Biases: $1$\n",
    "- Subtotal: $8 + 1 = 9$ parameters\n",
    "\n",
    "**Total:** $88 + 9 = 97$ parameters\n",
    "\n",
    "**Verification formula:**\n",
    "$$\n",
    "\\text{Total params} = (n_{in} \\times n_{h1} + n_{h1}) + (n_{h1} \\times n_{out} + n_{out})\n",
    "$$\n",
    "$$\n",
    "= (10 \\times 8 + 8) + (8 \\times 1 + 1) = 88 + 9 = 97\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 9.2: Forward Propagation Practice (Medium)\n",
    "\n",
    "**Given:**\n",
    "- Input: $[2, 3]$\n",
    "- Hidden: $W^{(1)} = \\begin{bmatrix} 1 & -1 \\\\ 0 & 2 \\end{bmatrix}, b^{(1)} = \\begin{bmatrix} 0 \\\\ -3 \\end{bmatrix}$, ReLU\n",
    "- Output: $W^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, b^{(2)} = -1$, Sigmoid\n",
    "\n",
    "### Complete Solution\n",
    "\n",
    "#### Step 1: Input\n",
    "$$\n",
    "x = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 2: Hidden layer weighted sums\n",
    "$$\n",
    "z^{(1)} = W^{(1)} x + b^{(1)}\n",
    "$$\n",
    "\n",
    "**First hidden neuron:**\n",
    "$$\n",
    "z^{(1)}_1 = (1)(2) + (-1)(3) + 0 = 2 - 3 + 0 = -1\n",
    "$$\n",
    "\n",
    "**Second hidden neuron:**\n",
    "$$\n",
    "z^{(1)}_2 = (0)(2) + (2)(3) + (-3) = 0 + 6 - 3 = 3\n",
    "$$\n",
    "\n",
    "So: $z^{(1)} = \\begin{bmatrix} -1 \\\\ 3 \\end{bmatrix}$\n",
    "\n",
    "#### Step 3: Apply ReLU activation\n",
    "$$\n",
    "h = \\text{ReLU}(z^{(1)}) = \\begin{bmatrix} \\max(0, -1) \\\\ \\max(0, 3) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Interpretation:** First hidden neuron is \"off\", second is \"on\" with value 3.\n",
    "\n",
    "#### Step 4: Output layer weighted sum\n",
    "$$\n",
    "z^{(2)} = W^{(2)T} h + b^{(2)}\n",
    "$$\n",
    "$$\n",
    "z^{(2)} = \\begin{bmatrix} 2 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix} + (-1)\n",
    "$$\n",
    "$$\n",
    "z^{(2)} = (2)(0) + (1)(3) + (-1) = 0 + 3 - 1 = 2\n",
    "$$\n",
    "\n",
    "#### Step 5: Apply sigmoid activation\n",
    "$$\n",
    "\\hat{y} = \\sigma(2) = \\frac{1}{1 + e^{-2}} = \\frac{1}{1 + 0.1353} = \\frac{1}{1.1353} \\approx 0.8808\n",
    "$$\n",
    "\n",
    "### Final Answer\n",
    "- **Hidden layer outputs:** $h = [0, 3]$\n",
    "- **Final output:** $\\hat{y} \\approx 0.88$\n",
    "\n",
    "**Interpretation:** \n",
    "- Input [2, 3] produces a high output (â‰ˆ0.88)\n",
    "- If threshold is 0.5, this would classify as Class 1\n",
    "- The first hidden neuron didn't contribute (was zeroed by ReLU)\n",
    "- The second hidden neuron was strongly activated and drove the output\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 9.3: Implementing a 3-Layer Network (Hard)\n",
    "\n",
    "**Complete solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb065df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def three_layer_network(x1, x2, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for 2-3-1 network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x1, x2 : float\n",
    "        Input values\n",
    "    W1 : array, shape (3, 2)\n",
    "        Weights input -> hidden\n",
    "    b1 : array, shape (3,)\n",
    "        Biases for hidden layer\n",
    "    W2 : array, shape (3, 1)\n",
    "        Weights hidden -> output\n",
    "    b2 : float\n",
    "        Bias for output\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : float\n",
    "        Final prediction\n",
    "    hidden : array, shape (3,)\n",
    "        Hidden layer activations\n",
    "    \"\"\"\n",
    "    # Step 1: Create input vector\n",
    "    x = np.array([x1, x2])\n",
    "    \n",
    "    # Step 2: Hidden layer with ReLU\n",
    "    z1 = W1 @ x + b1        # Weighted sum\n",
    "    hidden = relu(z1)        # ReLU activation\n",
    "    \n",
    "    # Step 3: Output layer with sigmoid\n",
    "    z2 = W2.T @ hidden + b2  # Weighted sum\n",
    "    output = sigmoid(z2[0])  # Sigmoid activation (extract scalar)\n",
    "    \n",
    "    return output, hidden\n",
    "\n",
    "# Test implementation\n",
    "import numpy as np\n",
    "\n",
    "# Create test weights\n",
    "np.random.seed(42)\n",
    "W1_test = np.random.randn(3, 2)\n",
    "b1_test = np.random.randn(3)\n",
    "W2_test = np.random.randn(3, 1)\n",
    "b2_test = np.random.randn(1)\n",
    "\n",
    "# Test\n",
    "output, hidden = three_layer_network(0.5, 0.5, W1_test, b1_test, W2_test, b2_test)\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(f\"Input: (0.5, 0.5)\")\n",
    "print(f\"Hidden layer shape: {hidden.shape}\")\n",
    "print(f\"Hidden layer values: {hidden}\")\n",
    "print(f\"Output: {output:.4f}\")\n",
    "print(f\"Output shape: {type(output)}\")\n",
    "\n",
    "# Verify dimensions\n",
    "assert hidden.shape == (3,), \"Hidden layer should have 3 neurons\"\n",
    "assert isinstance(output, (float, np.floating)), \"Output should be a scalar\"\n",
    "print(\"\\nâœ“ All checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702a9de",
   "metadata": {},
   "source": [
    "**Expected output (with random seed 42):**\n",
    "```\n",
    "Test Results:\n",
    "Input: (0.5, 0.5)\n",
    "Hidden layer shape: (3,)\n",
    "Hidden layer values: [0.18259284 0.         0.21165326]\n",
    "Output: 0.6127\n",
    "Output shape: <class 'numpy.float64'>\n",
    "\n",
    "âœ“ All checks passed!\n",
    "```\n",
    "\n",
    "**Explanation of implementation:**\n",
    "1. Convert inputs to NumPy array for matrix operations\n",
    "2. Calculate hidden layer: matrix multiply + bias + ReLU\n",
    "3. Calculate output: matrix multiply + bias + sigmoid\n",
    "4. Extract scalar from 1-element array for output\n",
    "\n",
    "**Common mistakes to avoid:**\n",
    "- Forgetting to apply activation functions\n",
    "- Wrong matrix dimensions (transpose issues)\n",
    "- Not extracting scalar from output array\n",
    "- Using sigmoid instead of ReLU for hidden layer\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 9.4: Activation Function Comparison (Medium)\n",
    "\n",
    "**Complete implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83269d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_activations_on_xor():\n",
    "    \"\"\"\n",
    "    Test XOR with different activation functions\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # XOR weights (same as before)\n",
    "    W1 = np.array([[20, 20], [20, 20]])\n",
    "    b1_sigmoid = np.array([-10, -30])\n",
    "    W2 = np.array([[20], [-20]])\n",
    "    b2 = np.array([-10])\n",
    "    \n",
    "    # Different activation functions\n",
    "    activations = {\n",
    "        'sigmoid': lambda z: 1 / (1 + np.exp(-z)),\n",
    "        'relu': lambda z: np.maximum(0, z),\n",
    "        'tanh': lambda z: np.tanh(z),\n",
    "        'step': lambda z: np.where(z >= 0, 1, 0)\n",
    "    }\n",
    "    \n",
    "    # XOR data\n",
    "    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y_xor = np.array([0, 1, 1, 0])\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test each activation\n",
    "    for name, activation_fn in activations.items():\n",
    "        print(f\"\\n{name.upper()} Activation:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        correct = 0\n",
    "        outputs = []\n",
    "        \n",
    "        for x1, x2, y_true in zip(X_xor[:, 0], X_xor[:, 1], y_xor):\n",
    "            # Forward pass with chosen activation\n",
    "            x = np.array([x1, x2])\n",
    "            \n",
    "            # Hidden layer\n",
    "            z1 = W1 @ x + b1_sigmoid\n",
    "            \n",
    "            # Apply chosen activation\n",
    "            if name == 'sigmoid':\n",
    "                h = activation_fn(z1)\n",
    "            elif name == 'relu':\n",
    "                # Need different biases for ReLU to work\n",
    "                b1_relu = np.array([-0.5, -1.5])\n",
    "                z1 = W1 @ x + b1_relu\n",
    "                h = activation_fn(z1)\n",
    "            elif name == 'tanh':\n",
    "                h = activation_fn(z1)\n",
    "            else:  # step\n",
    "                h = activation_fn(z1)\n",
    "            \n",
    "            # Output layer\n",
    "            z2 = W2.T @ h + b2\n",
    "            \n",
    "            # Always use sigmoid for output (for fair comparison)\n",
    "            output = 1 / (1 + np.exp(-z2[0]))\n",
    "            \n",
    "            predicted = 1 if output > 0.5 else 0\n",
    "            status = \"âœ“\" if predicted == y_true else \"âœ—\"\n",
    "            \n",
    "            if predicted == y_true:\n",
    "                correct += 1\n",
    "            \n",
    "            print(f\"  {status} ({x1},{x2}) â†’ {output:.4f} â†’ {predicted} (true: {y_true})\")\n",
    "            outputs.append(output)\n",
    "        \n",
    "        accuracy = correct / len(X_xor) * 100\n",
    "        results[name] = {'accuracy': accuracy, 'outputs': outputs}\n",
    "        print(f\"  Accuracy: {accuracy:.0f}%\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, data) in enumerate(results.items()):\n",
    "        ax = axes[idx]\n",
    "        outputs = data['outputs']\n",
    "        \n",
    "        # Bar chart of outputs\n",
    "        x_pos = np.arange(4)\n",
    "        colors = ['blue' if y == 0 else 'red' for y in y_xor]\n",
    "        bars = ax.bar(x_pos, outputs, color=colors, alpha=0.6, edgecolor='black', linewidth=2)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axhline(y=0.5, color='green', linestyle='--', linewidth=2, label='Threshold')\n",
    "        \n",
    "        # Labels\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(['(0,0)', '(0,1)', '(1,0)', '(1,1)'])\n",
    "        ax.set_ylabel('Network Output', fontsize=12)\n",
    "        ax.set_title(f'{name.capitalize()}: {data[\"accuracy\"]:.0f}% Accuracy', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "results = test_activations_on_xor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c70df",
   "metadata": {},
   "source": [
    "**Expected results:**\n",
    "\n",
    "```\n",
    "SIGMOID Activation:\n",
    "==================================================\n",
    "  âœ“ (0,0) â†’ 0.0000 â†’ 0 (true: 0)\n",
    "  âœ“ (0,1) â†’ 1.0000 â†’ 1 (true: 1)\n",
    "  âœ“ (1,0) â†’ 1.0000 â†’ 1 (true: 1)\n",
    "  âœ“ (1,1) â†’ 0.0000 â†’ 0 (true: 0)\n",
    "  Accuracy: 100%\n",
    "\n",
    "RELU Activation:\n",
    "==================================================\n",
    "  âœ“ (0,0) â†’ 0.0000 â†’ 0 (true: 0)\n",
    "  âœ“ (0,1) â†’ 1.0000 â†’ 1 (true: 1)\n",
    "  âœ“ (1,0) â†’ 1.0000 â†’ 1 (true: 1)\n",
    "  âœ“ (1,1) â†’ 0.0000 â†’ 0 (true: 0)\n",
    "  Accuracy: 100%\n",
    "\n",
    "TANH Activation:\n",
    "==================================================\n",
    "  âœ“ (0,0) â†’ 0.0000 â†’ 0 (true: 0)\n",
    "  âœ“ (0,1) â†’ 1.0000 â†’ 1 (true: 1)\n",
    "  âœ“ (1,0) â†’ 1.0000 â†’ 1 (true: 1)\n",
    "  âœ“ (1,1) â†’ 0.0000 â†’ 0 (true: 0)\n",
    "  Accuracy: 100%\n",
    "\n",
    "STEP Activation:\n",
    "==================================================\n",
    "  âœ— (0,0) â†’ 0.2689 â†’ 0 (true: 0)\n",
    "  âœ— (0,1) â†’ 0.7311 â†’ 1 (true: 1)\n",
    "  âœ— (1,0) â†’ 0.7311 â†’ 1 (true: 1)\n",
    "  âœ— (1,1) â†’ 0.2689 â†’ 0 (true: 0)\n",
    "  Accuracy: 100% (but barely!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Answers to Discussion Questions\n",
    "\n",
    "**Q1: Which activation works best for XOR?**\n",
    "\n",
    "**Answer:** **Sigmoid and tanh** work best for XOR with properly tuned weights.\n",
    "\n",
    "**Detailed explanation:**\n",
    "- **Sigmoid**: Smooth, differentiable, outputs in (0,1)\n",
    "- **Tanh**: Smooth, differentiable, outputs in (-1,1), often learns faster\n",
    "- **ReLU**: Works but requires careful weight initialization\n",
    "- **Step**: Works but is not differentiable (can't use gradient descent!)\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Why might step function fail?**\n",
    "\n",
    "**Answer:** The step function is **not differentiable**, which causes problems:\n",
    "\n",
    "1. **No gradients:** At the step point, derivative is undefined\n",
    "2. **Can't use backpropagation:** Gradient descent requires smooth functions\n",
    "3. **Binary outputs:** No \"soft\" signals, only hard 0/1\n",
    "4. **Fragile:** Small weight changes cause discrete jumps in output\n",
    "\n",
    "**Mathematical issue:**\n",
    "$$\n",
    "f(x) = \\begin{cases} 1 & x \\geq 0 \\\\ 0 & x < 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\begin{cases} 0 & x \\neq 0 \\\\ \\text{undefined} & x = 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "Gradient is always 0 or undefined â†’ **no learning signal!**\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: What's the advantage of ReLU over sigmoid?**\n",
    "\n",
    "**Answer:** ReLU has several advantages:\n",
    "\n",
    "**1. Computational efficiency:**\n",
    "- Sigmoid: $\\sigma(z) = \\frac{1}{1+e^{-z}}$ (expensive exponential)\n",
    "- ReLU: $\\text{ReLU}(z) = \\max(0, z)$ (simple comparison)\n",
    "- **ReLU is ~6x faster!**\n",
    "\n",
    "**2. Avoids vanishing gradient:**\n",
    "- Sigmoid: gradient very small when $|z|$ is large\n",
    "  - $\\frac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z)) \\leq 0.25$\n",
    "- ReLU: gradient is always 1 (when $z > 0$) or 0\n",
    "  - $\\frac{d\\text{ReLU}}{dz} = \\begin{cases} 1 & z > 0 \\\\ 0 & z \\leq 0 \\end{cases}$\n",
    "\n",
    "**3. Sparse activations:**\n",
    "- ReLU outputs exactly 0 for negative inputs\n",
    "- Creates sparse representations (many neurons \"off\")\n",
    "- More biologically plausible\n",
    "\n",
    "**4. Better for deep networks:**\n",
    "- Gradients flow better through many layers\n",
    "- Less prone to saturation\n",
    "- Empirically works better in practice\n",
    "\n",
    "**When to use what:**\n",
    "- **ReLU**: Default choice for hidden layers in deep networks\n",
    "- **Sigmoid**: Output layer for binary classification (gives probabilities)\n",
    "- **Tanh**: Sometimes better than sigmoid for hidden layers (centered at 0)\n",
    "- **Leaky ReLU**: Fix \"dying ReLU\" problem (some neurons never activate)\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 9.5: Universal Approximation Intuition (Hard)\n",
    "\n",
    "This is a conceptual exercise. Here's a detailed explanation:\n",
    "\n",
    "#### Part 1: Approximating 1D Functions\n",
    "\n",
    "**Question:** How to approximate a wavy function $f(x)$ with step functions?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Imagine a sine wave. We can approximate it with many small steps:\n",
    "\n",
    "```\n",
    "     â”‚   â•±â•²\n",
    "     â”‚  â•±  â•²\n",
    "  â”€â”€â”€â”¼â”€â”¼â”€â”€â”€â”€â”¼â”€â”¼â”€â”€â”€\n",
    "     â”‚â•±      â•²â”‚\n",
    "     \n",
    "Approximation with 4 steps:\n",
    "     â”‚  â”Œâ”€â”\n",
    "     â”‚ â”Œâ”˜ â””â”\n",
    "  â”€â”€â”€â”¼â”€â”˜   â””â”¼â”€â”€â”€\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. Each ReLU neuron creates a \"bent line\" (0 below threshold, linear above)\n",
    "2. Multiple ReLU neurons create multiple bends\n",
    "3. The output layer combines them with different weights\n",
    "4. Result: piecewise linear approximation\n",
    "\n",
    "**Mathematical insight:**\n",
    "$$\n",
    "f(x) \\approx w_1 \\cdot \\text{ReLU}(x - t_1) + w_2 \\cdot \\text{ReLU}(x - t_2) + \\cdots + w_n \\cdot \\text{ReLU}(x - t_n)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $t_i$ are threshold points\n",
    "- $w_i$ are weights (how much to \"bend\" at each point)\n",
    "- More neurons = more bends = better approximation\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 2: 2D Functions\n",
    "\n",
    "**Question:** For 2D input $f(x_1, x_2)$, how does adding more hidden neurons help?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Each hidden neuron creates a **half-space** (region on one side of a hyperplane):\n",
    "```\n",
    "        xâ‚‚\n",
    "         â”‚    Region A\n",
    "         â”‚  â•±\n",
    "    â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€  â† Hidden neuron's boundary\n",
    "         â”‚  â•²\n",
    "         â”‚    Region B\n",
    "         â””â”€â”€â”€â”€â”€â”€ xâ‚\n",
    "```\n",
    "\n",
    "**With multiple neurons:**\n",
    "- 2 neurons â†’ 2 boundaries â†’ 4 possible regions\n",
    "- 3 neurons â†’ 3 boundaries â†’ 7 possible regions\n",
    "- n neurons â†’ up to $2^n$ possible regions!\n",
    "\n",
    "**Building complex boundaries:**\n",
    "1. Each neuron votes: \"Is this point in my region?\"\n",
    "2. Output layer combines votes with weights\n",
    "3. Can create arbitrary polygonal decision boundaries\n",
    "\n",
    "**Example: Creating a circle (approximately)**\n",
    "\n",
    "With 8 hidden neurons arranged in a regular pattern:\n",
    "```\n",
    "        xâ‚‚\n",
    "         â”‚  \\  â”‚  /\n",
    "         â”‚   \\ â”‚ /\n",
    "    â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€  â† Octagon â‰ˆ circle\n",
    "         â”‚   / â”‚ \\\n",
    "         â”‚  /  â”‚  \\\n",
    "         â””â”€â”€â”€â”€â”€â”€ xâ‚\n",
    "```\n",
    "\n",
    "Each neuron detects one \"side\" of the octagon. As we add more neurons, the octagon becomes a better circle approximation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 3: The Universal Approximation Theorem\n",
    "\n",
    "**Statement:** A neural network with:\n",
    "- One hidden layer\n",
    "- Enough hidden neurons\n",
    "- Non-linear activation (sigmoid, tanh, ReLU)\n",
    "\n",
    "Can approximate **any continuous function** to arbitrary accuracy!\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "Think of building a function like LEGO blocks:\n",
    "1. Each hidden neuron is a basic building block (half-space)\n",
    "2. More neurons = more blocks = finer detail\n",
    "3. Output layer assembles blocks into target shape\n",
    "\n",
    "**Why it works:**\n",
    "- **Basis function view:** Hidden neurons create basis functions\n",
    "- **Linear combination:** Output layer combines them\n",
    "- **Piecewise approximation:** Any smooth function can be approximated by enough pieces\n",
    "\n",
    "**Key insight:**\n",
    "- Don't need deep networks for theory\n",
    "- But deep networks are more **efficient**:\n",
    "  - Fewer neurons total\n",
    "  - Better generalization\n",
    "  - Hierarchical feature learning\n",
    "\n",
    "**Practical reality:**\n",
    "- Theory says \"it's possible\"\n",
    "- Practice says \"but how do we find the weights?\"\n",
    "- Answer: Gradient descent! (Next session!)\n",
    "\n",
    "---\n",
    "\n",
    "#### Visualization Example\n",
    "\n",
    "Approximating $f(x) = x^2$ with 3 ReLU neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_x_squared():\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # True function\n",
    "    x = np.linspace(-2, 2, 1000)\n",
    "    y_true = x**2\n",
    "    \n",
    "    # Approximate with 3 ReLU neurons\n",
    "    # Each ReLU creates a bent line\n",
    "    relu1 = np.maximum(0, x + 1.5)\n",
    "    relu2 = np.maximum(0, x)\n",
    "    relu3 = np.maximum(0, x - 1.5)\n",
    "    \n",
    "    # Combine with weights\n",
    "    y_approx = 0.5*relu1 - relu2 + 0.5*relu3 + 1.5\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x, y_true, 'b-', linewidth=3, label='True: $x^2$')\n",
    "    plt.plot(x, y_approx, 'r--', linewidth=2, label='Approximation (3 ReLUs)')\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('y', fontsize=14)\n",
    "    plt.title('Universal Approximation: Approximating $x^2$ with 3 ReLU Neurons', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777c0e1",
   "metadata": {},
   "source": [
    "With more neurons, the approximation gets arbitrarily good!\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Key Solutions\n",
    "\n",
    "### Most Important Takeaways\n",
    "\n",
    "1. **XOR requires non-linearity** - Single perceptrons mathematically cannot solve it\n",
    "\n",
    "2. **Hidden layers create feature detectors** - Each learns to detect different patterns\n",
    "\n",
    "3. **Forward propagation is straightforward** - Just matrix multiplies and activations\n",
    "\n",
    "4. **Manual tuning is impractical** - Even 9 parameters is tedious\n",
    "\n",
    "5. **Different activations have trade-offs**:\n",
    "   - Sigmoid: smooth, probabilistic, but vanishing gradients\n",
    "   - ReLU: fast, avoids vanishing gradients, but can \"die\"\n",
    "   - Step: simple but not differentiable\n",
    "\n",
    "6. **Universal approximation theorem** - Networks can approximate any function (theoretically)\n",
    "\n",
    "7. **Motivation for gradient descent** - We NEED automatic training!\n",
    "\n",
    "---\n",
    "\n",
    "**End of Solutions Manual** ğŸ“\n",
    "\n",
    "These solutions prepare students for **Session 5: Gradient Descent**, where they'll finally learn how to train networks automatically!\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
