{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280ef147",
   "metadata": {},
   "source": [
    "# Session 11: Building CNNs\n",
    "## Assembling the Vision Machine\n",
    "\n",
    "**Course: Neural Networks for Engineers**  \n",
    "**Duration: 2 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Part I â€” Concepts & Exercises (â‰ˆ 50 min)\n",
    "1. [Recap & Motivation](#recap)\n",
    "2. [Pooling Layers](#pooling)\n",
    "3. [CNN Architecture Patterns](#patterns)\n",
    "4. [Classic Architecture: LeNet-5](#lenet)\n",
    "5. [Training CNNs: Practical Considerations](#training)\n",
    "\n",
    "### Part II â€” Build, Train, Explore (â‰ˆ 70 min)\n",
    "6. [Implement LeNet-5 in PyTorch](#implement-lenet)\n",
    "7. [Train on MNIST (Full Dataset)](#train-mnist)\n",
    "8. [Visualize What the CNN Learned](#visualize)\n",
    "9. [Architecture Experiments](#experiments)\n",
    "10. [Summary](#summary)\n",
    "\n",
    "---\n",
    "\n",
    "# Part I â€” Concepts & Exercises\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap & Motivation {#recap}\n",
    "\n",
    "### What We Know\n",
    "\n",
    "âœ… **Convolution**: Slide a kernel, compute local dot products â†’ feature map (Session 10)  \n",
    "âœ… **Padding & stride**: Control output size; $O = \\lfloor (H + 2p - k)/s \\rfloor + 1$ (Session 10)  \n",
    "âœ… **Feature maps**: Each kernel detects one type of pattern (Session 10)  \n",
    "âœ… **PyTorch**: `nn.Conv2d`, `nn.Module`, training loop (Sessions 9â€“10)\n",
    "\n",
    "### ğŸ¤” Quick Questions (from Session 10's \"Think About\")\n",
    "\n",
    "**Q1:** After two $3 \\times 3$ convolutions (no padding), a $28 \\times 28$ image becomes $24 \\times 24$. After ten such layers, what size would it be?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Each layer shrinks the image by 2 in each dimension: $28 \\to 26 \\to 24 \\to 22 \\to 20 \\to 18 \\to 16 \\to 14 \\to 12 \\to 10 \\to 8$. After 10 layers: $8 \\times 8$. After 14 layers it would reach $0 \\times 0$ â€” impossible! This is why we need **padding** or a way to **intentionally** reduce size (pooling) rather than losing it accidentally.\n",
    "</details>\n",
    "\n",
    "**Q2:** Can a CNN beat our MLP's ~97% on MNIST?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Yes! State-of-the-art CNNs achieve **99.7%+** on MNIST. Even simple CNNs easily reach **99%+**. The spatial structure that MLPs ignore is exactly what CNNs exploit. We'll see this today.\n",
    "</details>\n",
    "\n",
    "**Q3:** What loss function guides learned kernels?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The same `CrossEntropyLoss` we've been using! The kernels are just parameters â€” backpropagation computes $\\partial L / \\partial K$ for every kernel weight, and the optimizer updates them. No one tells the network to learn edge detectors â€” it discovers them because edges help minimize classification loss.\n",
    "</details>\n",
    "\n",
    "### The Missing Piece\n",
    "\n",
    "We can detect features with convolutions. But to classify an image, we need to go from feature maps to a class label:\n",
    "\n",
    "```\n",
    "Input image â†’ [Feature extraction] â†’ [Classification]\n",
    "  28Ã—28          Conv layers             FC layers â†’ 10 classes\n",
    "\n",
    "We know this part (Session 10)     We know this part (Session 9)\n",
    "\n",
    "Today: How to connect them.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Pooling Layers {#pooling}\n",
    "\n",
    "### The Problem\n",
    "\n",
    "After several convolutions, we have many feature maps at full (or nearly full) resolution. This is:\n",
    "- **Expensive**: Too many values to process\n",
    "- **Fragile**: Features tied to exact pixel positions\n",
    "\n",
    "We need a way to **reduce spatial dimensions** while keeping the important information.\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "**Max pooling** slides a window across the feature map and keeps only the **maximum value** in each window:\n",
    "\n",
    "```\n",
    "Input (4Ã—4):                Max Pool 2Ã—2, stride 2:\n",
    "\n",
    "  1  3  2  1                â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
    "  4  6  5  2      â†’        â”‚max  â”‚max  â”‚     6  5\n",
    "  7  2  3  1                â”‚ 4,6 â”‚ 5,2 â”‚     8  4\n",
    "  8  1  4  3                â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤\n",
    "                            â”‚max  â”‚max  â”‚\n",
    "                            â”‚ 8,2 â”‚ 4,3 â”‚\n",
    "                            â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "  Output (2Ã—2): halved in each dimension!\n",
    "```\n",
    "\n",
    "**Key properties:**\n",
    "- Reduces spatial size (typically by half: $2 \\times 2$ pool, stride 2)\n",
    "- **Keeps the strongest activation** in each region\n",
    "- Provides a small amount of **translation invariance** (the \"6\" can shift by 1 pixel and the max is still 6)\n",
    "- **No learnable parameters!** It's a fixed operation.\n",
    "\n",
    "### Average Pooling\n",
    "\n",
    "Instead of the maximum, take the **mean** of each window:\n",
    "\n",
    "```\n",
    "Same input:                 Avg Pool 2Ã—2, stride 2:\n",
    "\n",
    "  1  3  2  1                (1+3+4+6)/4  (2+1+5+2)/4     3.5  2.5\n",
    "  4  6  5  2      â†’\n",
    "  7  2  3  1                (7+2+8+1)/4  (3+1+4+3)/4     4.5  2.75\n",
    "  8  1  4  3\n",
    "```\n",
    "\n",
    "| | Max Pooling | Average Pooling |\n",
    "|---|---|---|\n",
    "| Keeps | Strongest activation | Average activation |\n",
    "| Used for | Most hidden layers | Sometimes final layer (Global Avg Pool) |\n",
    "| Effect | \"Is this feature present?\" | \"How much of this feature?\" |\n",
    "| In practice | **Most common** | Used in modern architectures |\n",
    "\n",
    "### âœï¸ Exercise 2.1 â€” Max Pooling by Hand\n",
    "\n",
    "Apply $2 \\times 2$ max pooling (stride 2) to this $4 \\times 4$ feature map:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 3 & 1 & 4 & 2 \\\\ 0 & 5 & 1 & 3 \\\\ 2 & 4 & 6 & 0 \\\\ 1 & 3 & 2 & 8 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "Top-left $2 \\times 2$: $\\max(3, 1, 0, 5) = 5$  \n",
    "Top-right $2 \\times 2$: $\\max(4, 2, 1, 3) = 4$  \n",
    "Bottom-left $2 \\times 2$: $\\max(2, 4, 1, 3) = 4$  \n",
    "Bottom-right $2 \\times 2$: $\\max(6, 0, 2, 8) = 8$\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\begin{bmatrix} 5 & 4 \\\\ 4 & 8 \\end{bmatrix}\n",
    "$$\n",
    "</details>\n",
    "\n",
    "### âœï¸ Exercise 2.2 â€” Translation Invariance Demo\n",
    "\n",
    "Consider these two $4 \\times 4$ feature maps â€” the second is the first shifted one pixel to the right:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 0 & 9 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\n",
    "\\qquad\n",
    "B = \\begin{bmatrix} 0 & 0 & 9 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Apply $2 \\times 2$ max pooling (stride 2) to both. Are the outputs the same?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "$\\text{MaxPool}(A) = \\begin{bmatrix} 9 & 0 \\\\ 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "$\\text{MaxPool}(B) = \\begin{bmatrix} 0 & 9 \\\\ 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "The outputs are **not** identical â€” the 9 moved from position (0,0) to (0,1). But the **presence** of the strong activation is preserved in the top row. Max pooling provides invariance to **small** shifts (within the pool window), not large ones. Multiple pooling layers provide invariance to progressively larger shifts.\n",
    "</details>\n",
    "\n",
    "### âœï¸ Exercise 2.3 â€” Dimension Tracking\n",
    "\n",
    "An input of size $32 \\times 32$ passes through the following layers. Track the spatial dimensions:\n",
    "\n",
    "| Layer | Output size |\n",
    "|---|---|\n",
    "| Input | $32 \\times 32$ |\n",
    "| Conv2d(3Ã—3, padding=1, stride=1) | ? |\n",
    "| ReLU | ? |\n",
    "| MaxPool2d(2Ã—2, stride=2) | ? |\n",
    "| Conv2d(3Ã—3, padding=1, stride=1) | ? |\n",
    "| ReLU | ? |\n",
    "| MaxPool2d(2Ã—2, stride=2) | ? |\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Layer | Output size | Reasoning |\n",
    "|---|---|---|\n",
    "| Input | $32 \\times 32$ | |\n",
    "| Conv2d(3Ã—3, p=1, s=1) | $32 \\times 32$ | Same padding preserves size |\n",
    "| ReLU | $32 \\times 32$ | Element-wise, no size change |\n",
    "| MaxPool2d(2Ã—2, s=2) | $16 \\times 16$ | Halved |\n",
    "| Conv2d(3Ã—3, p=1, s=1) | $16 \\times 16$ | Same padding preserves size |\n",
    "| ReLU | $16 \\times 16$ | No size change |\n",
    "| MaxPool2d(2Ã—2, s=2) | $8 \\times 8$ | Halved again |\n",
    "\n",
    "**Pattern:** Conv+ReLU maintain size (with same padding), MaxPool halves it. Two pool layers: $32 \\to 16 \\to 8$.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 3. CNN Architecture Patterns {#patterns}\n",
    "\n",
    "### The Standard Pattern\n",
    "\n",
    "Almost every CNN follows this structure:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     FEATURE EXTRACTOR        â”‚   â”‚       CLASSIFIER        â”‚\n",
    "â”‚                              â”‚   â”‚                         â”‚\n",
    "â”‚  Conv â†’ ReLU â†’ Pool          â”‚   â”‚  Flatten â†’ FC â†’ ReLU   â”‚\n",
    "â”‚  Conv â†’ ReLU â†’ Pool          â”‚â†’â†’â†’â”‚  FC â†’ Softmax           â”‚\n",
    "â”‚  Conv â†’ ReLU â†’ Pool          â”‚   â”‚                         â”‚\n",
    "â”‚  ...                         â”‚   â”‚  (Same as Session 9 MLP)â”‚\n",
    "â”‚                              â”‚   â”‚                         â”‚\n",
    "â”‚  Spatial dims shrink â†“       â”‚   â”‚  No spatial dims        â”‚\n",
    "â”‚  Channel count grows â†‘       â”‚   â”‚  Just a vector          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The \"Funnel\" Shape\n",
    "\n",
    "As we go deeper, spatial dimensions **decrease** and channel count **increases**:\n",
    "\n",
    "```\n",
    "Layer        Channels    Spatial      Total values\n",
    "â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Input        1           28 Ã— 28      784\n",
    "After Conv1  16          28 Ã— 28      12,544\n",
    "After Pool1  16          14 Ã— 14      3,136\n",
    "After Conv2  32          14 Ã— 14      6,272\n",
    "After Pool2  32          7 Ã— 7        1,568\n",
    "Flatten      â€”           â€”            1,568\n",
    "FC1          128         â€”            128\n",
    "Output       10          â€”            10\n",
    "```\n",
    "\n",
    "**Intuition:** Early layers have few channels but high resolution (detect simple local features like edges). Deep layers have many channels but low resolution (detect complex global features like shapes, parts).\n",
    "\n",
    "### The Flatten Operation\n",
    "\n",
    "At some point we must transition from a 3D tensor `(channels, H, W)` to a 1D vector for the fully-connected classifier. This is **flattening**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before flatten: shape (batch, 32, 7, 7)\n",
    "x = x.view(x.size(0), -1)     # or x.flatten(1)\n",
    "# After flatten: shape (batch, 32*7*7) = (batch, 1568)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962ceb6",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 3.1 â€” Trace a Full CNN\n",
    "\n",
    "Trace the tensor shape through this network (input: batch of 4 images, $1 \\times 28 \\times 28$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(1, 8, 5, padding=2)    # ?\n",
    "nn.ReLU()                         # ?\n",
    "nn.MaxPool2d(2, 2)                # ?\n",
    "nn.Conv2d(8, 16, 5, padding=2)   # ?\n",
    "nn.ReLU()                         # ?\n",
    "nn.MaxPool2d(2, 2)                # ?\n",
    "# flatten\n",
    "nn.Linear(?, 120)                 # ?\n",
    "nn.ReLU()                         # ?\n",
    "nn.Linear(120, 10)                # ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1b96e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Layer | Output shape |\n",
    "|---|---|\n",
    "| Input | `(4, 1, 28, 28)` |\n",
    "| Conv2d(1â†’8, k=5, p=2) | `(4, 8, 28, 28)` |\n",
    "| ReLU | `(4, 8, 28, 28)` |\n",
    "| MaxPool2d(2, 2) | `(4, 8, 14, 14)` |\n",
    "| Conv2d(8â†’16, k=5, p=2) | `(4, 16, 14, 14)` |\n",
    "| ReLU | `(4, 16, 14, 14)` |\n",
    "| MaxPool2d(2, 2) | `(4, 16, 7, 7)` |\n",
    "| Flatten | `(4, 784)` â€” because $16 \\times 7 \\times 7 = 784$ |\n",
    "| Linear(784â†’120) | `(4, 120)` |\n",
    "| ReLU | `(4, 120)` |\n",
    "| Linear(120â†’10) | `(4, 10)` |\n",
    "\n",
    "The `nn.Linear` needs **784** input features (the flattened feature volume $16 \\times 7 \\times 7$).\n",
    "</details>\n",
    "\n",
    "### âœï¸ Exercise 3.2 â€” Parameter Count\n",
    "\n",
    "For the CNN in Exercise 3.1, compute the number of parameters in each layer:\n",
    "\n",
    "| Layer | Parameters |\n",
    "|---|---|\n",
    "| Conv2d(1â†’8, k=5) | ? |\n",
    "| Conv2d(8â†’16, k=5) | ? |\n",
    "| Linear(784â†’120) | ? |\n",
    "| Linear(120â†’10) | ? |\n",
    "| **Total** | ? |\n",
    "\n",
    "And compare with our Session 9 MLP (784â†’256â†’128â†’10).\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**CNN:**\n",
    "\n",
    "| Layer | Calculation | Parameters |\n",
    "|---|---|---|\n",
    "| Conv2d(1â†’8, k=5) | $8 \\times (1 \\times 5 \\times 5) + 8$ | 208 |\n",
    "| Conv2d(8â†’16, k=5) | $16 \\times (8 \\times 5 \\times 5) + 16$ | 3,216 |\n",
    "| Linear(784â†’120) | $784 \\times 120 + 120$ | 94,200 |\n",
    "| Linear(120â†’10) | $120 \\times 10 + 10$ | 1,210 |\n",
    "| **Total** | | **98,834** |\n",
    "\n",
    "**Session 9 MLP:**\n",
    "\n",
    "| Layer | Parameters |\n",
    "|---|---|\n",
    "| Linear(784â†’256) | 200,960 |\n",
    "| Linear(256â†’128) | 32,896 |\n",
    "| Linear(128â†’10) | 1,290 |\n",
    "| **Total** | **235,146** |\n",
    "\n",
    "The CNN has **2.4Ã— fewer parameters** but (as we'll see) achieves better accuracy. Most of the CNN's parameters are in the first FC layer â€” the conv layers are very efficient.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Classic Architecture: LeNet-5 {#lenet}\n",
    "\n",
    "### The Architecture That Started It All\n",
    "\n",
    "LeNet-5 (Yann LeCun, 1998) was designed for handwritten digit recognition â€” the exact task we've been working on! It was used by the US Postal Service to read ZIP codes.\n",
    "\n",
    "```\n",
    "Input: 1 Ã— 32 Ã— 32 (we'll adapt to 28Ã—28)\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Conv(1â†’6, 5Ã—5) â†’ ReLU â†’ Pool(2Ã—2)                            â”‚\n",
    "â”‚       â†“                                                         â”‚\n",
    "â”‚  Conv(6â†’16, 5Ã—5) â†’ ReLU â†’ Pool(2Ã—2)                           â”‚\n",
    "â”‚       â†“                                                         â”‚\n",
    "â”‚  Flatten â†’ FC(400â†’120) â†’ ReLU â†’ FC(120â†’84) â†’ ReLU â†’ FC(84â†’10)â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**For our 28Ã—28 MNIST input** (no padding on convolutions):\n",
    "\n",
    "| Layer | Output shape | Parameters |\n",
    "|---|---|---|\n",
    "| Input | `(1, 28, 28)` | â€” |\n",
    "| Conv(1â†’6, k=5) | `(6, 24, 24)` | 156 |\n",
    "| ReLU | `(6, 24, 24)` | â€” |\n",
    "| MaxPool(2, 2) | `(6, 12, 12)` | â€” |\n",
    "| Conv(6â†’16, k=5) | `(16, 8, 8)` | 2,416 |\n",
    "| ReLU | `(16, 8, 8)` | â€” |\n",
    "| MaxPool(2, 2) | `(16, 4, 4)` | â€” |\n",
    "| Flatten | `(256)` | â€” |\n",
    "| FC(256â†’120) | `(120)` | 30,840 |\n",
    "| ReLU | `(120)` | â€” |\n",
    "| FC(120â†’84) | `(84)` | 10,164 |\n",
    "| ReLU | `(84)` | â€” |\n",
    "| FC(84â†’10) | `(10)` | 850 |\n",
    "| **Total** | | **44,426** |\n",
    "\n",
    "Only **44K parameters** â€” half the size of our Session 9 MLP!\n",
    "\n",
    "### Why Depth Matters\n",
    "\n",
    "A single convolution layer detects **simple** features (edges, gradients). Stacking layers lets the network build a **hierarchy**:\n",
    "\n",
    "```\n",
    "Layer 1 (Conv1):    Edges, gradients\n",
    "                         â†“ combine\n",
    "Layer 2 (Conv2):    Corners, curves, textures\n",
    "                         â†“ combine\n",
    "Layer 3 (FC):       Digit parts (loops, strokes)\n",
    "                         â†“ combine\n",
    "Layer 4 (FC):       Whole digits (0-9)\n",
    "```\n",
    "\n",
    "Each layer builds on the previous one. This is the power of **deep** learning â€” not just more parameters, but more levels of abstraction.\n",
    "\n",
    "### âœï¸ Exercise 4.1 â€” Receptive Field\n",
    "\n",
    "The **receptive field** is the region of the input that influences a single output neuron.\n",
    "\n",
    "**Part A:** After one $5 \\times 5$ conv (no padding), each output pixel \"sees\" a $5 \\times 5$ patch of the input. What is the receptive field after a $2 \\times 2$ max pool?\n",
    "\n",
    "**Part B:** After Conv1 ($5 \\times 5$) + Pool ($2 \\times 2$) + Conv2 ($5 \\times 5$), what is the receptive field on the original input?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Part A:** The $2 \\times 2$ pool combines $2 \\times 2$ adjacent conv outputs. Each of those already sees $5 \\times 5$ input pixels. The pools overlap by 4 pixels in each dimension (because stride 1 in conv), so the receptive field after pool is:\n",
    "\n",
    "$5 + (2 - 1) = 6$ in each dimension â†’ $\\mathbf{6 \\times 6}$ receptive field.\n",
    "\n",
    "More precisely: the pool output at position $(i,j)$ looks at conv outputs $(2i, 2j)$ through $(2i+1, 2j+1)$, which look at input pixels $(2i, 2j)$ through $(2i+5, 2j+5)$.\n",
    "\n",
    "**Part B:** After Conv2 ($5 \\times 5$), each Conv2 output sees a $5 \\times 5$ region of Conv1's pooled output. Each of those sees $6 \\times 6$ input pixels. With the additional $5 \\times 5$ spread and stride-2 pooling:\n",
    "\n",
    "Receptive field = $5 + (5 - 1) \\times 2 = 5 + 8 = 13$ â†’ approximately $\\mathbf{14 \\times 14}$ input pixels.\n",
    "\n",
    "A single neuron deep in the network \"sees\" **half the image**! This is how local operations build global understanding.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Training CNNs: Practical Considerations {#training}\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "CNNs learn from spatial patterns, so we can create more training data by applying **random spatial transformations**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dba267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomRotation(10),           # Rotate Â±10Â°\n",
    "    T.RandomAffine(0, translate=(0.1, 0.1)),  # Shift up to 10%\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),                    # No augmentation for test!\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ba799",
   "metadata": {},
   "source": [
    "**Key rule:** Augment **training** data only. Test data must reflect real-world conditions.\n",
    "\n",
    "**Common augmentations for MNIST:**\n",
    "\n",
    "| Augmentation | Effect | Why it helps |\n",
    "|---|---|---|\n",
    "| Rotation (Â±10Â°) | Slightly tilted digits | People write at different angles |\n",
    "| Translation (10%) | Shifted position | Digits aren't always centered |\n",
    "| Scaling (Â±10%) | Slightly larger/smaller | Handwriting size varies |\n",
    "\n",
    "### Batch Training with DataLoader\n",
    "\n",
    "For large datasets, we can't fit all images in memory at once. PyTorch's `DataLoader` handles **mini-batch** training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Training loop now iterates over batches:\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc7d94",
   "metadata": {},
   "source": [
    "This is the **batch SGD** we discussed in Session 5, now automated!\n",
    "\n",
    "### âœï¸ Exercise 5.1 â€” Augmentation Reasoning\n",
    "\n",
    "For each dataset, which augmentations make sense?\n",
    "\n",
    "| Dataset | Rotation? | Horizontal flip? | Vertical flip? | Color jitter? |\n",
    "|---|---|---|---|---|\n",
    "| MNIST digits | ? | ? | ? | ? |\n",
    "| Cats vs dogs | ? | ? | ? | ? |\n",
    "| Satellite images (find buildings) | ? | ? | ? | ? |\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Dataset | Rotation? | H-flip? | V-flip? | Color jitter? |\n",
    "|---|---|---|---|---|\n",
    "| MNIST digits | âœ… Small (Â±15Â°) | âŒ (6 â‰  mirrored 6) | âŒ (6 â‰  9 upside down) | âŒ (grayscale) |\n",
    "| Cats vs dogs | âœ… Small | âœ… (a cat facing left is still a cat) | âŒ (upside-down cats are rare) | âœ… (lighting varies) |\n",
    "| Satellite images | âœ… Full 360Â° | âœ… | âœ… (no \"up\" in satellite view) | âœ… (seasons, time of day) |\n",
    "\n",
    "The key principle: only augment in ways that **preserve the label**. A horizontally flipped \"b\" becomes \"d\" â€” that would confuse the model!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "# Part II â€” Build, Train, Explore\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Implement LeNet-5 in PyTorch {#implement-lenet}\n",
    "\n",
    "### ğŸ’» Exercise 6.1 â€” Define LeNet-5\n",
    "\n",
    "**Task:** Implement LeNet-5 as an `nn.Module`. Use the architecture from Section 4, adapted for $28 \\times 28$ input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6808a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-5 adapted for 28Ã—28 MNIST.\n",
    "    \n",
    "    Architecture:\n",
    "        Conv(1â†’6, 5Ã—5) â†’ ReLU â†’ MaxPool(2Ã—2)\n",
    "        Conv(6â†’16, 5Ã—5) â†’ ReLU â†’ MaxPool(2Ã—2)\n",
    "        Flatten â†’ FC(256â†’120) â†’ ReLU â†’ FC(120â†’84) â†’ ReLU â†’ FC(84â†’10)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define the feature extractor (2 conv blocks)\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: Conv(1â†’6, k=5) â†’ ReLU â†’ MaxPool(2,2)\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "            # Block 2: Conv(6â†’16, k=5) â†’ ReLU â†’ MaxPool(2,2)\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "        )\n",
    "        \n",
    "        # TODO: Define the classifier (3 FC layers)\n",
    "        # After features: shape is (batch, 16, 4, 4) â†’ flatten to 256\n",
    "        self.classifier = nn.Sequential(\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: features â†’ flatten â†’ classifier\n",
    "        x = ___\n",
    "        x = x.view(x.size(0), -1)   # Flatten: (batch, 16, 4, 4) â†’ (batch, 256)\n",
    "        x = ___\n",
    "        return x\n",
    "\n",
    "# Create and verify\n",
    "model = LeNet5()\n",
    "print(model)\n",
    "\n",
    "# Verify with a dummy input\n",
    "dummy = torch.randn(2, 1, 28, 28)   # batch=2, channels=1, 28Ã—28\n",
    "out = model(dummy)\n",
    "print(f\"\\nInput:  {dummy.shape}\")\n",
    "print(f\"Output: {out.shape}\")  # Should be (2, 10)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8140085",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c4229",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943557b7",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### ğŸ’» Exercise 6.2 â€” Verify Shapes Layer by Layer\n",
    "\n",
    "**Task:** Write a diagnostic function that prints the tensor shape after every layer. This is essential for debugging CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_shapes(model, input_shape=(1, 1, 28, 28)):\n",
    "    \"\"\"Print the tensor shape after each layer in the model.\"\"\"\n",
    "    x = torch.randn(input_shape)\n",
    "    print(f\"{'Input':>30s}: {list(x.shape)}\")\n",
    "    \n",
    "    # TODO: Iterate through self.features and self.classifier\n",
    "    # Apply each layer one at a time and print the shape\n",
    "    # Don't forget to flatten between features and classifier!\n",
    "    \n",
    "    for name, layer in model.features.named_children():\n",
    "        x = layer(x)\n",
    "        print(f\"{str(layer):>30s}: {list(x.shape)}\")\n",
    "    \n",
    "    x = x.view(x.size(0), -1)\n",
    "    print(f\"{'Flatten':>30s}: {list(x.shape)}\")\n",
    "    \n",
    "    for name, layer in model.classifier.named_children():\n",
    "        ___\n",
    "        ___\n",
    "\n",
    "trace_shapes(LeNet5())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0736f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_shapes(model, input_shape=(1, 1, 28, 28)):\n",
    "    x = torch.randn(input_shape)\n",
    "    print(f\"{'Input':>30s}: {list(x.shape)}\")\n",
    "    \n",
    "    for name, layer in model.features.named_children():\n",
    "        x = layer(x)\n",
    "        print(f\"{str(layer):>30s}: {list(x.shape)}\")\n",
    "    \n",
    "    x = x.view(x.size(0), -1)\n",
    "    print(f\"{'Flatten':>30s}: {list(x.shape)}\")\n",
    "    \n",
    "    for name, layer in model.classifier.named_children():\n",
    "        x = layer(x)\n",
    "        print(f\"{str(layer):>30s}: {list(x.shape)}\")\n",
    "\n",
    "trace_shapes(LeNet5())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0f90f",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```\n",
    "                         Input: [1, 1, 28, 28]\n",
    "        Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)): [1, 6, 24, 24]\n",
    "                          ReLU(): [1, 6, 24, 24]\n",
    "   MaxPool2d(kernel_size=2, stride=2, ...): [1, 6, 12, 12]\n",
    "       Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)): [1, 16, 8, 8]\n",
    "                          ReLU(): [1, 16, 8, 8]\n",
    "   MaxPool2d(kernel_size=2, stride=2, ...): [1, 16, 4, 4]\n",
    "                       Flatten: [1, 256]\n",
    "         Linear(in_features=256, out_features=120, ...): [1, 120]\n",
    "                          ReLU(): [1, 120]\n",
    "          Linear(in_features=120, out_features=84, ...): [1, 84]\n",
    "                          ReLU(): [1, 84]\n",
    "           Linear(in_features=84, out_features=10, ...): [1, 10]\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Train on MNIST (Full Dataset) {#train-mnist}\n",
    "\n",
    "### ğŸ’» Exercise 7.1 â€” Load MNIST with DataLoader\n",
    "\n",
    "**Task:** Set up the MNIST dataset with data augmentation for training and DataLoaders for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51408f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# TODO: Define transforms\n",
    "# Training: RandomRotation(10), RandomAffine (translate 10%), ToTensor\n",
    "# Testing: just ToTensor\n",
    "train_transform = transforms.Compose([\n",
    "    ___,\n",
    "    ___,\n",
    "    ___,\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    ___,\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, \n",
    "                                transform=train_transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, \n",
    "                               transform=test_transform)\n",
    "\n",
    "# TODO: Create DataLoaders\n",
    "# Training: batch_size=64, shuffle=True\n",
    "# Testing: batch_size=256, shuffle=False\n",
    "train_loader = DataLoader(___)\n",
    "test_loader = DataLoader(___)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)} (of size 64)\")\n",
    "print(f\"Test batches:     {len(test_loader)} (of size 256)\")\n",
    "\n",
    "# Verify a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "# Expected: (64, 1, 28, 28) and (64,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde89d04",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15d39a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, \n",
    "                                transform=train_transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, \n",
    "                               transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f0cc7",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### ğŸ’» Exercise 7.2 â€” Write the Batch Training Loop\n",
    "\n",
    "**Task:** Write a complete training loop using DataLoader. This differs from Session 9 because we iterate over **batches** instead of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d485713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, test_loader, n_epochs=10, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train a CNN with batch SGD.\n",
    "    \n",
    "    Returns: train_losses, test_accs (per epoch)\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # â”€â”€ Train â”€â”€\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            # TODO: The 5-step training pattern (from Session 9)\n",
    "            outputs = ___\n",
    "            loss = ___\n",
    "            ___          # zero grad\n",
    "            ___          # backward\n",
    "            ___          # step\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # â”€â”€ Evaluate on test set â”€â”€\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                # TODO: Forward pass, count correct predictions\n",
    "                outputs = ___\n",
    "                preds = ___\n",
    "                correct += ___\n",
    "                total += ___\n",
    "        \n",
    "        test_acc = correct / total * 100\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{n_epochs}: \"\n",
    "              f\"Train Loss = {avg_loss:.4f} | Test Acc = {test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bdb75",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3347fd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, test_loader, n_epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        test_acc = correct / total * 100\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{n_epochs}: \"\n",
    "              f\"Train Loss = {avg_loss:.4f} | Test Acc = {test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437e83c",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### ğŸ’» Exercise 7.3 â€” Train and Plot\n",
    "\n",
    "**Task:** Train LeNet-5 for 10 epochs and plot the training loss and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "train_losses, test_accs = train_cnn(model, train_loader, test_loader, n_epochs=10, lr=0.001)\n",
    "\n",
    "# TODO: Create a 1Ã—2 figure\n",
    "# Left: Training loss per epoch (line plot)\n",
    "# Right: Test accuracy per epoch (line plot)\n",
    "# Add horizontal dashed line at 97% (our MLP baseline from Session 9)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: loss\n",
    "ax = axes[0]\n",
    "___\n",
    "\n",
    "# Right: accuracy with MLP baseline\n",
    "ax = axes[1]\n",
    "___\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal test accuracy: {test_accs[-1]:.2f}%\")\n",
    "print(f\"Session 9 MLP baseline: ~97%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea39622",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154b0aa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(range(1, len(train_losses)+1), train_losses, 'b-o', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Training Loss', fontsize=14)\n",
    "ax.set_title('LeNet-5 Training Loss', fontsize=16)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(range(1, len(test_accs)+1), test_accs, 'g-o', linewidth=2, markersize=6)\n",
    "ax.axhline(y=97, color='red', linestyle='--', linewidth=2, label='MLP baseline (~97%)')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=14)\n",
    "ax.set_title('LeNet-5 Test Accuracy', fontsize=16)\n",
    "ax.set_ylim(95, 100)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36a75f",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### ğŸ’» Exercise 7.4 â€” Confusion Matrix\n",
    "\n",
    "**Task:** Build and display the $10 \\times 10$ confusion matrix on the test set. Which digits does the CNN confuse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all predictions\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "# TODO: Build and display confusion matrix\n",
    "# Reuse the pattern from Session 9 Project C\n",
    "cm = np.zeros((10, 10), dtype=int)\n",
    "for true, pred in zip(all_labels, all_preds):\n",
    "    cm[true, pred] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "___\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Per-digit accuracy\n",
    "print(\"\\nPer-digit accuracy:\")\n",
    "for d in range(10):\n",
    "    acc = cm[d, d] / cm[d].sum() * 100\n",
    "    print(f\"  Digit {d}: {acc:.1f}% ({cm[d, d]}/{cm[d].sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b650a22",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e2395",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                color=color, fontsize=9)\n",
    "\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xlabel('Predicted', fontsize=14)\n",
    "ax.set_ylabel('True', fontsize=14)\n",
    "ax.set_title(f'LeNet-5 Confusion Matrix (Acc: {test_accs[-1]:.2f}%)', fontsize=16)\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf56a6b",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Visualize What the CNN Learned {#visualize}\n",
    "\n",
    "### The Key Question\n",
    "\n",
    "We hand-designed edge and blur kernels in Session 10. Now the CNN **learned** its own kernels through backpropagation. What did it discover?\n",
    "\n",
    "### ğŸ’» Exercise 8.1 â€” Visualize Learned Filters\n",
    "\n",
    "**Task:** Extract and display the 6 filters learned by Conv1 (the first convolutional layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4607932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract Conv1's learned weights\n",
    "# model.features[0] is the first Conv2d layer\n",
    "# .weight has shape (6, 1, 5, 5) â€” 6 filters, 1 input channel, 5Ã—5\n",
    "conv1_weights = model.features[0].weight.data.clone()\n",
    "\n",
    "print(f\"Conv1 filter shape: {conv1_weights.shape}\")\n",
    "\n",
    "# TODO: Display the 6 filters in a 1Ã—6 grid\n",
    "fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
    "for i in range(6):\n",
    "    # Extract filter i: shape (1, 5, 5) â†’ squeeze to (5, 5)\n",
    "    filt = ___\n",
    "    ax = axes[i]\n",
    "    ax.imshow(filt, cmap='gray')\n",
    "    ax.set_title(f'Filter {i}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('LeNet-5 Conv1: Learned Filters (5Ã—5)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f0ddf",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55bb94",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "conv1_weights = model.features[0].weight.data.clone()\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
    "for i in range(6):\n",
    "    filt = conv1_weights[i, 0].numpy()\n",
    "    ax = axes[i]\n",
    "    ax.imshow(filt, cmap='gray')\n",
    "    ax.set_title(f'Filter {i}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('LeNet-5 Conv1: Learned Filters (5Ã—5)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8450703",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Observation:** You should see filters that resemble **edge detectors** at various angles, **gradient detectors**, and maybe a **blob detector**. The CNN reinvented what took decades of computer vision research â€” automatically, from data!\n",
    "\n",
    "### ğŸ’» Exercise 8.2 â€” Visualize Feature Maps\n",
    "\n",
    "**Task:** Pass a single MNIST digit through the network and display the feature maps after each conv layer. This shows what each filter \"sees.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test digit\n",
    "test_img, test_label = test_dataset[0]\n",
    "print(f\"Digit: {test_label}, shape: {test_img.shape}\")\n",
    "\n",
    "# TODO: Get feature maps at each stage\n",
    "# We need to run parts of the model manually.\n",
    "# features[0] = Conv1, features[1] = ReLU, features[2] = Pool1\n",
    "# features[3] = Conv2, features[4] = ReLU, features[5] = Pool2\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = test_img.unsqueeze(0)      # Add batch dim: (1, 1, 28, 28)\n",
    "    \n",
    "    # After Conv1 + ReLU\n",
    "    after_conv1 = model.features[1](model.features[0](x))     # (1, 6, 24, 24)\n",
    "    \n",
    "    # After Pool1\n",
    "    after_pool1 = model.features[2](after_conv1)               # (1, 6, 12, 12)\n",
    "    \n",
    "    # TODO: After Conv2 + ReLU\n",
    "    after_conv2 = ___                                           # (1, 16, 8, 8)\n",
    "    \n",
    "    # TODO: After Pool2\n",
    "    after_pool2 = ___                                           # (1, 16, 4, 4)\n",
    "\n",
    "# TODO: Display feature maps in a multi-row figure\n",
    "# Row 0: Original image (1 panel)\n",
    "# Row 1: Conv1 feature maps (6 panels)\n",
    "# Row 2: Conv2 feature maps (16 panels, use 2 rows of 8)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Original\n",
    "ax = fig.add_subplot(4, 8, 1)\n",
    "ax.imshow(test_img.squeeze(), cmap='gray')\n",
    "ax.set_title(f'Input (digit {test_label})', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "# Conv1 feature maps (6 maps)\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(4, 8, 9 + i)   # Second row\n",
    "    fmap = after_conv1[0, i].numpy()\n",
    "    ax.imshow(fmap, cmap='viridis')\n",
    "    ax.set_title(f'Conv1-{i}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "# TODO: Conv2 feature maps (16 maps in 2 rows of 8)\n",
    "for i in range(16):\n",
    "    row = 2 + i // 8    # rows 2 and 3\n",
    "    col = i % 8\n",
    "    ax = fig.add_subplot(4, 8, row * 8 + col + 1)\n",
    "    fmap = ___\n",
    "    ax.imshow(fmap, cmap='viridis')\n",
    "    ax.set_title(f'C2-{i}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Feature Maps Through LeNet-5', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7998000",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6399e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = test_img.unsqueeze(0)\n",
    "    \n",
    "    after_conv1 = model.features[1](model.features[0](x))\n",
    "    after_pool1 = model.features[2](after_conv1)\n",
    "    after_conv2 = model.features[4](model.features[3](after_pool1))\n",
    "    after_pool2 = model.features[5](after_conv2)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "ax = fig.add_subplot(4, 8, 1)\n",
    "ax.imshow(test_img.squeeze(), cmap='gray')\n",
    "ax.set_title(f'Input (digit {test_label})', fontsize=11)\n",
    "ax.axis('off')\n",
    "\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(4, 8, 9 + i)\n",
    "    fmap = after_conv1[0, i].numpy()\n",
    "    ax.imshow(fmap, cmap='viridis')\n",
    "    ax.set_title(f'Conv1-{i}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "for i in range(16):\n",
    "    row = 2 + i // 8\n",
    "    col = i % 8\n",
    "    ax = fig.add_subplot(4, 8, row * 8 + col + 1)\n",
    "    fmap = after_conv2[0, i].numpy()\n",
    "    ax.imshow(fmap, cmap='viridis')\n",
    "    ax.set_title(f'C2-{i}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Feature Maps Through LeNet-5', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70570f92",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Observations to look for:**\n",
    "- **Conv1 maps**: Should look like edges/gradients applied to the digit â€” some highlight horizontal strokes, others vertical\n",
    "- **Conv2 maps**: More abstract â€” combinations of edges that detect curves, junctions, and parts of digits\n",
    "- **Some maps may be mostly dark**: the filter didn't fire for this particular digit, but it would for others\n",
    "\n",
    "### ğŸ’» Exercise 8.3 â€” Compare Feature Maps Across Digits\n",
    "\n",
    "**Task:** Pick 3 different digits (e.g., 0, 1, 7) and show the Conv1 feature maps for each. Does the same filter respond differently to different digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eef06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find one example each of digits 0, 1, and 7\n",
    "digits_to_show = [0, 1, 7]\n",
    "digit_images = []\n",
    "for d in digits_to_show:\n",
    "    idx = next(i for i in range(len(test_dataset)) if test_dataset[i][1] == d)\n",
    "    digit_images.append(test_dataset[idx][0])\n",
    "\n",
    "# TODO: Compute Conv1 + ReLU feature maps for each digit\n",
    "# Display in a 3Ã—7 grid: column 0 = original, columns 1-6 = feature maps\n",
    "fig, axes = plt.subplots(3, 7, figsize=(18, 8))\n",
    "\n",
    "for row, (d, img) in enumerate(zip(digits_to_show, digit_images)):\n",
    "    # Original\n",
    "    axes[row, 0].imshow(img.squeeze(), cmap='gray')\n",
    "    axes[row, 0].set_title(f'Digit {d}', fontsize=12)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Feature maps\n",
    "    with torch.no_grad():\n",
    "        fmaps = model.features[1](model.features[0](img.unsqueeze(0)))\n",
    "    \n",
    "    for i in range(6):\n",
    "        ax = axes[row, i+1]\n",
    "        ___\n",
    "        ax.axis('off')\n",
    "        if row == 0:\n",
    "            ax.set_title(f'Filter {i}', fontsize=10)\n",
    "\n",
    "plt.suptitle('Same Filters, Different Digits', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2d926",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c70092",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 7, figsize=(18, 8))\n",
    "\n",
    "for row, (d, img) in enumerate(zip(digits_to_show, digit_images)):\n",
    "    axes[row, 0].imshow(img.squeeze(), cmap='gray')\n",
    "    axes[row, 0].set_title(f'Digit {d}', fontsize=12)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fmaps = model.features[1](model.features[0](img.unsqueeze(0)))\n",
    "    \n",
    "    for i in range(6):\n",
    "        ax = axes[row, i+1]\n",
    "        ax.imshow(fmaps[0, i].numpy(), cmap='viridis')\n",
    "        ax.axis('off')\n",
    "        if row == 0:\n",
    "            ax.set_title(f'Filter {i}', fontsize=10)\n",
    "\n",
    "plt.suptitle('Same Filters, Different Digits', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09b1f5",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Key observation:** The same filter produces **different patterns** for different digits. Filter 2 might highlight the horizontal bar in \"7\" and the top curve in \"0\" â€” both are horizontal features, but they appear in different places. This is **weight sharing** in action: one filter, many locations.\n",
    "\n",
    "### ğŸ’» Exercise 8.4 â€” What Gets Misclassified?\n",
    "\n",
    "**Task:** Find some misclassified digits, show them with their Conv1 feature maps, and analyze why the CNN failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "wrong_indices = np.where(all_preds != all_labels)[0]\n",
    "print(f\"Misclassified: {len(wrong_indices)} out of {len(all_labels)} \"\n",
    "      f\"({len(wrong_indices)/len(all_labels)*100:.2f}%)\")\n",
    "\n",
    "# Show first 5 misclassified digits with their feature maps\n",
    "fig, axes = plt.subplots(5, 7, figsize=(18, 13))\n",
    "\n",
    "for row in range(min(5, len(wrong_indices))):\n",
    "    idx = wrong_indices[row]\n",
    "    img, true_label = test_dataset[idx]\n",
    "    pred_label = all_preds[idx]\n",
    "    \n",
    "    # Original\n",
    "    axes[row, 0].imshow(img.squeeze(), cmap='gray')\n",
    "    axes[row, 0].set_title(f'True: {true_label}\\nPred: {pred_label}', \n",
    "                            fontsize=11, color='red')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Feature maps\n",
    "    with torch.no_grad():\n",
    "        fmaps = model.features[1](model.features[0](img.unsqueeze(0)))\n",
    "    \n",
    "    for i in range(6):\n",
    "        axes[row, i+1].imshow(fmaps[0, i].numpy(), cmap='viridis')\n",
    "        axes[row, i+1].axis('off')\n",
    "\n",
    "plt.suptitle('Misclassified Digits: Where the CNN Fails', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dabea2",
   "metadata": {},
   "source": [
    "**Write in your notebook:** For 2â€“3 of the misclassified digits, can you see **why** the CNN was confused? Do the feature maps look ambiguous?\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Architecture Experiments {#experiments}\n",
    "\n",
    "### ğŸ’» Exercise 9.1 â€” CNN vs MLP: Head-to-Head\n",
    "\n",
    "**Task:** Train an MLP and a CNN on the same data and training setup, and compare fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20663c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP baseline (from Session 9, adapted for DataLoader)\n",
    "class MLP_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),           # (batch, 1, 28, 28) â†’ (batch, 784)\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# TODO: Train both models for 10 epochs, same lr, same data\n",
    "mlp = MLP_MNIST()\n",
    "cnn = LeNet5()\n",
    "\n",
    "print(\"Training MLP...\")\n",
    "mlp_losses, mlp_accs = train_cnn(mlp, train_loader, test_loader, n_epochs=10, lr=0.001)\n",
    "\n",
    "print(\"\\nTraining CNN...\")\n",
    "cnn_losses, cnn_accs = train_cnn(cnn, train_loader, test_loader, n_epochs=10, lr=0.001)\n",
    "\n",
    "# TODO: Plot both on the same figure (accuracy over epochs)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "___\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Parameter comparison\n",
    "mlp_params = sum(p.numel() for p in mlp.parameters())\n",
    "cnn_params = sum(p.numel() for p in cnn.parameters())\n",
    "print(f\"\\nMLP parameters:  {mlp_params:,}\")\n",
    "print(f\"CNN parameters:  {cnn_params:,}\")\n",
    "print(f\"CNN / MLP ratio: {cnn_params/mlp_params:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaaa57b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” plot</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f836ae",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(range(1, 11), mlp_accs, 'b-o', linewidth=2, label=f'MLP ({mlp_params:,} params)')\n",
    "ax.plot(range(1, 11), cnn_accs, 'g-o', linewidth=2, label=f'CNN ({cnn_params:,} params)')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=14)\n",
    "ax.set_title('MLP vs CNN on MNIST', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_ylim(95, 100)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb2f6e",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### ğŸ’» Exercise 9.2 â€” Architecture Variations\n",
    "\n",
    "**Task:** Modify LeNet-5 and measure the impact. Implement **three variants** and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c30ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant A: Deeper â€” add a third conv block\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # TODO: Conv(1â†’6, 3Ã—3, pad=1) â†’ ReLU â†’ Pool(2,2)\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "            # TODO: Conv(6â†’16, 3Ã—3, pad=1) â†’ ReLU â†’ Pool(2,2)\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "            # TODO: Conv(16â†’32, 3Ã—3, pad=1) â†’ ReLU â†’ Pool(2,2)\n",
    "            # (After: 32 Ã— 3 Ã— 3 = 288 â†’ need to adjust FC input)\n",
    "            ___,\n",
    "            ___,\n",
    "            ___,\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 3 * 3, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Variant B: Wider â€” more filters\n",
    "class WideCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 5), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Variant C: Tiny â€” minimal CNN\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Just one conv layer! Conv(1â†’8, 5Ã—5) â†’ ReLU â†’ Pool(2,2) â†’ Flatten â†’ FC â†’ 10\n",
    "        ___\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0dbc1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” DeepCNN features and TinyCNN</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff12e0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# DeepCNN features\n",
    "self.features = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),    # â†’ 6Ã—14Ã—14\n",
    "    nn.Conv2d(6, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),   # â†’ 16Ã—7Ã—7\n",
    "    nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),  # â†’ 32Ã—3Ã—3\n",
    ")\n",
    "\n",
    "# TinyCNN\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 5), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Linear(8 * 12 * 12, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28722fc7",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Task:** Train all variants and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = {\n",
    "    \"LeNet-5\": LeNet5(),\n",
    "    \"Deep (3 conv)\": DeepCNN(),\n",
    "    \"Wide (32â†’64)\": WideCNN(),\n",
    "    \"Tiny (1 conv)\": TinyCNN(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model_v in variants.items():\n",
    "    n_params = sum(p.numel() for p in model_v.parameters())\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name} ({n_params:,} parameters)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    losses, accs = train_cnn(model_v, train_loader, test_loader, n_epochs=10, lr=0.001)\n",
    "    results[name] = {\"losses\": losses, \"accs\": accs, \"params\": n_params}\n",
    "\n",
    "# TODO: Plot all accuracy curves on one figure, with legend showing param count\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for name, res in results.items():\n",
    "    ax.plot(range(1, 11), res[\"accs\"], '-o', linewidth=2, markersize=5,\n",
    "            label=f'{name} ({res[\"params\"]:,} params, {res[\"accs\"][-1]:.2f}%)')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=14)\n",
    "ax.set_title('CNN Architecture Comparison on MNIST', fontsize=16)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700499b",
   "metadata": {},
   "source": [
    "### âœï¸ Exercise 9.3 â€” Analysis Questions\n",
    "\n",
    "Answer in your notebook:\n",
    "\n",
    "1. **Which architecture achieved the best accuracy?** Is the best one also the biggest?\n",
    "2. **TinyCNN has only 1 conv layer.** How does it compare to the Session 9 MLP? What does this tell you about the value of even a single convolution?\n",
    "3. **DeepCNN has 3 conv layers.** Did it beat LeNet-5? After pooling 3 times ($28 \\to 14 \\to 7 \\to 3$), the feature maps are only $3 \\times 3$. Is this too small?\n",
    "4. **WideCNN uses dropout in the FC layer.** Based on Session 8, why is this a good idea here?\n",
    "5. You have a new task: classify $64 \\times 64$ RGB images into 100 classes. Based on what you've learned, sketch an architecture (layers, filter sizes, channels). How many conv+pool blocks would you use?\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Summary {#summary}\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "âœ… **Max pooling**: Keep strongest activations, reduce spatial dimensions by half  \n",
    "âœ… **CNN architecture**: Convâ†’ReLUâ†’Pool (feature extractor) â†’ Flatten â†’ FC (classifier)  \n",
    "âœ… **The funnel**: Channels grow, spatial dims shrink  \n",
    "âœ… **LeNet-5**: The classic CNN â€” 44K parameters, 99%+ on MNIST  \n",
    "âœ… **DataLoader**: Batch training on full datasets  \n",
    "âœ… **Data augmentation**: Random transforms to improve generalization  \n",
    "âœ… **Feature map visualization**: See what each filter detects  \n",
    "âœ… **Architecture experiments**: Deeper, wider, and smaller variants\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **CNNs beat MLPs on images by exploiting spatial structure:**\n",
    "   - Fewer parameters through weight sharing\n",
    "   - Translation invariance through convolutions + pooling\n",
    "   - Hierarchical features through stacking\n",
    "\n",
    "2. **What CNNs learn automatically:**\n",
    "   - Layer 1: Edge detectors (just like our Sobel kernels from Session 10!)\n",
    "   - Layer 2: Combinations of edges â†’ corners, curves, textures\n",
    "   - FC layers: Combine spatial features into class predictions\n",
    "\n",
    "3. **Architecture matters, but not as much as you'd think:**\n",
    "   - Even a tiny 1-conv-layer CNN beats an MLP\n",
    "   - Going deeper helps, but with diminishing returns on simple tasks\n",
    "   - Wider (more filters) can be as effective as deeper\n",
    "\n",
    "### The Complete Pipeline (Sessions 1â€“11)\n",
    "\n",
    "```\n",
    "Session 2:  Perceptron        â†’ a single neuron classifies\n",
    "Session 4:  MLP               â†’ hidden layers solve XOR\n",
    "Session 5:  Gradient descent   â†’ automatic weight learning\n",
    "Session 6:  Backpropagation    â†’ trains deep networks\n",
    "Session 7:  Softmax + CCE      â†’ multi-class classification\n",
    "Session 8:  Regularization     â†’ prevents overfitting\n",
    "Session 9:  PyTorch            â†’ frameworks automate everything\n",
    "Session 10: Convolutions       â†’ exploit spatial structure\n",
    "Session 11: CNNs               â†’ state-of-the-art image classification\n",
    "```\n",
    "\n",
    "From a single neuron that computes $y = \\text{step}(wx + b)$ to a convolutional network that classifies handwritten digits at 99%+ accuracy â€” in 11 sessions!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Session 12: Final Project & Best Practices**\n",
    "\n",
    "In the final session:\n",
    "- **End-to-end ML pipeline**: From data loading to results analysis\n",
    "- **Professional practices**: Code organization, experiment tracking, reproducibility\n",
    "- **Final project**: Apply everything to a new challenge\n",
    "  - Option A: Image classification on a new dataset\n",
    "  - Option B: MLP vs CNN comparison study\n",
    "  - Option C: Custom architecture design challenge\n",
    "\n",
    "**The goal:** Demonstrate your complete understanding by building something on your own!\n",
    "\n",
    "### Before Next Session\n",
    "\n",
    "**Prepare your final project:**\n",
    "1. Choose one of the three project options (or propose your own)\n",
    "2. Think about your approach: What architecture? What hyperparameters? How will you evaluate?\n",
    "3. Review Sessions 7â€“11: loss functions, regularization, training loop, CNN architecture\n",
    "4. Bring questions!\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 11** ğŸ“\n",
    "\n",
    "**You now understand:**\n",
    "- âœ… How to design and build CNN architectures\n",
    "- âœ… How to train on real datasets with DataLoader\n",
    "- âœ… What CNNs learn and why they work\n",
    "\n",
    "**Next up:** The Final Project â€” putting it all together! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
