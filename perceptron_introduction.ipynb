{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning: The Perceptron\n",
    "\n",
    "**Course: Neural Networks and Machine Learning**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Motivation: From Biology to Mathematics](#motivation)\n",
    "2. [The Perceptron Model](#model)\n",
    "3. [Geometric Interpretation](#geometry)\n",
    "4. [The Learning Algorithm](#algorithm)\n",
    "5. [Implementation from Scratch](#implementation)\n",
    "6. [Applications and Examples](#examples)\n",
    "7. [Limitations and Linear Separability](#limitations)\n",
    "8. [Exercises](#exercises)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation: From Biology to Mathematics <a name=\"motivation\"></a>\n",
    "\n",
    "### The Biological Neuron\n",
    "\n",
    "The perceptron is inspired by biological neurons:\n",
    "- **Dendrites** receive signals from other neurons\n",
    "- The **cell body** integrates these signals\n",
    "- If the integrated signal exceeds a threshold, the **axon** fires\n",
    "\n",
    "### The Mathematical Abstraction\n",
    "\n",
    "The perceptron (Rosenblatt, 1957) translates this into mathematics:\n",
    "\n",
    "$$\n",
    "y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$ are input features (like signals from dendrites)\n",
    "- $w_i$ are weights (synaptic strengths)\n",
    "- $b$ is the bias (threshold)\n",
    "- $f$ is the activation function (firing rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Perceptron Model <a name=\"model\"></a>\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "A perceptron is defined by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(\\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$\n",
    "\n",
    "Or equivalently, using an augmented weight vector:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(\\mathbf{w}^T \\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x} = [1, x_1, x_2, \\ldots, x_n]^T$ and $\\mathbf{w} = [b, w_1, w_2, \\ldots, w_n]^T$\n",
    "\n",
    "The sign function is:\n",
    "\n",
    "$$\n",
    "\\text{sign}(z) = \\begin{cases}\n",
    "+1 & \\text{if } z \\geq 0 \\\\\n",
    "-1 & \\text{if } z < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Binary Classification**: Output is either +1 or -1\n",
    "2. **Linear Decision Boundary**: The perceptron creates a hyperplane in feature space\n",
    "3. **Supervised Learning**: We need labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sign activation function\n",
    "z = np.linspace(-5, 5, 1000)\n",
    "sign_output = np.sign(z)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(z, sign_output, 'b-', linewidth=2, label='sign(z)')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('z', fontsize=12)\n",
    "plt.ylabel('sign(z)', fontsize=12)\n",
    "plt.title('Perceptron Activation Function', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geometric Interpretation <a name=\"geometry\"></a>\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "The perceptron defines a **hyperplane** in the input space:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^T \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "For 2D input $(x_1, x_2)$, this becomes a line:\n",
    "\n",
    "$$\n",
    "w_1 x_1 + w_2 x_2 + b = 0\n",
    "$$\n",
    "\n",
    "Or in slope-intercept form:\n",
    "\n",
    "$$\n",
    "x_2 = -\\frac{w_1}{w_2}x_1 - \\frac{b}{w_2}\n",
    "$$\n",
    "\n",
    "### Weight Vector Properties\n",
    "\n",
    "- The weight vector $\\mathbf{w}$ is **perpendicular** to the decision boundary\n",
    "- $\\|\\mathbf{w}\\|$ determines the \"confidence\" of classification\n",
    "- Points on one side of the hyperplane are classified as +1, others as -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing geometric interpretation\n",
    "def plot_decision_boundary(w, b, xlim=(-5, 5), ylim=(-5, 5)):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for a 2D perceptron\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Create a mesh\n",
    "    x1 = np.linspace(xlim[0], xlim[1], 100)\n",
    "    x2 = np.linspace(ylim[0], ylim[1], 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    \n",
    "    # Calculate decision function\n",
    "    Z = w[0] * X1 + w[1] * X2 + b\n",
    "    \n",
    "    # Plot decision regions\n",
    "    ax.contourf(X1, X2, Z, levels=[-1000, 0, 1000], colors=['lightblue', 'lightcoral'], alpha=0.3)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contour(X1, X2, Z, levels=[0], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot weight vector (perpendicular to boundary)\n",
    "    origin = np.array([0, 0])\n",
    "    ax.quiver(*origin, w[0], w[1], scale=1, scale_units='xy', angles='xy', \n",
    "              color='red', width=0.008, label='Weight vector w')\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(f'Decision Boundary: $w_1={w[0]}, w_2={w[1]}, b={b}$', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.text(xlim[1]-1, ylim[0]+0.5, 'Class -1', fontsize=12, color='blue')\n",
    "    ax.text(xlim[0]+0.5, ylim[1]-0.5, 'Class +1', fontsize=12, color='red')\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Example: visualize a perceptron\n",
    "w = np.array([2, 1])\n",
    "b = -3\n",
    "plot_decision_boundary(w, b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3.1 (Easy)\n",
    "\n",
    "**Question**: Given weights $w_1 = 1$, $w_2 = -2$, and bias $b = 4$:\n",
    "\n",
    "a) Write the equation of the decision boundary\n",
    "\n",
    "b) Classify the following points: $(0, 0)$, $(5, 1)$, $(1, 3)$\n",
    "\n",
    "c) Visualize the decision boundary using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Learning Algorithm <a name=\"algorithm\"></a>\n",
    "\n",
    "### The Perceptron Learning Rule\n",
    "\n",
    "The perceptron uses a simple but powerful learning algorithm:\n",
    "\n",
    "**For each training example** $(\\mathbf{x}^{(i)}, y^{(i)})$:\n",
    "\n",
    "1. **Predict**: $\\hat{y}^{(i)} = \\text{sign}(\\mathbf{w}^T \\mathbf{x}^{(i)})$\n",
    "\n",
    "2. **Update** (if wrong):\n",
    "   $$\n",
    "   \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot (y^{(i)} - \\hat{y}^{(i)}) \\cdot \\mathbf{x}^{(i)}\n",
    "   $$\n",
    "   \n",
    "   where $\\eta$ is the **learning rate** (typically 0.01 to 1.0)\n",
    "\n",
    "### Understanding the Update Rule\n",
    "\n",
    "- **If correct** ($y^{(i)} = \\hat{y}^{(i)}$): No update needed\n",
    "- **If wrong**:\n",
    "  - When $y^{(i)} = +1$ but $\\hat{y}^{(i)} = -1$: Move $\\mathbf{w}$ toward $\\mathbf{x}^{(i)}$\n",
    "  - When $y^{(i)} = -1$ but $\\hat{y}^{(i)} = +1$: Move $\\mathbf{w}$ away from $\\mathbf{x}^{(i)}$\n",
    "\n",
    "### Perceptron Convergence Theorem\n",
    "\n",
    "**Theorem (Novikoff, 1962)**: If the training data is **linearly separable**, the perceptron algorithm will converge to a solution in a finite number of steps.\n",
    "\n",
    "$$\n",
    "\\text{Number of mistakes} \\leq \\left(\\frac{R}{\\gamma}\\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $R$ = maximum norm of training examples\n",
    "- $\\gamma$ = margin (minimum distance to decision boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Pseudocode\n",
    "\n",
    "```\n",
    "PERCEPTRON-TRAIN(X, y, Œ∑, max_epochs):\n",
    "    Initialize w = 0, b = 0\n",
    "    \n",
    "    for epoch = 1 to max_epochs:\n",
    "        errors = 0\n",
    "        \n",
    "        for each (x_i, y_i) in training data:\n",
    "            # Predict\n",
    "            ≈∑_i = sign(w^T x_i + b)\n",
    "            \n",
    "            # Update if wrong\n",
    "            if ≈∑_i ‚â† y_i:\n",
    "                w ‚Üê w + Œ∑ ¬∑ y_i ¬∑ x_i\n",
    "                b ‚Üê b + Œ∑ ¬∑ y_i\n",
    "                errors += 1\n",
    "        \n",
    "        if errors == 0:\n",
    "            break  # Converged!\n",
    "    \n",
    "    return w, b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementation from Scratch <a name=\"implementation\"></a>\n",
    "\n",
    "Let's implement a perceptron class from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Perceptron classifier implementation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float (default=0.01)\n",
    "        Learning rate (eta)\n",
    "    max_epochs : int (default=1000)\n",
    "        Maximum number of passes over the training data\n",
    "    random_state : int (default=None)\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_epochs=1000, random_state=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.random_state = random_state\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_per_epoch = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the perceptron to training data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values (+1 or -1)\n",
    "        \"\"\"\n",
    "        # Initialize random number generator\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        self.errors_per_epoch = []\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.max_epochs):\n",
    "            errors = 0\n",
    "            \n",
    "            for xi, yi in zip(X, y):\n",
    "                # Calculate prediction\n",
    "                linear_output = np.dot(xi, self.weights) + self.bias\n",
    "                y_pred = np.sign(linear_output)\n",
    "                \n",
    "                # Update if misclassified\n",
    "                if y_pred != yi:\n",
    "                    update = self.learning_rate * yi\n",
    "                    self.weights += update * xi\n",
    "                    self.bias += update\n",
    "                    errors += 1\n",
    "            \n",
    "            self.errors_per_epoch.append(errors)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {epoch + 1} epochs\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Samples to predict\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array, shape (n_samples,)\n",
    "            Predicted class labels (+1 or -1)\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return np.sign(linear_output)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy on test data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        y : array-like, shape (n_samples,)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy : float\n",
    "            Fraction of correctly classified samples\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "print(\"Perceptron class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applications and Examples <a name=\"examples\"></a>\n",
    "\n",
    "### Example 6.1: Logical AND Function\n",
    "\n",
    "Let's start with a simple example - teaching a perceptron to learn the AND logic gate.\n",
    "\n",
    "Truth table:\n",
    "```\n",
    "x1  x2  | AND\n",
    "--------+-----\n",
    " 0   0  |  0\n",
    " 0   1  |  0\n",
    " 1   0  |  0\n",
    " 1   1  |  1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND gate dataset (converting 0/1 to -1/+1 for perceptron)\n",
    "X_and = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "\n",
    "y_and = np.array([-1, -1, -1, 1])  # -1 for False, +1 for True\n",
    "\n",
    "# Train perceptron\n",
    "perceptron_and = Perceptron(learning_rate=0.1, max_epochs=10, random_state=42)\n",
    "perceptron_and.fit(X_and, y_and)\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nAND Gate Predictions:\")\n",
    "for xi, yi_true in zip(X_and, y_and):\n",
    "    yi_pred = perceptron_and.predict(xi.reshape(1, -1))[0]\n",
    "    print(f\"Input: {xi} | True: {yi_true:+d} | Predicted: {yi_pred:+d}\")\n",
    "\n",
    "print(f\"\\nAccuracy: {perceptron_and.score(X_and, y_and):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot 1: Learning curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(perceptron_and.errors_per_epoch) + 1), \n",
    "         perceptron_and.errors_per_epoch, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Number of Errors', fontsize=12)\n",
    "plt.title('Learning Curve (AND Gate)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "x1_min, x1_max = -0.5, 1.5\n",
    "x2_min, x2_max = -0.5, 1.5\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                       np.linspace(x2_min, x2_max, 100))\n",
    "Z = perceptron_and.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=ListedColormap(['blue', 'red']))\n",
    "plt.scatter(X_and[y_and == -1, 0], X_and[y_and == -1, 1], \n",
    "            c='blue', marker='o', s=200, edgecolors='k', label='Class -1')\n",
    "plt.scatter(X_and[y_and == 1, 0], X_and[y_and == 1, 1], \n",
    "            c='red', marker='s', s=200, edgecolors='k', label='Class +1')\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('Decision Boundary (AND Gate)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.xlim(x1_min, x1_max)\n",
    "plt.ylim(x2_min, x2_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6.2: Iris Dataset (2 Features)\n",
    "\n",
    "Let's apply the perceptron to a real dataset - classifying two species of Iris flowers using only two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Select only two classes (linearly separable) and two features\n",
    "# Class 0 (Setosa) vs Class 1 (Versicolor)\n",
    "X_iris = iris.data[:100, [0, 2]]  # sepal length and petal length\n",
    "y_iris = iris.target[:100]\n",
    "y_iris = np.where(y_iris == 0, -1, 1)  # Convert to -1/+1\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris_scaled, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train perceptron\n",
    "perceptron_iris = Perceptron(learning_rate=0.1, max_epochs=100, random_state=42)\n",
    "perceptron_iris.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_accuracy = perceptron_iris.score(X_train, y_train)\n",
    "test_accuracy = perceptron_iris.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot decision regions for 2D classification\n",
    "    \"\"\"\n",
    "    markers = ('o', 's')\n",
    "    colors = ('blue', 'red')\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Plot decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    \n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # Plot samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                   alpha=0.8, c=colors[idx],\n",
    "                   marker=markers[idx], s=80,\n",
    "                   label=f'Class {cl}', edgecolors='black')\n",
    "    \n",
    "    plt.xlabel('Sepal length (standardized)', fontsize=12)\n",
    "    plt.ylabel('Petal length (standardized)', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training and test results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_regions(X_train, y_train, perceptron_iris, \n",
    "                     title='Training Data (Iris)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_regions(X_test, y_test, perceptron_iris,\n",
    "                     title='Test Data (Iris)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(perceptron_iris.errors_per_epoch) + 1),\n",
    "         perceptron_iris.errors_per_epoch, 'o-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Number of Misclassifications', fontsize=12)\n",
    "plt.title('Perceptron Learning Curve (Iris Dataset)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 6.1 (Easy)\n",
    "\n",
    "**Question**: Train a perceptron to learn the OR logic gate.\n",
    "\n",
    "Truth table:\n",
    "```\n",
    "x1  x2  | OR\n",
    "--------+----\n",
    " 0   0  |  0\n",
    " 0   1  |  1\n",
    " 1   0  |  1\n",
    " 1   1  |  1\n",
    "```\n",
    "\n",
    "a) Create the training data\n",
    "\n",
    "b) Train the perceptron\n",
    "\n",
    "c) Visualize the decision boundary\n",
    "\n",
    "d) What are the learned weights and bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limitations and Linear Separability <a name=\"limitations\"></a>\n",
    "\n",
    "### The XOR Problem\n",
    "\n",
    "The most famous limitation of the perceptron was discovered by Minsky and Papert (1969): **it cannot learn XOR**.\n",
    "\n",
    "XOR truth table:\n",
    "```\n",
    "x1  x2  | XOR\n",
    "--------+-----\n",
    " 0   0  |  0\n",
    " 0   1  |  1\n",
    " 1   0  |  1\n",
    " 1   1  |  0\n",
    "```\n",
    "\n",
    "### Why XOR is Impossible for a Single Perceptron\n",
    "\n",
    "XOR is **not linearly separable**: there is no single straight line that can separate the classes.\n",
    "\n",
    "**Mathematical Proof:**\n",
    "\n",
    "For a perceptron to classify XOR correctly, we need:\n",
    "- $w_1 \\cdot 0 + w_2 \\cdot 0 + b < 0$ (for class -1)\n",
    "- $w_1 \\cdot 0 + w_2 \\cdot 1 + b > 0$ (for class +1)\n",
    "- $w_1 \\cdot 1 + w_2 \\cdot 0 + b > 0$ (for class +1)\n",
    "- $w_1 \\cdot 1 + w_2 \\cdot 1 + b < 0$ (for class -1)\n",
    "\n",
    "From equations 2 and 3: $w_2 > -b$ and $w_1 > -b$\n",
    "\n",
    "Adding these: $w_1 + w_2 > -2b$\n",
    "\n",
    "But equation 4 requires: $w_1 + w_2 < -b$\n",
    "\n",
    "This is a **contradiction**! Therefore, no single perceptron can learn XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate XOR failure\n",
    "X_xor = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "\n",
    "y_xor = np.array([-1, 1, 1, -1])  # XOR labels\n",
    "\n",
    "# Try to train perceptron on XOR\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, max_epochs=100, random_state=42)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Check predictions\n",
    "print(\"\\nXOR Predictions (will fail):\")\n",
    "for xi, yi_true in zip(X_xor, y_xor):\n",
    "    yi_pred = perceptron_xor.predict(xi.reshape(1, -1))[0]\n",
    "    status = \"‚úì\" if yi_pred == yi_true else \"‚úó\"\n",
    "    print(f\"{status} Input: {xi} | True: {yi_true:+d} | Predicted: {yi_pred:+d}\")\n",
    "\n",
    "print(f\"\\nAccuracy: {perceptron_xor.score(X_xor, y_xor):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XOR problem\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "datasets = [\n",
    "    (X_and, y_and, \"AND (Linearly Separable)\"),\n",
    "    (X_xor, y_xor, \"XOR (NOT Linearly Separable)\"),\n",
    "]\n",
    "\n",
    "for idx, (X, y, title) in enumerate(datasets):\n",
    "    plt.subplot(1, 3, idx + 1)\n",
    "    plt.scatter(X[y == -1, 0], X[y == -1, 1], \n",
    "                c='blue', marker='o', s=200, edgecolors='k', label='Class -1')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], \n",
    "                c='red', marker='s', s=200, edgecolors='k', label='Class +1')\n",
    "    plt.xlabel('$x_1$', fontsize=12)\n",
    "    plt.ylabel('$x_2$', fontsize=12)\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curve for XOR\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, len(perceptron_xor.errors_per_epoch) + 1),\n",
    "         perceptron_xor.errors_per_epoch, 'o-', linewidth=2, color='darkred')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Number of Errors', fontsize=12)\n",
    "plt.title('XOR: Perceptron Cannot Converge', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Linear Separability\n",
    "\n",
    "A dataset is **linearly separable** if there exists a hyperplane that perfectly separates the two classes:\n",
    "\n",
    "$$\n",
    "\\exists \\mathbf{w}, b : \\text{sign}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) = y^{(i)} \\quad \\forall i\n",
    "$$\n",
    "\n",
    "### Solutions to XOR Problem\n",
    "\n",
    "1. **Feature Engineering**: Add polynomial features (e.g., $x_1 x_2$)\n",
    "2. **Multi-Layer Networks**: Stack multiple perceptrons (‚Üí Neural Networks!)\n",
    "3. **Kernel Methods**: Map to higher-dimensional space (‚Üí SVMs)\n",
    "\n",
    "This limitation led to the development of **multi-layer perceptrons (MLPs)** and modern deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 7.1 (Medium)\n",
    "\n",
    "**Feature Engineering for XOR**\n",
    "\n",
    "We can make XOR linearly separable by adding a polynomial feature!\n",
    "\n",
    "**Question**: \n",
    "a) Add the feature $x_3 = x_1 \\cdot x_2$ to the XOR dataset\n",
    "\n",
    "b) Train a perceptron on this augmented dataset\n",
    "\n",
    "c) Verify it now works!\n",
    "\n",
    "d) Visualize the 3D feature space (optional challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# Hint: Create X_xor_augmented with shape (4, 3) where the third column is x1 * x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises <a name=\"exercises\"></a>\n",
    "\n",
    "### Easy Exercises\n",
    "\n",
    "#### üìù Exercise 8.1: Understanding the Math\n",
    "\n",
    "Given a perceptron with weights $\\mathbf{w} = [2, -1]^T$ and bias $b = 1$:\n",
    "\n",
    "a) Calculate the output for inputs: $(0, 0)$, $(1, 2)$, $(3, 1)$, $(-1, 4)$\n",
    "\n",
    "b) Write the equation of the decision boundary in the form $x_2 = mx_1 + c$\n",
    "\n",
    "c) Draw the decision boundary by hand (or with matplotlib)\n",
    "\n",
    "d) What is the distance from the origin to the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù Exercise 8.2: Logical Gates\n",
    "\n",
    "a) Train a perceptron to learn the NAND gate (NOT AND)\n",
    "\n",
    "b) Compare the learned weights with the AND gate\n",
    "\n",
    "c) Can you predict the weights for NAND before training? Verify your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Exercises\n",
    "\n",
    "#### üìù Exercise 8.3: Effect of Learning Rate\n",
    "\n",
    "Using the Iris dataset from Example 6.2:\n",
    "\n",
    "a) Train perceptrons with different learning rates: $\\eta \\in \\{0.001, 0.01, 0.1, 1.0\\}$\n",
    "\n",
    "b) Plot the learning curves (errors vs epochs) for all learning rates on the same graph\n",
    "\n",
    "c) Which learning rate converges fastest? Which is most stable?\n",
    "\n",
    "d) What happens if $\\eta$ is too large (e.g., $\\eta = 10$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù Exercise 8.4: Margin Analysis\n",
    "\n",
    "Create a linearly separable 2D dataset where one class is clustered around $(0, 0)$ and another around $(4, 4)$.\n",
    "\n",
    "a) Generate 50 samples per class using `np.random.randn`\n",
    "\n",
    "b) Train a perceptron and visualize the decision boundary\n",
    "\n",
    "c) Calculate the **margin** (distance from the closest points to the decision boundary)\n",
    "\n",
    "d) What happens to convergence speed as you increase the margin by moving classes further apart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù Exercise 8.5: Pocket Algorithm\n",
    "\n",
    "The **Pocket Algorithm** (Gallant, 1990) is a variant of the perceptron that works better on non-separable data.\n",
    "\n",
    "**Algorithm**: Keep track of the best weights seen so far (the ones with fewest errors).\n",
    "\n",
    "a) Implement the Pocket Algorithm as a new class `PocketPerceptron`\n",
    "\n",
    "b) Test it on the XOR dataset\n",
    "\n",
    "c) Compare its performance with the standard perceptron\n",
    "\n",
    "d) What is the best accuracy it can achieve on XOR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "class PocketPerceptron(Perceptron):\n",
    "    # Modify the fit method to keep track of best weights\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Exercises\n",
    "\n",
    "#### üìù Exercise 8.6: Multi-Class Perceptron\n",
    "\n",
    "Extend the perceptron to handle **multi-class classification** using the **one-vs-all** strategy.\n",
    "\n",
    "a) Implement a `MultiClassPerceptron` class that:\n",
    "   - Trains K binary perceptrons (one per class)\n",
    "   - Predicts by choosing the class with highest confidence score\n",
    "\n",
    "b) Test it on the full Iris dataset (3 classes)\n",
    "\n",
    "c) Visualize decision boundaries for all three classes\n",
    "\n",
    "d) Calculate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "class MultiClassPerceptron:\n",
    "    def __init__(self, learning_rate=0.01, max_epochs=1000, random_state=None):\n",
    "        # Your implementation\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Train one perceptron per class\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Return class with highest score\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù Exercise 8.7: Perceptron Convergence Theorem\n",
    "\n",
    "Verify the **Perceptron Convergence Theorem** experimentally.\n",
    "\n",
    "The theorem states that for linearly separable data:\n",
    "$$\n",
    "\\text{Number of mistakes} \\leq \\left(\\frac{R}{\\gamma}\\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $R = \\max_i \\|\\mathbf{x}^{(i)}\\|$ (maximum norm)\n",
    "- $\\gamma$ is the margin (distance from points to optimal decision boundary)\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "a) Generate several linearly separable datasets with different margins\n",
    "\n",
    "b) For each dataset:\n",
    "   - Calculate $R$ and estimate $\\gamma$\n",
    "   - Train a perceptron and count total mistakes (sum of all errors)\n",
    "   - Verify that mistakes $\\leq (R/\\gamma)^2$\n",
    "\n",
    "c) Plot: mistakes vs $(R/\\gamma)^2$ for multiple datasets\n",
    "\n",
    "d) What happens to convergence time as the margin decreases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù Exercise 8.8: Voted Perceptron\n",
    "\n",
    "The **Voted Perceptron** (Freund & Schapire, 1999) is an ensemble method.\n",
    "\n",
    "**Idea**: Instead of keeping one weight vector, keep all weight vectors seen during training, along with how long each survived. At test time, take a weighted vote.\n",
    "\n",
    "a) Implement the `VotedPerceptron` class\n",
    "\n",
    "b) Compare it with standard perceptron on noisy data:\n",
    "   - Generate linearly separable data\n",
    "   - Add 10% label noise (flip some labels randomly)\n",
    "   - Compare accuracies\n",
    "\n",
    "c) Plot the ensemble of decision boundaries\n",
    "\n",
    "d) Why does voting help with noisy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù Exercise 8.9: Kernel Perceptron (Challenge)\n",
    "\n",
    "The **Kernel Perceptron** uses the kernel trick to learn non-linear decision boundaries.\n",
    "\n",
    "**Key insight**: The weight vector can be written as:\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y^{(i)} \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "So we only need dot products $\\mathbf{x}^{(i)} \\cdot \\mathbf{x}^{(j)}$, which can be replaced by kernels!\n",
    "\n",
    "a) Implement a `KernelPerceptron` that:\n",
    "   - Stores $\\alpha_i$ values instead of weights\n",
    "   - Uses a kernel function $k(\\mathbf{x}_i, \\mathbf{x}_j)$\n",
    "\n",
    "b) Implement RBF (Gaussian) kernel:\n",
    "$$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "c) Test on the XOR dataset (without feature engineering!)\n",
    "\n",
    "d) Visualize the non-linear decision boundary\n",
    "\n",
    "**Hint**: Prediction becomes:\n",
    "$$\n",
    "\\hat{y} = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y^{(i)} k(\\mathbf{x}^{(i)}, \\mathbf{x})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here (advanced!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Perceptron Model**: $\\hat{y} = \\text{sign}(\\mathbf{w}^T \\mathbf{x} + b)$\n",
    "\n",
    "2. **Learning Rule**: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (y - \\hat{y}) \\mathbf{x}$\n",
    "\n",
    "3. **Convergence**: Guaranteed for linearly separable data\n",
    "\n",
    "4. **Limitations**: Cannot learn XOR (not linearly separable)\n",
    "\n",
    "5. **Solutions**: Feature engineering, multi-layer networks, kernels\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The perceptron is the foundation of neural networks\n",
    "- Understanding its limitations led to deep learning\n",
    "- Simple algorithm, powerful geometric interpretation\n",
    "- Still useful for large-scale online learning\n",
    "\n",
    "### Next Topics\n",
    "\n",
    "- Multi-layer Perceptrons (MLPs)\n",
    "- Backpropagation\n",
    "- Deep Neural Networks\n",
    "- Modern architectures (CNNs, RNNs, Transformers)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Rosenblatt, F. (1957). \"The Perceptron: A Perceiving and Recognizing Automaton\"\n",
    "2. Minsky, M., & Papert, S. (1969). \"Perceptrons\"\n",
    "3. Novikoff, A. B. (1962). \"On convergence proofs on perceptrons\"\n",
    "4. Freund, Y., & Schapire, R. E. (1999). \"Large margin classification using the perceptron algorithm\"\n",
    "5. Bishop, C. M. (2006). \"Pattern Recognition and Machine Learning\"\n",
    "6. Goodfellow, I., Bengio, Y., & Courville, A. (2016). \"Deep Learning\"\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
