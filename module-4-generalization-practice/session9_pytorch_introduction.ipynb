{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb97b9c5",
   "metadata": {},
   "source": [
    "# Session 9: PyTorch Introduction\n",
    "## From Scratch to Framework\n",
    "\n",
    "**Course: Neural Networks for Engineers**  \n",
    "**Duration: 2 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Part I â€” PyTorch Fundamentals (â‰ˆ 50 min)\n",
    "1. [Recap & Motivation](#recap)\n",
    "2. [Tensors: The Building Block](#tensors)\n",
    "3. [Autograd: Automatic Differentiation](#autograd)\n",
    "4. [nn.Module: Building Networks](#nn-module)\n",
    "5. [The PyTorch Training Loop](#training-loop)\n",
    "\n",
    "### Part II â€” Mini-Projects (â‰ˆ 70 min)\n",
    "6. [Mini-Project A: From Perceptron to MLP in PyTorch](#project-a)\n",
    "7. [Mini-Project B: The Full Pipeline â€” Spiral Classifier](#project-b)\n",
    "8. [Mini-Project C: MNIST â€” Your First Real Dataset](#project-c)\n",
    "\n",
    "---\n",
    "\n",
    "# Part I â€” PyTorch Fundamentals\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap & Motivation {#recap}\n",
    "\n",
    "### What We've Built So Far (by Hand)\n",
    "\n",
    "Over Sessions 2â€“8, we implemented **everything** from scratch in NumPy:\n",
    "\n",
    "| Component | Session | Lines of code |\n",
    "|---|---|---|\n",
    "| Forward propagation | 4 | ~20 |\n",
    "| Backpropagation | 6 | ~30 |\n",
    "| Cross-entropy + softmax | 7 | ~15 |\n",
    "| Early stopping | 8 | ~30 |\n",
    "| L2 regularization | 8 | 3 lines added |\n",
    "| Dropout | 8 | 2 lines added |\n",
    "| SGD, Momentum, Adam | 8 | ~40 |\n",
    "| **Total** | | **~170 lines** |\n",
    "\n",
    "And we still only support **2-layer networks** with **one activation function**.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "To add a third hidden layer, we'd need to:\n",
    "- Write new forward pass code for the extra layer\n",
    "- Write new backward pass code (more $\\delta$ computations)\n",
    "- Update the weight-saving logic in early stopping\n",
    "- Update the gradient computation in FlexMLP\n",
    "- Update every optimizer's velocity/moment lists\n",
    "\n",
    "This doesn't scale. For a 50-layer ResNet, manual backprop would be **thousands of lines** of error-prone code.\n",
    "\n",
    "### The Solution: PyTorch\n",
    "\n",
    "PyTorch gives us:\n",
    "\n",
    "| Our manual code | PyTorch equivalent |\n",
    "|---|---|\n",
    "| `model.forward()` + stored `z`, `a` | `model(x)` â€” automatic |\n",
    "| `model.backward()` with chain rule | `loss.backward()` â€” **automatic for any graph** |\n",
    "| `SGD`, `Adam` classes | `torch.optim.SGD`, `torch.optim.Adam` |\n",
    "| `categorical_cross_entropy()` | `nn.CrossEntropyLoss()` |\n",
    "| `RegularizedMLP` with manual L2 | `weight_decay` parameter in optimizer |\n",
    "| Manual dropout mask + scaling | `nn.Dropout(p)` |\n",
    "| NumPy arrays on CPU only | Tensors on **CPU or GPU** |\n",
    "\n",
    "### ðŸ¤” Quick Questions (from Session 8's \"Think About\")\n",
    "\n",
    "**Q1:** What parts of our manual implementation were tedious and error-prone?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Backpropagation: computing $\\delta$ for each layer, getting the transposes right, remembering to store $z$ during forward pass, dividing by $N$, not regularizing biases. Gradient checking helped, but it was slow.\n",
    "</details>\n",
    "\n",
    "**Q2:** If a library handles gradients automatically, what do **you** still need to decide?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "The architecture (how many layers, how many neurons, which activations), the loss function, the optimizer and its hyperparameters, the training/validation split, when to stop, and how to evaluate the model. The **engineering decisions** remain yours â€” PyTorch automates the **calculus**.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Tensors: The Building Block {#tensors}\n",
    "\n",
    "### What is a Tensor?\n",
    "\n",
    "A tensor is PyTorch's version of a NumPy array â€” a multi-dimensional array of numbers. The key difference: tensors can **track gradients** and run on **GPUs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96ced2",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe307ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python lists\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"From list:   {a}, shape: {a.shape}, dtype: {a.dtype}\")\n",
    "\n",
    "# From NumPy (zero-copy when possible!)\n",
    "import numpy as np\n",
    "np_array = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "b = torch.from_numpy(np_array)\n",
    "print(f\"From numpy:  {b}, shape: {b.shape}\")\n",
    "\n",
    "# Common constructors (just like NumPy)\n",
    "zeros = torch.zeros(3, 4)       # 3Ã—4 of zeros\n",
    "ones = torch.ones(2, 3)         # 2Ã—3 of ones\n",
    "rand = torch.randn(2, 3)       # 2Ã—3 from N(0,1)\n",
    "eye = torch.eye(3)              # 3Ã—3 identity\n",
    "\n",
    "# Range\n",
    "r = torch.arange(0, 10, 2)      # [0, 2, 4, 6, 8]\n",
    "l = torch.linspace(0, 1, 5)     # [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "print(f\"\\nzeros: {zeros.shape}\")\n",
    "print(f\"randn: {rand}\")\n",
    "print(f\"arange: {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8addfe4c",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "Almost everything works like NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"x + y = {x + y}\")\n",
    "print(f\"x * y = {x * y}\")       # Element-wise (NOT matrix multiply)\n",
    "print(f\"x ** 2 = {x ** 2}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 2)\n",
    "C = A @ B                        # Matrix multiply (same as NumPy!)\n",
    "print(f\"A @ B shape: {C.shape}\")  # (3, 2)\n",
    "\n",
    "# Reductions\n",
    "print(f\"sum:  {x.sum()}\")\n",
    "print(f\"mean: {x.mean()}\")\n",
    "print(f\"max:  {x.max()}\")\n",
    "\n",
    "# Reshaping\n",
    "M = torch.arange(12).reshape(3, 4)\n",
    "print(f\"Reshaped:\\n{M}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6304917",
   "metadata": {},
   "source": [
    "### NumPy â†” PyTorch Cheat Sheet\n",
    "\n",
    "| NumPy | PyTorch | Notes |\n",
    "|---|---|---|\n",
    "| `np.array([1,2,3])` | `torch.tensor([1,2,3])` | |\n",
    "| `np.zeros((3,4))` | `torch.zeros(3, 4)` | No tuple needed |\n",
    "| `np.random.randn(3,4)` | `torch.randn(3, 4)` | |\n",
    "| `A @ B` or `np.dot(A,B)` | `A @ B` or `torch.matmul(A,B)` | Same `@` syntax! |\n",
    "| `A * B` | `A * B` | Element-wise |\n",
    "| `np.sum(A, axis=0)` | `A.sum(dim=0)` | `axis` â†’ `dim` |\n",
    "| `np.max(A, axis=1)` | `A.max(dim=1)` | Returns `(values, indices)` |\n",
    "| `A.reshape(3, -1)` | `A.reshape(3, -1)` or `A.view(3, -1)` | `view` is faster |\n",
    "| `np.concatenate` | `torch.cat` | |\n",
    "| `A.T` | `A.T` or `A.t()` | |\n",
    "\n",
    "### Key Difference: Data Types\n",
    "\n",
    "PyTorch defaults to `float32` (NumPy defaults to `float64`). This matters for GPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe351498",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])          # int64 by default\n",
    "y = torch.tensor([1.0, 2.0, 3.0])    # float32 by default\n",
    "\n",
    "print(f\"int tensor dtype: {x.dtype}\")   # torch.int64\n",
    "print(f\"float tensor dtype: {y.dtype}\") # torch.float32\n",
    "\n",
    "# Convert explicitly\n",
    "x_float = x.float()   # int â†’ float32\n",
    "y_double = y.double()  # float32 â†’ float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f5c2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Autograd: Automatic Differentiation {#autograd}\n",
    "\n",
    "### The Magic of `requires_grad`\n",
    "\n",
    "This is the feature that makes PyTorch revolutionary. Set `requires_grad=True` on a tensor, and PyTorch will **track every operation** to compute gradients automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor that tracks gradients\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "x = torch.tensor(3.0)\n",
    "\n",
    "# Forward pass: y = w * x + 1\n",
    "y = w * x + 1\n",
    "print(f\"y = {y}\")           # tensor(7., grad_fn=<AddBackward0>)\n",
    "\n",
    "# Notice \"grad_fn\" â€” PyTorch recorded the computation!\n",
    "\n",
    "# Backward pass: compute dy/dw\n",
    "y.backward()\n",
    "print(f\"dy/dw = {w.grad}\")  # tensor(3.) â€” which is x, as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d151a91",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### How It Works\n",
    "\n",
    "When `requires_grad=True`, PyTorch builds a **computational graph** â€” exactly like the ones we drew in Session 6:\n",
    "\n",
    "```\n",
    "w (requires_grad=True)\n",
    " â”‚\n",
    " â”œâ”€â”€ [*] â”€â”€ (w * x) â”€â”€ [+] â”€â”€ y\n",
    " â”‚                       â”‚\n",
    " x                       1\n",
    "\n",
    "backward(): dy/dw = x = 3.0 âœ“\n",
    "```\n",
    "\n",
    "### Autograd Replaces Our Manual Backprop\n",
    "\n",
    "**Session 6 (manual):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We had to compute every derivative by hand:\n",
    "delta2 = (self.a2 - y_true) / N\n",
    "dW2 = delta2 @ self.a1.T\n",
    "delta1 = (self.W2.T @ delta2) * (self.z1 > 0).astype(float)\n",
    "dW1 = delta1 @ self.X.T\n",
    "# ... and pray we didn't make a mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49298eeb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**PyTorch (automatic):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, target)\n",
    "loss.backward()   # ALL gradients computed automatically!\n",
    "# w.grad now contains âˆ‚loss/âˆ‚w for every parameter w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a453b183",
   "metadata": {},
   "source": [
    "### Example: Linear Regression in 5 Lines\n",
    "\n",
    "Remember the linear regression from Session 5? Here's the entire gradient computation in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35985ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_true = torch.tensor([2.2, 3.8, 6.1, 7.9, 10.1])  # y â‰ˆ 2x\n",
    "\n",
    "# Parameters (trainable)\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Forward + loss + backward\n",
    "y_pred = w * x + b\n",
    "loss = ((y_pred - y_true) ** 2).mean()   # MSE\n",
    "loss.backward()                           # Compute ALL gradients\n",
    "\n",
    "print(f\"dL/dw = {w.grad:.4f}\")\n",
    "print(f\"dL/db = {b.grad:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c272b2b",
   "metadata": {},
   "source": [
    "### Important: `zero_grad()` and `no_grad()`\n",
    "\n",
    "**Gradients accumulate** by default â€” you must zero them before each backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# First backward\n",
    "y1 = (w * 3) ** 2\n",
    "y1.backward()\n",
    "print(f\"After first backward:  w.grad = {w.grad}\")   # 18.0\n",
    "\n",
    "# Second backward WITHOUT zeroing â€” WRONG!\n",
    "y2 = (w * 3) ** 2\n",
    "y2.backward()\n",
    "print(f\"Without zero_grad:     w.grad = {w.grad}\")   # 36.0 (accumulated!)\n",
    "\n",
    "# Correct: zero before each backward\n",
    "w.grad.zero_()\n",
    "y3 = (w * 3) ** 2\n",
    "y3.backward()\n",
    "print(f\"After zero_grad:       w.grad = {w.grad}\")   # 18.0 âœ“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac513db",
   "metadata": {},
   "source": [
    "**Disable gradient tracking** for validation/inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # No graph built â†’ faster, uses less memory\n",
    "    val_output = model(val_data)\n",
    "    val_loss = criterion(val_output, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713d232",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About It\n",
    "\n",
    "**Q:** In our manual code, we had to **store** all intermediate values ($z^{(l)}$, $a^{(l)}$) during the forward pass for use in the backward pass. Does PyTorch need us to do this?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "**No!** When you compute operations on tensors with `requires_grad=True`, PyTorch automatically builds a graph that stores everything it needs. The `loss.backward()` call walks this graph in reverse. That's why it's called **automatic** differentiation â€” you just write the forward pass, and backward is free.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 4. nn.Module: Building Networks {#nn-module}\n",
    "\n",
    "### The nn.Module Pattern\n",
    "\n",
    "Every PyTorch network inherits from `nn.Module` and implements two things:\n",
    "1. `__init__`: define the layers\n",
    "2. `forward`: define the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa61ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNetwork(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super().__init__()   # Always call this first!\n",
    "        self.hidden = nn.Linear(n_input, n_hidden)   # Weights + bias, auto-initialized\n",
    "        self.output = nn.Linear(n_hidden, n_output)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))    # Hidden layer + ReLU\n",
    "        x = self.output(x)               # Output layer (raw logits)\n",
    "        return x\n",
    "\n",
    "# Create and inspect\n",
    "net = MyFirstNetwork(n_input=2, n_hidden=4, n_output=3)\n",
    "print(net)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c761bb",
   "metadata": {},
   "source": [
    "### What `nn.Linear` Does\n",
    "\n",
    "`nn.Linear(in_features, out_features)` is exactly our manual $z = Wx + b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5dbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(3, 2)  # 3 inputs â†’ 2 outputs\n",
    "\n",
    "# It has weights and bias\n",
    "print(f\"Weight shape: {layer.weight.shape}\")  # (2, 3)\n",
    "print(f\"Bias shape:   {layer.bias.shape}\")    # (2,)\n",
    "print(f\"Weight:\\n{layer.weight.data}\")\n",
    "print(f\"Bias: {layer.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57213b4",
   "metadata": {},
   "source": [
    "**Convention difference:** In PyTorch, input shape is `(N, features)` â€” batch first. In our NumPy code, we used `(features, N)`. PyTorch's convention is more common in practice.\n",
    "\n",
    "| | Our NumPy code | PyTorch |\n",
    "|---|---|---|\n",
    "| Input shape | `(n_features, N)` | `(N, n_features)` |\n",
    "| Weight shape | `(n_output, n_input)` | `(n_output, n_input)` |\n",
    "| Operation | `W @ X + b` | `F.linear(x, W, b)` or `layer(x)` |\n",
    "\n",
    "### nn.Sequential: Quick Network Building\n",
    "\n",
    "For simple networks, `nn.Sequential` avoids writing a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ec33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two are equivalent:\n",
    "\n",
    "# Method 1: nn.Sequential\n",
    "net_seq = nn.Sequential(\n",
    "    nn.Linear(2, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 3),\n",
    ")\n",
    "\n",
    "# Method 2: Custom class\n",
    "class NetCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(4, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "net_custom = NetCustom()\n",
    "\n",
    "# Both produce the same structure\n",
    "print(\"Sequential:\", net_seq)\n",
    "print(\"\\nCustom:\", net_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d844586",
   "metadata": {},
   "source": [
    "**When to use which:**\n",
    "\n",
    "| Use `nn.Sequential` | Use custom `nn.Module` |\n",
    "|---|---|\n",
    "| Layers are a simple chain | Skip connections, multiple inputs/outputs |\n",
    "| No conditional logic | Different behavior during train/eval |\n",
    "| Quick prototyping | Complex architectures (ResNet, U-Net) |\n",
    "\n",
    "### Common Layers and Activations\n",
    "\n",
    "| Layer | Our NumPy | PyTorch |\n",
    "|---|---|---|\n",
    "| Fully connected | `W @ x + b` | `nn.Linear(in, out)` |\n",
    "| ReLU | `np.maximum(0, z)` | `nn.ReLU()` |\n",
    "| Sigmoid | `1 / (1 + np.exp(-z))` | `nn.Sigmoid()` |\n",
    "| Dropout | Manual mask + scaling | `nn.Dropout(p)` |\n",
    "| Batch normalization | (not covered) | `nn.BatchNorm1d(features)` |\n",
    "| Softmax | Manual exp + normalize | `nn.Softmax(dim=1)` |\n",
    "\n",
    "**Important:** For classification, PyTorch's `nn.CrossEntropyLoss` applies softmax internally. You do **not** add a softmax layer at the end of your network â€” just output raw logits.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. The PyTorch Training Loop {#training-loop}\n",
    "\n",
    "### The Standard Pattern\n",
    "\n",
    "Every PyTorch training loop follows the same 5-step pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNetwork(...)\n",
    "criterion = nn.CrossEntropyLoss()        # Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # â‘  Forward pass\n",
    "    outputs = model(X_train)\n",
    "    \n",
    "    # â‘¡ Compute loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # â‘¢ Zero gradients (BEFORE backward!)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # â‘£ Backward pass (compute all gradients)\n",
    "    loss.backward()\n",
    "    \n",
    "    # â‘¤ Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa7034",
   "metadata": {},
   "source": [
    "**That's it.** Five lines inside the loop. Compare with our 20+ line manual loop!\n",
    "\n",
    "### Side-by-Side: Manual vs PyTorch\n",
    "\n",
    "| Step | Our Manual Code (Session 6â€“8) | PyTorch |\n",
    "|---|---|---|\n",
    "| Forward | `model.forward(X)` | `outputs = model(X)` |\n",
    "| Loss | `model.loss(y)` | `loss = criterion(outputs, y)` |\n",
    "| Zero grads | *(not needed â€” we computed fresh each time)* | `optimizer.zero_grad()` |\n",
    "| Backward | `model.backward(X, y, lr)` | `loss.backward()` |\n",
    "| Update | *(inside backward)* | `optimizer.step()` |\n",
    "| Regularization | Manual `+ lambda * W` in gradient | `weight_decay=0.01` in optimizer |\n",
    "| Dropout | Manual mask in `forward()` | `nn.Dropout(p)` â€” auto handles train/eval |\n",
    "\n",
    "### Train vs Eval Mode\n",
    "\n",
    "Dropout and batch normalization behave differently during training and inference. Toggle with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4374ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()   # Enable dropout, batch norm in training mode\n",
    "model.eval()    # Disable dropout, batch norm uses running statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42cfd2f",
   "metadata": {},
   "source": [
    "### Complete Training Loop with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val,\n",
    "                criterion, optimizer, n_epochs, patience=50):\n",
    "    \"\"\"\n",
    "    The complete PyTorch training loop with early stopping.\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state = model.state_dict().copy()   # Save best weights (PyTorch way!)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # â”€â”€ Train â”€â”€\n",
    "        model.train()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())  # .item() â†’ Python float\n",
    "        \n",
    "        # â”€â”€ Validate â”€â”€\n",
    "        model.eval()\n",
    "        with torch.no_grad():             # No gradient needed for validation!\n",
    "            val_out = model(X_val)\n",
    "            val_loss = criterion(val_out, y_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        # â”€â”€ Early stopping â”€â”€\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        if epoch - best_epoch >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (best: {best_epoch})\")\n",
    "            break\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch:5d}: Train {loss.item():.4f} | Val {val_loss.item():.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return train_losses, val_losses, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772674a9",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About It\n",
    "\n",
    "**Q:** Our manual early stopping required saving 4 arrays (W1, b1, W2, b2) with `.copy()`. For a 50-layer network, that would be 100+ arrays. How does PyTorch solve this?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "`model.state_dict()` returns a dictionary of **all** parameters, regardless of how many layers there are. And `model.load_state_dict(state)` restores them all in one call. This is why frameworks scale â€” the code doesn't change when the architecture changes.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "# Part II â€” Mini-Projects\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Mini-Project A: From Perceptron to MLP in PyTorch {#project-a}\n",
    "\n",
    "### ðŸŽ¯ Goal\n",
    "\n",
    "Rebuild the Perceptron (Session 2) and XOR MLP (Session 6) in PyTorch, seeing how autograd eliminates manual backprop.\n",
    "\n",
    "**Skills reused:** Perceptron logic (Session 2), XOR problem (Session 4), backprop training (Session 6).\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1 â€” Linear Regression with Autograd\n",
    "\n",
    "Before networks, let's see autograd in action on the simplest case: learning $y = 2x + 1$ with raw tensors.\n",
    "\n",
    "**Task:** Complete the training loop using only tensors and autograd â€” no `nn.Module` yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e29723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: y = 2x + 1 + noise\n",
    "torch.manual_seed(42)\n",
    "x = torch.linspace(0, 5, 50)\n",
    "y_true = 2 * x + 1 + torch.randn(50) * 0.5\n",
    "\n",
    "# Learnable parameters\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "lr = 0.01\n",
    "losses = []\n",
    "\n",
    "for epoch in range(500):\n",
    "    # TODO: Forward pass â€” compute y_pred = w * x + b\n",
    "    y_pred = ___\n",
    "    \n",
    "    # TODO: Compute MSE loss\n",
    "    loss = ___\n",
    "    \n",
    "    # TODO: Backward pass â€” compute gradients\n",
    "    ___\n",
    "    \n",
    "    # TODO: Update weights manually (inside torch.no_grad() block!)\n",
    "    # Why no_grad? Because we don't want the update to be tracked in the graph.\n",
    "    with torch.no_grad():\n",
    "        w -= ___\n",
    "        b -= ___\n",
    "    \n",
    "    # TODO: Zero the gradients for next iteration\n",
    "    w.grad.___\n",
    "    b.grad.___\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Learned: y = {w.item():.3f}x + {b.item():.3f}\")\n",
    "print(f\"True:    y = 2.000x + 1.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86524ba1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    y_pred = w * x + b\n",
    "    loss = ((y_pred - y_true) ** 2).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bd080",
   "metadata": {},
   "source": [
    "Note how we **never computed a derivative by hand** â€” `loss.backward()` did it all!\n",
    "</details>\n",
    "\n",
    "### Phase 2 â€” Perceptron as nn.Module\n",
    "\n",
    "**Task:** Implement a Perceptron classifier for the AND gate using `nn.Module`. Use `nn.Linear` and `nn.Sigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define a single linear layer: 2 inputs â†’ 1 output\n",
    "        self.linear = ___\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Linear â†’ Sigmoid\n",
    "        return ___\n",
    "\n",
    "# AND gate data (PyTorch convention: batch first â†’ shape (N, features))\n",
    "X_and = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_and = torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)\n",
    "\n",
    "# TODO: Create model, loss function (BCELoss for binary), and optimizer (SGD, lr=1.0)\n",
    "model = ___\n",
    "criterion = ___\n",
    "optimizer = ___\n",
    "\n",
    "# TODO: Train for 1000 epochs (5-line loop!)\n",
    "for epoch in range(1000):\n",
    "    ___\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    preds = model(X_and)\n",
    "    print(\"AND gate results:\")\n",
    "    for i in range(4):\n",
    "        x1, x2 = X_and[i]\n",
    "        p = preds[i].item()\n",
    "        print(f\"  ({x1:.0f}, {x2:.0f}) â†’ {p:.3f} â†’ {int(p > 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd6e05",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c33f8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "model = Perceptron()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.0)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    outputs = model(X_and)\n",
    "    loss = criterion(outputs, y_and)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a10c64",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 3 â€” XOR MLP: The Moment of Truth\n",
    "\n",
    "**Task:** Build and train an MLP to solve XOR. In Session 4, we did this manually. In Session 6, we implemented backprop. Now: just define the architecture and let PyTorch handle everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define a 2-layer MLP: 2 â†’ 4 â†’ 1\n",
    "        # Hidden layer with ReLU, output with Sigmoid\n",
    "        # Hint: Use nn.Sequential or define layers individually\n",
    "        ___\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ___\n",
    "\n",
    "# XOR data\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# TODO: Create model, BCELoss, Adam optimizer (lr=0.01)\n",
    "model = ___\n",
    "criterion = ___\n",
    "optimizer = ___\n",
    "\n",
    "# TODO: Train for 5000 epochs, print loss every 1000\n",
    "losses = []\n",
    "for epoch in range(5000):\n",
    "    ___\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_xor)\n",
    "    print(\"\\nXOR results:\")\n",
    "    for i in range(4):\n",
    "        x1, x2 = X_xor[i]\n",
    "        p = preds[i].item()\n",
    "        print(f\"  ({x1:.0f}, {x2:.0f}) â†’ {p:.3f} â†’ {int(p > 0.5)} \"\n",
    "              f\"(true: {int(y_xor[i].item())})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b8bfd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb173d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class XOR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = XOR_MLP()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(5000):\n",
    "    outputs = model(X_xor)\n",
    "    loss = criterion(outputs, y_xor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d3889",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 4 â€” Compare: Manual vs PyTorch\n",
    "\n",
    "**Task:** Plot the XOR loss curves from Session 6 (manual backprop) and this session (PyTorch) on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Session 6 manual version (simplified re-run)\n",
    "def sigmoid_np(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "class ManualMLP:\n",
    "    def __init__(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(4, 2) * np.sqrt(2.0 / 2)\n",
    "        self.b1 = np.zeros((4, 1))\n",
    "        self.W2 = np.random.randn(1, 4) * np.sqrt(2.0 / 4)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "    \n",
    "    def train_epoch(self, X, y, lr):\n",
    "        self.z1 = self.W1 @ X + self.b1\n",
    "        self.a1 = np.maximum(0, self.z1)\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = sigmoid_np(self.z2)\n",
    "        loss = np.mean((y - self.a2) ** 2)\n",
    "        \n",
    "        N = X.shape[1]\n",
    "        da2 = -2 * (y - self.a2) / N\n",
    "        dz2 = da2 * self.a2 * (1 - self.a2)\n",
    "        self.W2 -= lr * (dz2 @ self.a1.T)\n",
    "        self.b2 -= lr * np.sum(dz2, axis=1, keepdims=True)\n",
    "        dz1 = (self.W2.T @ dz2) * (self.z1 > 0).astype(float)\n",
    "        self.W1 -= lr * (dz1 @ X.T)\n",
    "        self.b1 -= lr * np.sum(dz1, axis=1, keepdims=True)\n",
    "        return loss\n",
    "\n",
    "X_np = np.array([[0, 0, 1, 1], [0, 1, 0, 1]], dtype=float)\n",
    "y_np = np.array([[0, 1, 1, 0]], dtype=float)\n",
    "\n",
    "manual_model = ManualMLP(seed=42)\n",
    "manual_losses = [manual_model.train_epoch(X_np, y_np, lr=2.0) for _ in range(5000)]\n",
    "\n",
    "# TODO: Plot both loss curves on the same axes\n",
    "# Blue line for manual, orange line for PyTorch\n",
    "# Add xlabel, ylabel, title, legend, grid, log scale on y-axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "___\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63e3e2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a03c2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(manual_losses, label='Manual (NumPy, lr=2.0)', linewidth=2, alpha=0.7)\n",
    "ax.plot(losses, label='PyTorch (Adam, lr=0.01)', linewidth=2, alpha=0.7)\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "ax.set_title('XOR Training: Manual vs PyTorch', fontsize=16)\n",
    "ax.set_yscale('log')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578ce7f",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Mini-Project B: The Full Pipeline â€” Spiral Classifier {#project-b}\n",
    "\n",
    "### ðŸŽ¯ Goal\n",
    "\n",
    "Rebuild the spiral classifier from Session 7â€“8 entirely in PyTorch: data preparation, model, training loop with early stopping, evaluation, and decision boundary visualization.\n",
    "\n",
    "**Skills reused:** Spiral dataset (Session 7), train/val/test split (Session 8), learning curves (Session 8), evaluation metrics (Session 7).\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1 â€” Prepare Data as Tensors\n",
    "\n",
    "**Task:** Generate the spiral dataset, split it, and convert everything to PyTorch tensors. Watch out for the shape convention change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a48381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data (NumPy)\n",
    "from session8_toolkit import generate_spiral, train_val_test_split  # or copy from Session 8\n",
    "\n",
    "np.random.seed(42)\n",
    "N_per_class = 150\n",
    "N_classes = 3\n",
    "N = N_per_class * N_classes\n",
    "X_np = np.zeros((2, N))\n",
    "y_np = np.zeros(N, dtype=int)\n",
    "for k in range(N_classes):\n",
    "    s, e = k * N_per_class, (k + 1) * N_per_class\n",
    "    r = np.linspace(0.2, 1.0, N_per_class)\n",
    "    theta = np.linspace(k * 4.0, (k + 1) * 4.0, N_per_class) + np.random.randn(N_per_class) * 0.25\n",
    "    X_np[0, s:e] = r * np.cos(theta)\n",
    "    X_np[1, s:e] = r * np.sin(theta)\n",
    "    y_np[s:e] = k\n",
    "\n",
    "# Shuffle\n",
    "idx = np.random.permutation(N)\n",
    "X_np, y_np = X_np[:, idx], y_np[idx]\n",
    "\n",
    "# Split (70/15/15)\n",
    "n_test = int(N * 0.15)\n",
    "n_val = int(N * 0.15)\n",
    "n_train = N - n_val - n_test\n",
    "\n",
    "X_train_np, y_train_np = X_np[:, :n_train], y_np[:n_train]\n",
    "X_val_np, y_val_np = X_np[:, n_train:n_train+n_val], y_np[n_train:n_train+n_val]\n",
    "X_test_np, y_test_np = X_np[:, n_train+n_val:], y_np[n_train+n_val:]\n",
    "\n",
    "# TODO: Convert to PyTorch tensors\n",
    "# IMPORTANT: Transpose X from (2, N) to (N, 2) â€” PyTorch uses batch-first!\n",
    "# IMPORTANT: y should be LongTensor for CrossEntropyLoss (class indices, not one-hot)\n",
    "X_train = torch.tensor(___, dtype=torch.float32)   # shape: (n_train, 2)\n",
    "y_train = torch.tensor(___, dtype=torch.long)       # shape: (n_train,)\n",
    "X_val = torch.tensor(___, dtype=torch.float32)\n",
    "y_val = torch.tensor(___, dtype=torch.long)\n",
    "X_test = torch.tensor(___, dtype=torch.float32)\n",
    "y_test = torch.tensor(___, dtype=torch.long)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "# Expected: X_train: torch.Size([315, 2]), y_train: torch.Size([315])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7993c0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train_np.T, dtype=torch.float32)  # (N, 2)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.long)        # (N,)\n",
    "X_val = torch.tensor(X_val_np.T, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val_np, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test_np.T, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9430d7",
   "metadata": {},
   "source": [
    "Key points:\n",
    "- `.T` transposes from `(features, N)` to `(N, features)` â€” PyTorch convention\n",
    "- `dtype=torch.long` for class labels â€” `CrossEntropyLoss` requires integer indices, not one-hot\n",
    "</details>\n",
    "\n",
    "### Phase 2 â€” Build the Model\n",
    "\n",
    "**Task:** Define a multi-class MLP with regularization. Use `nn.Sequential` with:\n",
    "- Linear(2 â†’ 100) â†’ ReLU â†’ Dropout(0.2) â†’ Linear(100 â†’ 3)\n",
    "\n",
    "**No softmax at the end** â€” `CrossEntropyLoss` includes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiralNet(nn.Module):\n",
    "    def __init__(self, n_hidden=100, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # TODO: Define the network using nn.Sequential\n",
    "        # Layers: Linear(2, n_hidden) â†’ ReLU â†’ Dropout(dropout) â†’ Linear(n_hidden, 3)\n",
    "        self.net = nn.Sequential(\n",
    "            ___\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SpiralNet(n_hidden=100, dropout=0.2)\n",
    "print(model)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "# Expected: 2*100 + 100 + 100*3 + 3 = 603 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207edbd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a2fb3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class SpiralNet(nn.Module):\n",
    "    def __init__(self, n_hidden=100, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hidden, 3),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a63f",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 3 â€” Train with Early Stopping\n",
    "\n",
    "**Task:** Write the full training loop. Use `nn.CrossEntropyLoss` and `Adam` with `weight_decay` for L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a299f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create model, loss, optimizer\n",
    "# Use weight_decay=0.005 for L2 regularization (replaces our manual lambda!)\n",
    "model = SpiralNet(n_hidden=100, dropout=0.2)\n",
    "criterion = ___\n",
    "optimizer = ___\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_state = None\n",
    "patience = 200\n",
    "n_epochs = 5000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # â”€â”€ Train â”€â”€\n",
    "    model.train()     # Enables dropout\n",
    "    \n",
    "    # TODO: The 5-step training pattern\n",
    "    outputs = ___\n",
    "    loss = ___\n",
    "    ___              # zero_grad\n",
    "    ___              # backward\n",
    "    ___              # step\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # â”€â”€ Validate â”€â”€\n",
    "    model.eval()      # Disables dropout\n",
    "    with torch.no_grad():\n",
    "        val_out = model(X_val)\n",
    "        val_loss = criterion(val_out, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    # â”€â”€ Early stopping â”€â”€\n",
    "    # TODO: Track best val loss and save best model state\n",
    "    if ___:\n",
    "        best_val_loss = val_loss.item()\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if epoch - best_epoch >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch} (best: {best_epoch})\")\n",
    "        break\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:5d}: Train {loss.item():.4f} | Val {val_loss.item():.4f}\")\n",
    "\n",
    "# TODO: Restore best weights\n",
    "model.load_state_dict(___)\n",
    "print(f\"\\nBest epoch: {best_epoch}, Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4133dbc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534bcc0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = SpiralNet(n_hidden=100, dropout=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.005)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_state = None\n",
    "patience = 200\n",
    "n_epochs = 5000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(X_val)\n",
    "        val_loss = criterion(val_out, y_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if epoch - best_epoch >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch} (best: {best_epoch})\")\n",
    "        break\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:5d}: Train {loss.item():.4f} | Val {val_loss.item():.4f}\")\n",
    "\n",
    "model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87802a4f",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 4 â€” Evaluate and Visualize\n",
    "\n",
    "**Task:** Compute test accuracy, plot learning curves, and draw the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d43f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute test accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test)\n",
    "    # Hint: torch.argmax(test_out, dim=1) gives predicted classes\n",
    "    test_preds = ___\n",
    "    test_acc = ___\n",
    "    print(f\"Test accuracy: {test_acc:.1f}%\")\n",
    "\n",
    "# TODO: Create a 1Ã—2 figure\n",
    "# Left: Learning curves with vertical line at best_epoch\n",
    "# Right: Decision boundary on test data\n",
    "#   - Create meshgrid, convert to tensor, forward through model\n",
    "#   - Use torch.argmax on model output to get class predictions\n",
    "#   - Convert back to numpy for matplotlib\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: learning curves\n",
    "ax = axes[0]\n",
    "___\n",
    "\n",
    "# Right: decision boundary\n",
    "ax = axes[1]\n",
    "# Hint for meshgrid â†’ tensor â†’ predict â†’ numpy:\n",
    "#   grid_tensor = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, dtype=torch.float32)\n",
    "#   with torch.no_grad(): Z = torch.argmax(model(grid_tensor), dim=1).numpy()\n",
    "___\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d79208",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7d9d5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test)\n",
    "    test_preds = torch.argmax(test_out, dim=1)\n",
    "    test_acc = (test_preds == y_test).float().mean().item() * 100\n",
    "    print(f\"Test accuracy: {test_acc:.1f}%\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Learning curves\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, label='Train', linewidth=1.5)\n",
    "ax.plot(val_losses, label='Val', linewidth=1.5)\n",
    "ax.axvline(x=best_epoch, color='red', linestyle='--', label=f'Best @ {best_epoch}')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "ax.set_title(f'Learning Curves (Test Acc: {test_acc:.1f}%)', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "ax = axes[1]\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_np[0].min()-0.3, X_np[0].max()+0.3, 200),\n",
    "    np.linspace(X_np[1].min()-0.3, X_np[1].max()+0.3, 200))\n",
    "grid_tensor = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Z = torch.argmax(model(grid_tensor), dim=1).numpy().reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, levels=[-0.5, 0.5, 1.5, 2.5],\n",
    "            colors=['#ADD8E6', '#FFCCCB', '#90EE90'], alpha=0.4)\n",
    "test_np = X_test.numpy()\n",
    "test_y_np = y_test.numpy()\n",
    "for k, c in enumerate(['blue', 'red', 'green']):\n",
    "    mask = test_y_np == k\n",
    "    ax.scatter(test_np[mask, 0], test_np[mask, 1], c=c, edgecolors='black', s=30, alpha=0.8)\n",
    "ax.set_xlabel('$x_1$', fontsize=14)\n",
    "ax.set_ylabel('$x_2$', fontsize=14)\n",
    "ax.set_title('Decision Boundary (Test Set)', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fdb01",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 5 â€” Line Count Comparison\n",
    "\n",
    "Count the lines you wrote for the PyTorch spiral classifier vs the Session 7â€“8 manual version:\n",
    "\n",
    "| Component | Manual (Sessions 7â€“8) | PyTorch (this session) |\n",
    "|---|---|---|\n",
    "| Model definition | ~40 lines (class with forward + backward) | ~10 lines |\n",
    "| Loss function | ~5 lines | 1 line (`nn.CrossEntropyLoss()`) |\n",
    "| Optimizer | ~20 lines (Adam class) | 1 line (`optim.Adam(...)`) |\n",
    "| Training loop | ~15 lines | ~10 lines |\n",
    "| Dropout | ~5 lines in forward + training flag | 1 line (`nn.Dropout(p)`) |\n",
    "| L2 regularization | 3 lines in backward | 0 lines (`weight_decay=`) |\n",
    "| **Total** | **~88 lines** | **~23 lines** |\n",
    "\n",
    "And the PyTorch version supports **any number of layers** with zero additional code.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Mini-Project C: MNIST â€” Your First Real Dataset {#project-c}\n",
    "\n",
    "### ðŸŽ¯ Goal\n",
    "\n",
    "Train an MLP on handwritten digit recognition (MNIST) â€” 10 classes, 28Ã—28 pixel images, 60,000 training samples. This is the standard \"Hello World\" of deep learning.\n",
    "\n",
    "**Skills reused:** Multi-class classification (Session 7), full training pipeline (Project B), evaluation metrics (Session 7).\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1 â€” Load MNIST\n",
    "\n",
    "PyTorch provides MNIST through `torchvision`. Each image is 28Ã—28 grayscale, which we **flatten** to a vector of 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91af1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Download and load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),            # Convert to tensor, scale to [0, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten 28Ã—28 â†’ 784\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True,  download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples:     {len(test_dataset)}\")\n",
    "print(f\"Image shape (flattened): {train_dataset[0][0].shape}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b84679",
   "metadata": {},
   "source": [
    "### Working with a Subset\n",
    "\n",
    "For speed (and to see overfitting effects), we'll use a **subset** of 5,000 training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for manageable training times\n",
    "n_train = 5000\n",
    "n_val = 1000\n",
    "\n",
    "# Load into tensors\n",
    "all_train_X = torch.stack([train_dataset[i][0] for i in range(n_train + n_val)])\n",
    "all_train_y = torch.tensor([train_dataset[i][1] for i in range(n_train + n_val)])\n",
    "\n",
    "X_train_mnist = all_train_X[:n_train]\n",
    "y_train_mnist = all_train_y[:n_train]\n",
    "X_val_mnist = all_train_X[n_train:]\n",
    "y_val_mnist = all_train_y[n_train:]\n",
    "\n",
    "# Full test set\n",
    "X_test_mnist = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "y_test_mnist = torch.tensor([test_dataset[i][1] for i in range(len(test_dataset))])\n",
    "\n",
    "print(f\"Train: {X_train_mnist.shape}\")   # (5000, 784)\n",
    "print(f\"Val:   {X_val_mnist.shape}\")     # (1000, 784)\n",
    "print(f\"Test:  {X_test_mnist.shape}\")    # (10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5f133",
   "metadata": {},
   "source": [
    "### Visualize Some Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img = X_train_mnist[i].reshape(28, 28)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(str(y_train_mnist[i].item()), fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5a7d5",
   "metadata": {},
   "source": [
    "### Phase 2 â€” Build the MNIST Classifier\n",
    "\n",
    "**Task:** Define an MLP for 10-class digit recognition. Architecture:\n",
    "- 784 â†’ 256 (ReLU, Dropout 0.2) â†’ 128 (ReLU, Dropout 0.2) â†’ 10\n",
    "\n",
    "This is your **first 3-layer network** â€” easy in PyTorch, would have been painful manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define a 3-layer MLP\n",
    "        # 784 â†’ 256 â†’ ReLU â†’ Dropout(0.2) â†’ 128 â†’ ReLU â†’ Dropout(0.2) â†’ 10\n",
    "        self.net = nn.Sequential(\n",
    "            ___\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MNISTNet()\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "# Expected: 784*256 + 256 + 256*128 + 128 + 128*10 + 10 = 234,506"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9424cc47",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcb5c9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a190e40",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### ðŸ¤” Think About It\n",
    "\n",
    "**Q:** Why is building a 3-layer network trivial in PyTorch but painful in our manual code?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "In our manual MLP, the backward pass was hardcoded for exactly 2 layers â€” adding a third would require writing new delta propagation code. In PyTorch, `loss.backward()` walks the computational graph **regardless of depth**. The forward pass defines the graph, the backward pass is automatic.\n",
    "</details>\n",
    "\n",
    "### Phase 3 â€” Train\n",
    "\n",
    "**Task:** Train the model with early stopping. This is the same loop as Project B â€” practice makes permanent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff165ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "best_state = None\n",
    "patience = 30\n",
    "n_epochs = 200\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # â”€â”€ Train â”€â”€\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Forward, loss, zero_grad, backward, step\n",
    "    ___\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # TODO: Compute training accuracy (with no_grad)\n",
    "    with torch.no_grad():\n",
    "        train_preds = ___\n",
    "        train_acc = ___\n",
    "        train_accs.append(train_acc)\n",
    "    \n",
    "    # â”€â”€ Validate â”€â”€\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(X_val_mnist)\n",
    "        val_loss = criterion(val_out, y_val_mnist)\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        val_preds = torch.argmax(val_out, dim=1)\n",
    "        val_acc = (val_preds == y_val_mnist).float().mean().item() * 100\n",
    "        val_accs.append(val_acc)\n",
    "    \n",
    "    # â”€â”€ Early stopping â”€â”€\n",
    "    if val_loss.item() < best_val_loss:\n",
    "        best_val_loss = val_loss.item()\n",
    "        best_epoch = epoch\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if epoch - best_epoch >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch} (best: {best_epoch})\")\n",
    "        break\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Train {loss.item():.4f} ({train_acc:.1f}%) | \"\n",
    "              f\"Val {val_loss.item():.4f} ({val_acc:.1f}%)\")\n",
    "\n",
    "model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4c4e1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” training step</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375250b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    # Train\n",
    "    model.train()\n",
    "    outputs = model(X_train_mnist)\n",
    "    loss = criterion(outputs, y_train_mnist)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_preds = torch.argmax(outputs, dim=1)\n",
    "        train_acc = (train_preds == y_train_mnist).float().mean().item() * 100\n",
    "        train_accs.append(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0a23b6",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 4 â€” Evaluate on Full Test Set\n",
    "\n",
    "**Task:** Compute test accuracy and build a confusion matrix on the 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out = model(X_test_mnist)\n",
    "    test_preds = torch.argmax(test_out, dim=1)\n",
    "    test_acc = (test_preds == y_test_mnist).float().mean().item() * 100\n",
    "    print(f\"Test accuracy on 10,000 images: {test_acc:.1f}%\")\n",
    "\n",
    "# TODO: Build and display confusion matrix (10Ã—10)\n",
    "# Reuse the confusion_matrix function from Session 7, or write it with PyTorch\n",
    "cm = np.zeros((10, 10), dtype=int)\n",
    "for true, pred in zip(y_test_mnist.numpy(), test_preds.numpy()):\n",
    "    cm[true, pred] += 1\n",
    "\n",
    "# TODO: Plot the confusion matrix as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "___\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print per-digit accuracy\n",
    "print(\"\\nPer-digit accuracy:\")\n",
    "for d in range(10):\n",
    "    digit_acc = cm[d, d] / cm[d].sum() * 100\n",
    "    print(f\"  Digit {d}: {digit_acc:.1f}% ({cm[d, d]}/{cm[d].sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28313c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” confusion matrix plot</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a9fc0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                color=color, fontsize=9)\n",
    "\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xlabel('Predicted', fontsize=14)\n",
    "ax.set_ylabel('True', fontsize=14)\n",
    "ax.set_title(f'MNIST Confusion Matrix (Test Acc: {test_acc:.1f}%)', fontsize=16)\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0aa5fa",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 5 â€” Visualize Predictions\n",
    "\n",
    "**Task:** Show some correctly classified and misclassified digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d148c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correct and incorrect predictions\n",
    "correct_mask = (test_preds == y_test_mnist).numpy()\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "correct_idx = np.where(correct_mask)[0][:10]\n",
    "incorrect_idx = np.where(incorrect_mask)[0][:10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 4))\n",
    "\n",
    "# Row 1: Correct predictions\n",
    "for i, idx in enumerate(correct_idx):\n",
    "    ax = axes[0, i]\n",
    "    img = X_test_mnist[idx].reshape(28, 28)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{test_preds[idx].item()}', color='green', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "axes[0, 0].set_ylabel('Correct', fontsize=12)\n",
    "\n",
    "# Row 2: Incorrect predictions\n",
    "for i, idx in enumerate(incorrect_idx):\n",
    "    ax = axes[1, i]\n",
    "    img = X_test_mnist[idx].reshape(28, 28)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{test_preds[idx].item()} (true: {y_test_mnist[idx].item()})',\n",
    "                 color='red', fontsize=10, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "axes[1, 0].set_ylabel('Wrong', fontsize=12)\n",
    "\n",
    "plt.suptitle('MNIST Predictions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09dc207",
   "metadata": {},
   "source": [
    "### Phase 6 â€” Architecture Experiment\n",
    "\n",
    "**Task:** Compare three architectures on MNIST. Train each, record validation accuracy. Which is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = {\n",
    "    \"Small (784â†’32â†’10)\": nn.Sequential(\n",
    "        nn.Linear(784, 32), nn.ReLU(), nn.Linear(32, 10)\n",
    "    ),\n",
    "    \"Medium (784â†’128â†’10)\": nn.Sequential(\n",
    "        nn.Linear(784, 128), nn.ReLU(), nn.Dropout(0.2), nn.Linear(128, 10)\n",
    "    ),\n",
    "    \"Deep (784â†’256â†’128â†’10)\": nn.Sequential(\n",
    "        nn.Linear(784, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "        nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.2),\n",
    "        nn.Linear(128, 10)\n",
    "    ),\n",
    "}\n",
    "\n",
    "# TODO: For each architecture:\n",
    "# 1. Create model, criterion, optimizer (Adam, lr=0.001, weight_decay=1e-4)\n",
    "# 2. Train for 100 epochs (no early stopping â€” keep it simple here)\n",
    "# 3. Record val accuracy history\n",
    "# 4. Print final test accuracy\n",
    "\n",
    "arch_results = {}\n",
    "\n",
    "for name, net_seq in architectures.items():\n",
    "    # TODO: Wrap in nn.Module or use directly, train, evaluate\n",
    "    model = type('Net', (nn.Module,), {\n",
    "        '__init__': lambda self, n=net_seq: (super(type(self), self).__init__(), setattr(self, 'net', n)),\n",
    "        'forward': lambda self, x: self.net(x)\n",
    "    })()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    val_hist = []\n",
    "    for epoch in range(100):\n",
    "        ___  # train + record val accuracy\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = torch.argmax(model(X_test_mnist), dim=1)\n",
    "        test_acc = (test_preds == y_test_mnist).float().mean().item() * 100\n",
    "    \n",
    "    arch_results[name] = {\"val_hist\": val_hist, \"test_acc\": test_acc}\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:>30s}: Test {test_acc:.1f}% | Params: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e7882",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” training loop per architecture</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a2121",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "    val_hist = []\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        out = model(X_train_mnist)\n",
    "        loss = criterion(out, y_train_mnist)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = torch.argmax(model(X_val_mnist), dim=1)\n",
    "            val_acc = (val_preds == y_val_mnist).float().mean().item() * 100\n",
    "            val_hist.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba3171",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Task:** Plot validation accuracy curves for all three architectures on the same axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3576f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# TODO: Plot val accuracy curves and add a legend with test accuracy\n",
    "for name, res in arch_results.items():\n",
    "    ax.plot(res[\"val_hist\"], label=f'{name} (Test: {res[\"test_acc\"]:.1f}%)', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Validation Accuracy (%)', fontsize=14)\n",
    "ax.set_title('Architecture Comparison on MNIST', fontsize=16)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac530c04",
   "metadata": {},
   "source": [
    "### Phase 7 â€” Reflection\n",
    "\n",
    "Answer in your notebook:\n",
    "\n",
    "1. How many lines of code did it take to go from our 2-layer NumPy MLP to a 3-layer PyTorch model on 784-dimensional data?\n",
    "2. Which MNIST digits are most often confused? (Look at the off-diagonal elements of the confusion matrix.)\n",
    "3. The deep model has 234K parameters but only 5K training samples. Is it overfitting? How can you tell?\n",
    "4. What accuracy would you expect from a random classifier on 10 classes? How much better is our model?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "âœ… **Tensors**: PyTorch arrays â€” like NumPy but with gradient tracking and GPU support  \n",
    "âœ… **Autograd**: `loss.backward()` computes all gradients automatically  \n",
    "âœ… **nn.Module**: Define networks with layers and a forward method  \n",
    "âœ… **nn.Sequential**: Quick network building for simple architectures  \n",
    "âœ… **Training loop**: forward â†’ loss â†’ zero_grad â†’ backward â†’ step  \n",
    "âœ… **train()/eval()**: Toggle dropout and batch norm behavior  \n",
    "âœ… **state_dict()**: Save and restore model weights\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **PyTorch automates the calculus, not the engineering:**\n",
    "   - Gradients are free â†’ focus on architecture and hyperparameters\n",
    "   - The 5-step training loop is always the same\n",
    "   - But choosing the right architecture, loss, optimizer, and regularization is still up to you\n",
    "\n",
    "2. **The transition from NumPy to PyTorch is small:**\n",
    "   - Nearly identical array syntax (`@`, `+`, `.reshape()`)\n",
    "   - Main differences: `axis` â†’ `dim`, `float64` â†’ `float32`, batch-first convention\n",
    "   - Your understanding of backprop, loss functions, and regularization carries over completely\n",
    "\n",
    "3. **Framework advantages compound with complexity:**\n",
    "   - 2-layer MLP: PyTorch saves some effort\n",
    "   - 3-layer MLP: PyTorch saves a lot of effort\n",
    "   - 50-layer ResNet: PyTorch makes it possible at all\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Session 10: The Convolution Operation**\n",
    "\n",
    "In the next session, we'll learn:\n",
    "- **Why MLPs fail for images**: The curse of dimensionality (MNIST's 784 inputs are tiny â€” real images have millions of pixels!)\n",
    "- **Convolutions**: Sliding filters that detect local patterns\n",
    "- **Feature detectors**: How kernels find edges, textures, and shapes\n",
    "- **Parameter efficiency**: A 3Ã—3 kernel has 9 parameters regardless of image size\n",
    "\n",
    "**The goal:** Understand the building block of Convolutional Neural Networks (CNNs)!\n",
    "\n",
    "### Before Next Session\n",
    "\n",
    "**Think about:**\n",
    "1. Our MNIST MLP flattens the 28Ã—28 image into a 784-dimensional vector. What spatial information is lost?\n",
    "2. If we wanted to classify 224Ã—224 RGB images, the input would have $224 \\times 224 \\times 3 = 150{,}528$ features. How many parameters would the first hidden layer need?\n",
    "3. When you recognize a digit, do you look at every pixel equally, or do you focus on **local patterns** (curves, lines, intersections)?\n",
    "\n",
    "**Optional reading:**\n",
    "- PyTorch official tutorials: https://pytorch.org/tutorials/\n",
    "- Stanford CS231n: \"Convolutional Neural Networks for Visual Recognition\"\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 9** ðŸŽ“\n",
    "\n",
    "**You now understand:**\n",
    "- âœ… How to build and train neural networks with PyTorch\n",
    "- âœ… How autograd replaces manual backpropagation\n",
    "- âœ… How to tackle real datasets like MNIST\n",
    "\n",
    "**Next up:** Convolutions â€” learning to see! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
