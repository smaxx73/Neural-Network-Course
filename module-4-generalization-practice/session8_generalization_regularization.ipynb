{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed127320",
   "metadata": {},
   "source": [
    "# Session 8: Generalization & Regularization\n",
    "## When Good Training Goes Bad\n",
    "\n",
    "**Course: Neural Networks for Engineers**  \n",
    "**Duration: 2 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Part I â€” Concepts (â‰ˆ 45 min)\n",
    "1. [Recap: What We Know So Far](#recap)\n",
    "2. [The Generalization Problem](#generalization)\n",
    "3. [Train / Validation / Test Splits](#splits)\n",
    "4. [Overfitting & Underfitting](#overfitting)\n",
    "5. [Regularization Techniques](#regularization)\n",
    "6. [Modern Optimizers](#optimizers)\n",
    "\n",
    "### Part II â€” Mini-Projects (â‰ˆ 75 min)\n",
    "7. [Mini-Project A: The Overfitting Lab](#project-a)\n",
    "8. [Mini-Project B: Regularization Showdown](#project-b)\n",
    "9. [Mini-Project C: Optimizer Olympics](#project-c)\n",
    "\n",
    "---\n",
    "\n",
    "# Part I â€” Concepts\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap: What We Know So Far {#recap}\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "âœ… **Perceptron & MLP**: From single neurons to multi-layer networks (Sessions 2â€“4)  \n",
    "âœ… **Gradient descent & backpropagation**: Automatic weight learning (Sessions 5â€“6)  \n",
    "âœ… **Classification**: Sigmoid/softmax, cross-entropy, evaluation metrics (Session 7)  \n",
    "âœ… **Open question**: Our spiral classifier hits 95% on training data â€” but will it work on **new** data?\n",
    "\n",
    "### ðŸ¤” Quick Questions (from Session 7's \"Think About\")\n",
    "\n",
    "**Q1:** Does 95% training accuracy mean the model will work well on new spirals?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "**Not necessarily.** The model may have **memorized** the training data instead of learning the underlying pattern. We need to test on data the model has never seen â€” this is the **generalization** problem.\n",
    "</details>\n",
    "\n",
    "**Q2:** What if we increased the hidden layer to 500 neurons?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "More neurons = more capacity to memorize. With 500 hidden neurons on a 300-sample spiral dataset, the model could fit every single point perfectly â€” including noise! It would likely **overfit**: perfect on training data, poor on new data.\n",
    "</details>\n",
    "\n",
    "**Q3:** How would you know if your model is too simple vs too complex?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "Compare **training loss** vs **validation loss**. If training loss is low but validation loss is high, the model is too complex (overfitting). If both are high, the model is too simple (underfitting). This is what **learning curves** show us.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Generalization Problem {#generalization}\n",
    "\n",
    "### The Central Question of Machine Learning\n",
    "\n",
    "We don't care about performance on data we've already seen. We care about **unseen data**.\n",
    "\n",
    "**Generalization** = the ability of a model to perform well on new, previously unseen data.\n",
    "\n",
    "### A Cautionary Tale\n",
    "\n",
    "Imagine fitting a polynomial to noisy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50892b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True function: y = sin(x)\n",
    "x_true = np.linspace(0, 2 * np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "\n",
    "# Training data (15 noisy samples)\n",
    "x_train = np.sort(np.random.uniform(0, 2 * np.pi, 15))\n",
    "y_train = np.sin(x_train) + np.random.randn(15) * 0.3\n",
    "\n",
    "# Fit polynomials of increasing degree\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "degrees = [2, 5, 14]\n",
    "titles = ['Degree 2 (Underfitting)', 'Degree 5 (Good fit)', 'Degree 14 (Overfitting)']\n",
    "\n",
    "for ax, deg, title in zip(axes, degrees, titles):\n",
    "    coeffs = np.polyfit(x_train, y_train, deg)\n",
    "    y_fit = np.polyval(coeffs, x_true)\n",
    "    \n",
    "    ax.plot(x_true, y_true, 'g--', linewidth=2, label='True function', alpha=0.7)\n",
    "    ax.scatter(x_train, y_train, c='blue', s=60, zorder=5, \n",
    "               edgecolors='black', label='Training data')\n",
    "    ax.plot(x_true, y_fit, 'r-', linewidth=2, label=f'Polynomial (deg {deg})')\n",
    "    \n",
    "    y_pred_train = np.polyval(coeffs, x_train)\n",
    "    train_err = np.mean((y_train - y_pred_train) ** 2)\n",
    "    test_err = np.mean((y_true - y_fit) ** 2)\n",
    "    \n",
    "    ax.set_title(f'{title}\\nTrain MSE: {train_err:.3f} | Test MSE: {test_err:.3f}', fontsize=13)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64493b",
   "metadata": {},
   "source": [
    "**Observation:** Degree 14 has the **lowest** training error but the **highest** test error. It memorized the noise!\n",
    "\n",
    "### The Bias-Variance Tradeoff (Intuitive)\n",
    "\n",
    "| | Too Simple (Underfitting) | Just Right | Too Complex (Overfitting) |\n",
    "|---|---|---|---|\n",
    "| **Training error** | High | Low | Very low |\n",
    "| **Test error** | High | Low | **High** |\n",
    "| **Problem** | Can't capture the pattern | â€” | Memorizes noise |\n",
    "| **Bias** | High (wrong assumptions) | Low | Low |\n",
    "| **Variance** | Low (stable predictions) | Low | High (sensitive to training data) |\n",
    "\n",
    "The goal is to find the sweet spot: complex enough to capture the pattern, simple enough to ignore the noise.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Train / Validation / Test Splits {#splits}\n",
    "\n",
    "### Why Three Sets?\n",
    "\n",
    "| Set | Purpose | When used | Typical size |\n",
    "|---|---|---|---|\n",
    "| **Training** | Learn weights | Every epoch | 60â€“80% |\n",
    "| **Validation** | Tune hyperparameters, detect overfitting | During training | 10â€“20% |\n",
    "| **Test** | Final, unbiased evaluation | Once, at the very end | 10â€“20% |\n",
    "\n",
    "### The Golden Rule\n",
    "\n",
    "> **Never** use test data for any decision during training or model selection. It must remain untouched until the final evaluation.\n",
    "\n",
    "If you peek at test data to tune your model, the test score becomes biased â€” it no longer reflects true generalization.\n",
    "\n",
    "### Learning Curves: The Diagnostic Tool\n",
    "\n",
    "Plot **training loss** and **validation loss** over epochs:\n",
    "\n",
    "```\n",
    "Loss                                Loss                                Loss\n",
    " â”‚ â•²                                â”‚ â•²                                â”‚ â•²  train\n",
    " â”‚  â•²                               â”‚  â•²  train                       â”‚   â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    " â”‚   â•²â”€â”€â”€â”€â”€ val                     â”‚   â•²â”€â”€â”€â”€â”€â”€                       â”‚          â•±â”€â”€â”€â”€ val\n",
    " â”‚    â•²â”€â”€â”€â”€ train                   â”‚    â•²â”€â”€â”€â”€â”€ val                   â”‚        â•±\n",
    " â”‚                                  â”‚                                 â”‚      â•±\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epoch                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epoch               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epoch\n",
    "   UNDERFITTING                       GOOD FIT                          OVERFITTING\n",
    "   Both losses high                   Both losses low                   Gap between curves\n",
    "   Model too simple                   and close together                Model too complex\n",
    "```\n",
    "\n",
    "**Key diagnostic:** the **gap** between training and validation loss tells you about overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Overfitting & Underfitting {#overfitting}\n",
    "\n",
    "### Causes and Cures\n",
    "\n",
    "| Problem | Cause | Symptoms | Solutions |\n",
    "|---|---|---|---|\n",
    "| **Underfitting** | Model too simple | Both train & val loss high | More neurons/layers, train longer, reduce regularization |\n",
    "| **Overfitting** | Model too complex or not enough data | Train loss â‰ª val loss | Regularization, more data, simpler model, early stopping |\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "The simplest regularization technique: **stop training when validation loss starts increasing**.\n",
    "\n",
    "The idea: save the best weights (lowest validation loss), and restore them when patience runs out.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Regularization Techniques {#regularization}\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Regularization = adding **constraints** or **penalties** to prevent the model from becoming too complex.\n",
    "\n",
    "Think of it as telling the model: \"Don't just fit the data â€” keep things **simple**.\"\n",
    "\n",
    "### L2 Regularization (Weight Decay)\n",
    "\n",
    "Add a penalty for large weights to the loss:\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{data}} + \\frac{\\lambda}{2} \\sum_l \\| W^{(l)} \\|^2_F\n",
    "$$\n",
    "\n",
    "Where $\\lambda$ controls the regularization strength and $\\| W \\|^2_F = \\sum_{ij} w_{ij}^2$ is the Frobenius norm.\n",
    "\n",
    "**Effect on gradient:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{\\text{total}}}{\\partial W^{(l)}} = \\frac{\\partial L_{\\text{data}}}{\\partial W^{(l)}} + \\lambda W^{(l)}\n",
    "$$\n",
    "\n",
    "The update becomes:\n",
    "\n",
    "$$\n",
    "W^{(l)} \\leftarrow (1 - \\eta\\lambda) W^{(l)} - \\eta \\frac{\\partial L_{\\text{data}}}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "That factor $(1 - \\eta\\lambda)$ **shrinks** the weights at every step â€” hence the name **weight decay**.\n",
    "\n",
    "**Intuition:** Large weights create sharp, complex decision boundaries. Penalizing large weights encourages smoother, simpler boundaries that generalize better.\n",
    "\n",
    "### L1 Regularization (Sparsity)\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{data}} + \\lambda \\sum_l \\| W^{(l)} \\|_1\n",
    "$$\n",
    "\n",
    "**Difference from L2:** L1 drives weights to **exactly zero**, creating sparse networks. L2 shrinks weights toward zero but rarely makes them exactly zero.\n",
    "\n",
    "| | L1 | L2 |\n",
    "|---|---|---|\n",
    "| **Penalty** | Sum of $|w|$ | Sum of $w^2$ |\n",
    "| **Effect** | Sparse weights (feature selection) | Small weights (smooth boundaries) |\n",
    "| **Gradient** | $\\lambda \\cdot \\text{sign}(w)$ | $\\lambda \\cdot w$ |\n",
    "\n",
    "### Dropout (Intuitive)\n",
    "\n",
    "During training, **randomly set** a fraction $p$ of hidden neurons to zero at each forward pass.\n",
    "\n",
    "```\n",
    "Without dropout:          With dropout (p=0.5):\n",
    "\n",
    "  xâ‚ â”€â”€ hâ‚ â”€â”€ hâ‚„ â”€â”€       xâ‚ â”€â”€ hâ‚ â”€â”€ â•³  â”€â”€\n",
    "       â•²â•±    â•²â•±               â•²â•±    â•²â•±\n",
    "       â•±â•²    â•±â•²               â•±â•²    â•±â•²\n",
    "  xâ‚‚ â”€â”€ hâ‚‚ â”€â”€ hâ‚… â”€â”€ Å·     xâ‚‚ â”€â”€ â•³  â”€â”€ hâ‚… â”€â”€ Å·\n",
    "       â•²â•±    â•²â•±               â•²â•±    â•²â•±\n",
    "       â•±â•²    â•±â•²               â•±â•²    â•±â•²\n",
    "  xâ‚ƒ â”€â”€ hâ‚ƒ â”€â”€ hâ‚† â”€â”€       xâ‚ƒ â”€â”€ hâ‚ƒ â”€â”€ hâ‚† â”€â”€\n",
    "\n",
    "  All neurons active        hâ‚‚ and hâ‚„ \"dropped\"\n",
    "```\n",
    "\n",
    "**Why it works:**\n",
    "- Forces the network to not rely on any single neuron\n",
    "- Like training an **ensemble** of smaller networks\n",
    "- Uses **inverted dropout**: scale surviving neurons by $\\frac{1}{1-p}$ during training so nothing changes at test time\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Modern Optimizers {#optimizers}\n",
    "\n",
    "### SGD with Momentum\n",
    "\n",
    "**Idea:** Accumulate a \"velocity\" â€” like a ball rolling downhill with inertia.\n",
    "\n",
    "$$\n",
    "v \\leftarrow \\beta v + (1 - \\beta) \\nabla L\n",
    "$$\n",
    "$$\n",
    "w \\leftarrow w - \\eta v\n",
    "$$\n",
    "\n",
    "Where $\\beta \\approx 0.9$ is the momentum coefficient.\n",
    "\n",
    "**Effect:** Smooths out oscillations and accelerates through consistent gradient directions.\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**Idea:** Adapt the learning rate **per weight**, using both first moment (mean) and second moment (variance) of gradients.\n",
    "\n",
    "$$\n",
    "m \\leftarrow \\beta_1 m + (1 - \\beta_1) \\nabla L \\qquad \\text{(mean of gradients)}\n",
    "$$\n",
    "$$\n",
    "v \\leftarrow \\beta_2 v + (1 - \\beta_2) (\\nabla L)^2 \\qquad \\text{(variance of gradients)}\n",
    "$$\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}\n",
    "$$\n",
    "\n",
    "Where $\\hat{m}$ and $\\hat{v}$ are bias-corrected: $\\hat{m} = \\frac{m}{1 - \\beta_1^t}$, $\\hat{v} = \\frac{v}{1 - \\beta_2^t}$.\n",
    "\n",
    "**Default hyperparameters:** $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$.\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "| Situation | Recommended optimizer |\n",
    "|---|---|\n",
    "| First try / don't know | **Adam** (lr=0.001) |\n",
    "| Want best generalization | **SGD + Momentum** (lr=0.01, requires tuning) |\n",
    "| Small dataset | **Adam** (converges faster) |\n",
    "| Large dataset + long training | **SGD + Momentum** (often generalizes better) |\n",
    "\n",
    "---\n",
    "\n",
    "# Part II â€” Mini-Projects\n",
    "\n",
    "### Shared Toolkit\n",
    "\n",
    "All mini-projects reuse code from Sessions 5â€“7. **Copy this at the top of your notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0971197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# â”€â”€â”€ Utilities from previous sessions â”€â”€â”€\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def softmax(z):\n",
    "    z_shifted = z - np.max(z, axis=0, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_hat):\n",
    "    N = y_true.shape[1]\n",
    "    y_hat_clipped = np.clip(y_hat, 1e-7, 1 - 1e-7)\n",
    "    return -np.sum(y_true * np.log(y_hat_clipped)) / N\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, n_classes):\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "def classification_report(y_true, y_pred, n_classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, n_classes)\n",
    "    print(f\"{'Class':>8} {'Prec':>8} {'Recall':>8} {'F1':>8} {'Support':>8}\")\n",
    "    print(\"-\" * 44)\n",
    "    for k in range(n_classes):\n",
    "        tp = cm[k, k]\n",
    "        fp = cm[:, k].sum() - tp\n",
    "        fn = cm[k, :].sum() - tp\n",
    "        support = cm[k, :].sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        print(f\"{'Class '+str(k):>8} {prec:8.3f} {rec:8.3f} {f1:8.3f} {support:8d}\")\n",
    "\n",
    "# â”€â”€â”€ Datasets â”€â”€â”€\n",
    "\n",
    "def generate_moons(n_samples=500, noise=0.2, seed=42):\n",
    "    \"\"\"Two interleaving half-circles.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = n_samples // 2\n",
    "    theta1 = np.linspace(0, np.pi, n)\n",
    "    theta2 = np.linspace(0, np.pi, n)\n",
    "    X1 = np.vstack([np.cos(theta1), np.sin(theta1)])\n",
    "    X2 = np.vstack([np.cos(theta2) + 0.5, -np.sin(theta2) + 0.5])\n",
    "    X = np.hstack([X1, X2]) + np.random.randn(2, n_samples) * noise\n",
    "    y_labels = np.hstack([np.zeros(n, dtype=int), np.ones(n, dtype=int)])\n",
    "    y_oh = np.zeros((2, n_samples))\n",
    "    y_oh[y_labels, np.arange(n_samples)] = 1\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    return X[:, idx], y_oh[:, idx], y_labels[idx]\n",
    "\n",
    "def generate_spiral(n_per_class=150, n_classes=3, noise=0.25, seed=42):\n",
    "    \"\"\"Spiral dataset from Session 7.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = n_per_class * n_classes\n",
    "    X = np.zeros((2, N))\n",
    "    y = np.zeros(N, dtype=int)\n",
    "    for k in range(n_classes):\n",
    "        s, e = k * n_per_class, (k + 1) * n_per_class\n",
    "        r = np.linspace(0.2, 1.0, n_per_class)\n",
    "        theta = np.linspace(k * 4.0, (k + 1) * 4.0, n_per_class) + np.random.randn(n_per_class) * noise\n",
    "        X[0, s:e] = r * np.cos(theta)\n",
    "        X[1, s:e] = r * np.sin(theta)\n",
    "        y[s:e] = k\n",
    "    y_oh = np.zeros((n_classes, N))\n",
    "    y_oh[y, np.arange(N)] = 1\n",
    "    idx = np.random.permutation(N)\n",
    "    return X[:, idx], y_oh[:, idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2463d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Mini-Project A: The Overfitting Lab {#project-a}\n",
    "\n",
    "### ðŸŽ¯ Goal\n",
    "\n",
    "Build a complete train/val/test pipeline **from scratch**, deliberately overfit a model, diagnose it, then cure it.\n",
    "\n",
    "**Skills reused:** MLP forward/backward (Session 6), CCE loss + softmax (Session 7), evaluation metrics (Session 7).  \n",
    "**New skills:** Data splitting, learning curves, early stopping.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1 â€” Implement Train/Val/Test Split\n",
    "\n",
    "**Task:** Write a function that shuffles data and splits it into three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Split data into train, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array, shape (n_features, N)\n",
    "    y : array, shape (n_classes, N)\n",
    "    val_ratio, test_ratio : float\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    N = X.shape[1]\n",
    "    \n",
    "    # TODO: Generate shuffled indices\n",
    "    indices = ___\n",
    "    \n",
    "    # TODO: Compute split boundaries\n",
    "    n_test = ___\n",
    "    n_val = ___\n",
    "    n_train = ___\n",
    "    \n",
    "    # TODO: Slice indices into three groups\n",
    "    train_idx = ___\n",
    "    val_idx = ___\n",
    "    test_idx = ___\n",
    "    \n",
    "    return (X[:, train_idx], y[:, train_idx],\n",
    "            X[:, val_idx], y[:, val_idx],\n",
    "            X[:, test_idx], y[:, test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527d4fe",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24894ad8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    N = X.shape[1]\n",
    "    indices = np.random.permutation(N)\n",
    "    n_test = int(N * test_ratio)\n",
    "    n_val = int(N * val_ratio)\n",
    "    n_train = N - n_val - n_test\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:n_train + n_val]\n",
    "    test_idx = indices[n_train + n_val:]\n",
    "    return (X[:, train_idx], y[:, train_idx],\n",
    "            X[:, val_idx], y[:, val_idx],\n",
    "            X[:, test_idx], y[:, test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7f237",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Verify your split:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90743607",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all_oh, y_all_lbl = generate_moons(n_samples=400, noise=0.2)\n",
    "X_tr, y_tr, X_va, y_va, X_te, y_te = train_val_test_split(X_all, y_all_oh)\n",
    "\n",
    "print(f\"Train: {X_tr.shape[1]}, Val: {X_va.shape[1]}, Test: {X_te.shape[1]}\")\n",
    "# Expected: Train: 280, Val: 60, Test: 60\n",
    "\n",
    "# Also keep label versions for accuracy computation\n",
    "y_tr_l = np.argmax(y_tr, axis=0)\n",
    "y_va_l = np.argmax(y_va, axis=0)\n",
    "y_te_l = np.argmax(y_te, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0670589",
   "metadata": {},
   "source": [
    "### Phase 2 â€” Write the Training Loop with Val Tracking\n",
    "\n",
    "Here is our base MLP (same as Session 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Multi-class MLP from Session 7.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_classes, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(n_hidden, n_input) * np.sqrt(2.0 / n_input)\n",
    "        self.b1 = np.zeros((n_hidden, 1))\n",
    "        self.W2 = np.random.randn(n_classes, n_hidden) * np.sqrt(2.0 / n_hidden)\n",
    "        self.b2 = np.zeros((n_classes, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = self.W1 @ X + self.b1\n",
    "        self.a1 = np.maximum(0, self.z1)  # ReLU\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def loss(self, y_true):\n",
    "        return categorical_cross_entropy(y_true, self.a2)\n",
    "    \n",
    "    def backward(self, X, y_true, lr):\n",
    "        N = X.shape[1]\n",
    "        delta2 = (self.a2 - y_true) / N\n",
    "        dW2 = delta2 @ self.a1.T\n",
    "        db2 = np.sum(delta2, axis=1, keepdims=True)\n",
    "        delta1 = (self.W2.T @ delta2) * (self.z1 > 0).astype(float)\n",
    "        dW1 = delta1 @ X.T\n",
    "        db1 = np.sum(delta1, axis=1, keepdims=True)\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.a2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f0c11",
   "metadata": {},
   "source": [
    "**Task:** Write the function that trains the model and records train + val loss at each epoch. The key constraint: **the validation set must never influence the weights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_tracking(model, X_tr, y_tr, X_va, y_va, lr, n_epochs):\n",
    "    \"\"\"\n",
    "    Train the model and record train + val loss at each epoch.\n",
    "    \n",
    "    IMPORTANT: Validation data must NOT influence weights!\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_losses, val_losses : lists of float\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # TODO (3 steps):\n",
    "        # 1. Forward pass on TRAINING data\n",
    "        # 2. Record training loss\n",
    "        # 3. Backward pass (updates weights) on TRAINING data\n",
    "        ___\n",
    "        ___\n",
    "        ___\n",
    "        \n",
    "        # TODO (2 steps):\n",
    "        # 4. Forward pass on VALIDATION data (no backward!)\n",
    "        # 5. Record validation loss\n",
    "        ___\n",
    "        ___\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f874e2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_tracking(model, X_tr, y_tr, X_va, y_va, lr, n_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train step\n",
    "        model.forward(X_tr)\n",
    "        train_losses.append(model.loss(y_tr))\n",
    "        model.backward(X_tr, y_tr, lr)\n",
    "        \n",
    "        # Val step (forward ONLY â€” no backward!)\n",
    "        model.forward(X_va)\n",
    "        val_losses.append(model.loss(y_va))\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29fc82",
   "metadata": {},
   "source": [
    "Note: we record training loss **before** the backward pass so it reflects the same weights used for the validation loss.\n",
    "</details>\n",
    "\n",
    "### Phase 3 â€” Overfit on Purpose and Diagnose\n",
    "\n",
    "**Task:** Train a **deliberately overpowered model** (200 hidden neurons for a 2D, 280-sample dataset). Then write the plotting code to produce a 1Ã—2 figure: learning curves on the left, decision boundary on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the overpowered model\n",
    "model_big = MLP(n_input=2, n_hidden=200, n_classes=2, seed=42)\n",
    "t_losses, v_losses = train_with_tracking(model_big, X_tr, y_tr, X_va, y_va,\n",
    "                                          lr=1.0, n_epochs=3000)\n",
    "\n",
    "# Print final metrics\n",
    "train_acc = np.mean(model_big.predict(X_tr) == y_tr_l) * 100\n",
    "val_acc = np.mean(model_big.predict(X_va) == y_va_l) * 100\n",
    "test_acc = np.mean(model_big.predict(X_te) == y_te_l) * 100\n",
    "print(f\"Train: {train_acc:.1f}% | Val: {val_acc:.1f}% | Test: {test_acc:.1f}%\")\n",
    "\n",
    "# TODO: Create a figure with 2 subplots side by side (figsize 16x6)\n",
    "#\n",
    "# Left plot â€” Learning curves:\n",
    "#   - Plot t_losses as 'Train loss' (blue, linewidth=2)\n",
    "#   - Plot v_losses as 'Val loss' (orange, linewidth=2)\n",
    "#   - Add xlabel ('Epoch'), ylabel ('CCE Loss'), title, legend, grid\n",
    "#\n",
    "# Right plot â€” Decision boundary:\n",
    "#   - Create a meshgrid covering the data range with Â±0.5 margin\n",
    "#   - Forward the grid through model_big.predict()\n",
    "#   - Use contourf to show predicted class regions\n",
    "#   - Scatter training points (circles) and val points (squares)\n",
    "#     colored by true label with cmap='bwr'\n",
    "#\n",
    "# This is YOUR first diagnostic figure â€” make it readable!\n",
    "\n",
    "fig, axes = plt.subplots(___, ___, figsize=(___))\n",
    "\n",
    "# Left: learning curves\n",
    "ax = axes[0]\n",
    "___\n",
    "\n",
    "# Right: decision boundary\n",
    "ax = axes[1]\n",
    "___\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07109669",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b387d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: learning curves\n",
    "ax = axes[0]\n",
    "ax.plot(t_losses, label='Train loss', linewidth=2, color='blue')\n",
    "ax.plot(v_losses, label='Val loss', linewidth=2, color='orange')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('CCE Loss', fontsize=14)\n",
    "ax.set_title('Learning Curves â€” 200 Hidden Neurons', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: decision boundary\n",
    "ax = axes[1]\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_all[0].min()-0.5, X_all[0].max()+0.5, 200),\n",
    "    np.linspace(X_all[1].min()-0.5, X_all[1].max()+0.5, 200))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()])\n",
    "Z = model_big.predict(grid).reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, levels=[-0.5, 0.5, 1.5], colors=['#ADD8E6', '#FFCCCB'], alpha=0.4)\n",
    "ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "ax.scatter(X_tr[0], X_tr[1], c=y_tr_l, cmap='bwr', edgecolors='black', s=40, alpha=0.7, label='Train')\n",
    "ax.scatter(X_va[0], X_va[1], c=y_va_l, cmap='bwr', edgecolors='black', s=40, marker='s', alpha=0.7, label='Val')\n",
    "ax.set_title('Decision Boundary', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd7f92",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Diagnostic questions** (write answers in your notebook):\n",
    "\n",
    "1. At roughly which epoch does the validation loss start diverging from the training loss?\n",
    "2. Is the decision boundary smooth or jagged? What does that tell you?\n",
    "3. What is the gap between train and test accuracy?\n",
    "\n",
    "### Phase 4 â€” Implement Early Stopping\n",
    "\n",
    "**Task:** Write an early stopping training loop. You must track the best validation loss, save/restore weights, and stop when patience runs out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, X_tr, y_tr, X_va, y_va,\n",
    "                               lr, max_epochs, patience=50):\n",
    "    \"\"\"\n",
    "    Train with early stopping: stop when val loss hasn't improved\n",
    "    for `patience` epochs. Restore the best weights at the end.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_losses, val_losses : lists\n",
    "    best_epoch : int\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # TODO: Save initial copies of all 4 model weight arrays.\n",
    "    # Hint: use .copy() â€” without it you'd save references, not snapshots!\n",
    "    best_W1 = ___\n",
    "    best_b1 = ___\n",
    "    best_W2 = ___\n",
    "    best_b2 = ___\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # â”€â”€ Train step â”€â”€\n",
    "        model.forward(X_tr)\n",
    "        train_losses.append(model.loss(y_tr))\n",
    "        model.backward(X_tr, y_tr, lr)\n",
    "        \n",
    "        # â”€â”€ Val step â”€â”€\n",
    "        model.forward(X_va)\n",
    "        val_loss = model.loss(y_va)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # TODO: If this is a new best val loss â†’\n",
    "        #       update best_val_loss, best_epoch, and save weight copies\n",
    "        if ___:\n",
    "            ___\n",
    "        \n",
    "        # TODO: If patience is exceeded â†’ print a message and break\n",
    "        if ___:\n",
    "            print(f\"Early stopping at epoch {epoch} \"\n",
    "                  f\"(best was epoch {best_epoch}, val loss {best_val_loss:.4f})\")\n",
    "            break\n",
    "    \n",
    "    # TODO: Restore the best weights into the model\n",
    "    ___\n",
    "    \n",
    "    return train_losses, val_losses, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97622341",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9b6eb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, X_tr, y_tr, X_va, y_va,\n",
    "                               lr, max_epochs, patience=50):\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    best_W1 = model.W1.copy()\n",
    "    best_b1 = model.b1.copy()\n",
    "    best_W2 = model.W2.copy()\n",
    "    best_b2 = model.b2.copy()\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.forward(X_tr)\n",
    "        train_losses.append(model.loss(y_tr))\n",
    "        model.backward(X_tr, y_tr, lr)\n",
    "        \n",
    "        model.forward(X_va)\n",
    "        val_loss = model.loss(y_va)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_W1 = model.W1.copy()\n",
    "            best_b1 = model.b1.copy()\n",
    "            best_W2 = model.W2.copy()\n",
    "            best_b2 = model.b2.copy()\n",
    "        \n",
    "        if epoch - best_epoch >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} \"\n",
    "                  f\"(best was epoch {best_epoch}, val loss {best_val_loss:.4f})\")\n",
    "            break\n",
    "    \n",
    "    model.W1 = best_W1\n",
    "    model.b1 = best_b1\n",
    "    model.W2 = best_W2\n",
    "    model.b2 = best_b2\n",
    "    \n",
    "    return train_losses, val_losses, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9e69e",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Test it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aca2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es = MLP(n_input=2, n_hidden=200, n_classes=2, seed=42)\n",
    "t_es, v_es, best_ep = train_with_early_stopping(\n",
    "    model_es, X_tr, y_tr, X_va, y_va,\n",
    "    lr=1.0, max_epochs=3000, patience=100\n",
    ")\n",
    "\n",
    "test_acc_es = np.mean(model_es.predict(X_te) == y_te_l) * 100\n",
    "print(f\"Test accuracy (no early stop):   {test_acc:.1f}%\")\n",
    "print(f\"Test accuracy (early stopping):  {test_acc_es:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48023e9f",
   "metadata": {},
   "source": [
    "### Phase 5 â€” Three-Way Comparison\n",
    "\n",
    "**Task:** Train a **right-sized model** (10 hidden neurons, 3000 epochs, no early stopping) and produce a 1Ã—3 figure comparing the learning curves of all three approaches. Add a vertical red dashed line at `best_ep` on the early-stopping panel. Print test accuracy for each in the panel title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the small model\n",
    "model_small = MLP(n_input=2, n_hidden=___, n_classes=2, seed=42)\n",
    "t_small, v_small = train_with_tracking(model_small, X_tr, y_tr, X_va, y_va,\n",
    "                                        lr=1.0, n_epochs=___)\n",
    "test_acc_small = np.mean(model_small.predict(X_te) == y_te_l) * 100\n",
    "\n",
    "# TODO: Create 1Ã—3 figure\n",
    "# Panel 1: \"200 neurons, no reg\"     â€” t_losses vs v_losses\n",
    "# Panel 2: \"200 neurons, early stop\" â€” t_es vs v_es, vertical line at best_ep\n",
    "# Panel 3: \"10 neurons\"              â€” t_small vs v_small\n",
    "# Title each panel with its test accuracy.\n",
    "fig, axes = plt.subplots(___, ___, figsize=(___))\n",
    "___\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896fa16",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e60bfb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model_small = MLP(n_input=2, n_hidden=10, n_classes=2, seed=42)\n",
    "t_small, v_small = train_with_tracking(model_small, X_tr, y_tr, X_va, y_va,\n",
    "                                        lr=1.0, n_epochs=3000)\n",
    "test_acc_small = np.mean(model_small.predict(X_te) == y_te_l) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "data = [\n",
    "    (t_losses, v_losses, f'200 neurons, no reg (Test: {test_acc:.1f}%)'),\n",
    "    (t_es, v_es, f'200 neurons, early stop (Test: {test_acc_es:.1f}%)'),\n",
    "    (t_small, v_small, f'10 neurons (Test: {test_acc_small:.1f}%)'),\n",
    "]\n",
    "\n",
    "for ax, (tl, vl, title) in zip(axes, data):\n",
    "    ax.plot(tl, label='Train', linewidth=2)\n",
    "    ax.plot(vl, label='Val', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].axvline(x=best_ep, color='red', linestyle='--', linewidth=2, label=f'Stop @ {best_ep}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Model Comparison', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcece5",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Write your conclusions:**\n",
    "- Which model generalizes best?\n",
    "- Is a big model with early stopping better or worse than a right-sized model?\n",
    "- When would you prefer one approach over the other?\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Mini-Project B: Regularization Showdown {#project-b}\n",
    "\n",
    "### ðŸŽ¯ Goal\n",
    "\n",
    "**Implement** L2 regularization and dropout inside an MLP, then run a controlled experiment to determine which technique works best.\n",
    "\n",
    "**Skills reused:** MLP backpropagation (Session 6), training pipeline (Project A).  \n",
    "**New skills:** L2 gradient penalty, inverted dropout, controlled experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1 â€” Implement L2 Regularization\n",
    "\n",
    "**Task:** Extend the MLP with an L2 penalty. You need to change **3 things** compared to the base MLP: the `loss` method, and two lines in `backward` (one for each weight matrix).\n",
    "\n",
    "Start from this skeleton â€” the `___` blanks are yours to fill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedMLP:\n",
    "    \"\"\"MLP with L2 regularization and dropout.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_classes,\n",
    "                 l2_lambda=0.0, dropout_rate=0.0, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(n_hidden, n_input) * np.sqrt(2.0 / n_input)\n",
    "        self.b1 = np.zeros((n_hidden, 1))\n",
    "        self.W2 = np.random.randn(n_classes, n_hidden) * np.sqrt(2.0 / n_hidden)\n",
    "        self.b2 = np.zeros((n_classes, 1))\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        self.X = X\n",
    "        self.z1 = self.W1 @ X + self.b1\n",
    "        self.a1 = np.maximum(0, self.z1)\n",
    "        \n",
    "        # Phase 2 will add dropout here\n",
    "        \n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def loss(self, y_true):\n",
    "        data_loss = categorical_cross_entropy(y_true, self.a2)\n",
    "        # TODO: Compute L2 penalty = (lambda / 2) * (||W1||Â² + ||W2||Â²)\n",
    "        # Remember: ||W||Â² = sum of all squared elements\n",
    "        l2_penalty = ___\n",
    "        return data_loss + l2_penalty\n",
    "    \n",
    "    def data_loss(self, y_true):\n",
    "        \"\"\"Loss WITHOUT regularization penalty â€” for fair comparison.\"\"\"\n",
    "        return categorical_cross_entropy(y_true, self.a2)\n",
    "    \n",
    "    def backward(self, X, y_true, lr):\n",
    "        N = X.shape[1]\n",
    "        delta2 = (self.a2 - y_true) / N\n",
    "        \n",
    "        # TODO: Weight gradient for W2 = (data gradient) + (L2 gradient)\n",
    "        # Recall from Part I: âˆ‚L_total/âˆ‚W = âˆ‚L_data/âˆ‚W + lambda * W\n",
    "        dW2 = delta2 @ self.a1.T + ___\n",
    "        db2 = np.sum(delta2, axis=1, keepdims=True)    # Biases: no L2\n",
    "        \n",
    "        delta1 = (self.W2.T @ delta2) * (self.z1 > 0).astype(float)\n",
    "        \n",
    "        # TODO: Same for W1\n",
    "        dW1 = delta1 @ X.T + ___\n",
    "        db1 = np.sum(delta1, axis=1, keepdims=True)    # Biases: no L2\n",
    "        \n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.forward(X, training=False)\n",
    "        return np.argmax(self.a2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313ecd1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” the 3 changed lines</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee009084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In loss():\n",
    "l2_penalty = (self.l2_lambda / 2) * (np.sum(self.W1 ** 2) + np.sum(self.W2 ** 2))\n",
    "\n",
    "# In backward():\n",
    "dW2 = delta2 @ self.a1.T + self.l2_lambda * self.W2\n",
    "dW1 = delta1 @ X.T + self.l2_lambda * self.W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466a943",
   "metadata": {},
   "source": [
    "We do **not** regularize biases â€” only weights.\n",
    "</details>\n",
    "\n",
    "**Verify:** With `l2_lambda=0.0`, the model should behave identically to the base MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: L2=0 should match base MLP\n",
    "np.random.seed(42)\n",
    "model_base = MLP(2, 10, 2, seed=42)\n",
    "model_reg = RegularizedMLP(2, 10, 2, l2_lambda=0.0, seed=42)\n",
    "\n",
    "X_test_input = np.random.randn(2, 5)\n",
    "y_test_input = np.zeros((2, 5)); y_test_input[0] = 1\n",
    "\n",
    "model_base.forward(X_test_input)\n",
    "model_reg.forward(X_test_input)\n",
    "\n",
    "print(f\"Outputs match: {np.allclose(model_base.a2, model_reg.a2)}\")  # Should be True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d841d74",
   "metadata": {},
   "source": [
    "### Phase 2 â€” Implement Dropout\n",
    "\n",
    "**Task:** Add inverted dropout to the `forward` method. You need to:\n",
    "1. Generate a random binary mask during training (keep each neuron with probability $1-p$)\n",
    "2. Zero out dropped neurons and scale survivors by $\\frac{1}{1-p}$\n",
    "3. Do **nothing** during inference (`training=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"Phase 2 will add dropout here\" comment in forward() with:\n",
    "\n",
    "    # Dropout (inverted)\n",
    "    if training and self.dropout_rate > 0:\n",
    "        # TODO: Create binary mask â€” each entry is 1 with prob (1 - dropout_rate), 0 otherwise\n",
    "        # Hint: np.random.rand(*shape) gives uniform [0, 1); compare with dropout_rate\n",
    "        self.mask = ___\n",
    "        \n",
    "        # TODO: Apply mask and scale to maintain expected value\n",
    "        self.a1 = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47d5a4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb71a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if training and self.dropout_rate > 0:\n",
    "        self.mask = (np.random.rand(*self.a1.shape) > self.dropout_rate).astype(float)\n",
    "        self.a1 = self.a1 * self.mask / (1 - self.dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa022c40",
   "metadata": {},
   "source": [
    "**Why `> dropout_rate`?** If `dropout_rate = 0.3`, we want to **keep** 70% of neurons, so we keep entries where `rand > 0.3`.\n",
    "\n",
    "**Why divide by `(1 - dropout_rate)`?** Zeroing out 30% of neurons reduces the total signal by 30%. Scaling the survivors by $1/0.7$ compensates, so the expected activation stays the same. At test time we use all neurons â€” no scaling needed.\n",
    "</details>\n",
    "\n",
    "**Verify your dropout:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(99)\n",
    "test_model = RegularizedMLP(2, 20, 2, dropout_rate=0.5)\n",
    "test_X = np.random.randn(2, 1)\n",
    "\n",
    "# With dropout\n",
    "test_model.forward(test_X, training=True)\n",
    "n_zero = np.sum(test_model.a1 == 0)\n",
    "n_total = test_model.a1.size\n",
    "print(f\"Zeroed: {n_zero}/{n_total} ({n_zero/n_total*100:.0f}%) â€” expect ~50%\")\n",
    "\n",
    "# Without dropout (all neurons active)\n",
    "test_model.forward(test_X, training=False)\n",
    "n_zero_inf = np.sum(test_model.a1 == 0)\n",
    "print(f\"Zeroed at inference: {n_zero_inf}/{n_total} â€” expect only ReLU zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6930024",
   "metadata": {},
   "source": [
    "### Phase 3 â€” Write the Experiment Loop\n",
    "\n",
    "**Task:** Train 4 configurations on the spiral dataset and collect results. You must handle the subtlety that **training uses dropout** but **loss recording must not** (or the curves will be noisy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all_oh, y_all_lbl = generate_spiral(n_per_class=100, noise=0.3)\n",
    "X_tr, y_tr, X_va, y_va, X_te, y_te = train_val_test_split(X_all, y_all_oh)\n",
    "y_tr_l, y_va_l, y_te_l = np.argmax(y_tr, 0), np.argmax(y_va, 0), np.argmax(y_te, 0)\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"No reg\",          \"l2\": 0.0,   \"drop\": 0.0},\n",
    "    {\"name\": \"L2 (Î»=0.01)\",     \"l2\": 0.01,  \"drop\": 0.0},\n",
    "    {\"name\": \"Dropout (p=0.3)\", \"l2\": 0.0,   \"drop\": 0.3},\n",
    "    {\"name\": \"L2 + Dropout\",    \"l2\": 0.005, \"drop\": 0.2},\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for cfg in configs:\n",
    "    # TODO: Create model, train for 4000 epochs (lr=0.8)\n",
    "    # CAREFUL: \n",
    "    #   - Call forward(X_tr, training=True) + backward() for the training step\n",
    "    #   - Call forward(X_tr, training=False) + data_loss() for recording train loss\n",
    "    #   - Call forward(X_va, training=False) + data_loss() for recording val loss\n",
    "    # Store: train_losses, val_losses, test_acc, model\n",
    "    \n",
    "    model = RegularizedMLP(n_input=2, n_hidden=100, n_classes=3,\n",
    "                           l2_lambda=cfg[\"l2\"], dropout_rate=cfg[\"drop\"], seed=42)\n",
    "    t_hist, v_hist = [], []\n",
    "    \n",
    "    for epoch in range(4000):\n",
    "        ___  # training step\n",
    "        \n",
    "        ___  # record clean losses\n",
    "    \n",
    "    test_acc = np.mean(model.predict(X_te) == y_te_l) * 100\n",
    "    results[cfg[\"name\"]] = {\n",
    "        \"train_loss\": t_hist, \"val_loss\": v_hist,\n",
    "        \"test_acc\": test_acc, \"model\": model\n",
    "    }\n",
    "    print(f\"{cfg['name']:>18s}: Test Acc = {test_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884682c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a55691",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for cfg in configs:\n",
    "    model = RegularizedMLP(n_input=2, n_hidden=100, n_classes=3,\n",
    "                           l2_lambda=cfg[\"l2\"], dropout_rate=cfg[\"drop\"], seed=42)\n",
    "    t_hist, v_hist = [], []\n",
    "    \n",
    "    for epoch in range(4000):\n",
    "        # Train with dropout active\n",
    "        model.forward(X_tr, training=True)\n",
    "        model.backward(X_tr, y_tr, lr=0.8)\n",
    "        \n",
    "        # Record clean losses (no dropout noise)\n",
    "        model.forward(X_tr, training=False)\n",
    "        t_hist.append(model.data_loss(y_tr))\n",
    "        model.forward(X_va, training=False)\n",
    "        v_hist.append(model.data_loss(y_va))\n",
    "    \n",
    "    test_acc = np.mean(model.predict(X_te) == y_te_l) * 100\n",
    "    results[cfg[\"name\"]] = {\n",
    "        \"train_loss\": t_hist, \"val_loss\": v_hist,\n",
    "        \"test_acc\": test_acc, \"model\": model\n",
    "    }\n",
    "    print(f\"{cfg['name']:>18s}: Test Acc = {test_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b56a2b",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 4 â€” Build the Comparison Dashboard\n",
    "\n",
    "**Task:** Create a **3Ã—4 figure** (3 rows, 4 columns â€” one column per config):\n",
    "- **Row 1:** Learning curves (train + val loss)\n",
    "- **Row 2:** Decision boundary on test data (contourf + scatter)\n",
    "- **Row 3:** Histogram of all weights (W1 and W2 concatenated, 50 bins, xlim Â±3)\n",
    "\n",
    "Title each column with config name + test accuracy. Title each row 3 panel with the weight standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(22, 14))\n",
    "\n",
    "# Pre-compute the decision boundary grid (shared across all panels)\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_all[0].min()-0.3, X_all[0].max()+0.3, 200),\n",
    "    np.linspace(X_all[1].min()-0.3, X_all[1].max()+0.3, 200))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "for col, (name, res) in enumerate(results.items()):\n",
    "    # TODO â€” Row 1: Learning curves\n",
    "    ax = axes[0, col]\n",
    "    ___\n",
    "    \n",
    "    # TODO â€” Row 2: Decision boundary with test points\n",
    "    ax = axes[1, col]\n",
    "    ___\n",
    "    \n",
    "    # TODO â€” Row 3: Weight histogram\n",
    "    ax = axes[2, col]\n",
    "    ___\n",
    "\n",
    "plt.suptitle('Regularization Showdown â€” Spiral (100 hidden neurons)', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be78ab",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e9777",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(22, 14))\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X_all[0].min()-0.3, X_all[0].max()+0.3, 200),\n",
    "    np.linspace(X_all[1].min()-0.3, X_all[1].max()+0.3, 200))\n",
    "grid = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "for col, (name, res) in enumerate(results.items()):\n",
    "    # Row 1: Learning curves\n",
    "    ax = axes[0, col]\n",
    "    ax.plot(res[\"train_loss\"], label='Train', linewidth=1.5)\n",
    "    ax.plot(res[\"val_loss\"], label='Val', linewidth=1.5)\n",
    "    ax.set_title(f'{name}\\nTest: {res[\"test_acc\"]:.1f}%', fontsize=13)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('CCE Loss')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 2: Decision boundary\n",
    "    ax = axes[1, col]\n",
    "    Z = res[\"model\"].predict(grid).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, levels=[-0.5, 0.5, 1.5, 2.5],\n",
    "                colors=['#ADD8E6', '#FFCCCB', '#90EE90'], alpha=0.4)\n",
    "    for k, c in enumerate(['blue', 'red', 'green']):\n",
    "        mask = y_te_l == k\n",
    "        ax.scatter(X_te[0, mask], X_te[1, mask], c=c, edgecolors='black', s=25, alpha=0.8)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    \n",
    "    # Row 3: Weight histogram\n",
    "    ax = axes[2, col]\n",
    "    all_w = np.concatenate([res[\"model\"].W1.flatten(), res[\"model\"].W2.flatten()])\n",
    "    ax.hist(all_w, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'Weight std = {np.std(all_w):.3f}', fontsize=12)\n",
    "    ax.set_xlabel('Weight value')\n",
    "    ax.set_xlim(-3, 3)\n",
    "\n",
    "plt.suptitle('Regularization Showdown â€” Spiral (100 hidden neurons)', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3cc86",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 5 â€” Analysis Report\n",
    "\n",
    "Answer in your notebook (3â€“5 sentences each):\n",
    "\n",
    "1. Which configuration has the **smallest train-val gap**? What does this mean?\n",
    "2. Compare the decision boundaries: which are **smooth** vs **jagged**? How does this relate to generalization?\n",
    "3. How did L2 change the weight distribution compared to \"no reg\"? What about dropout?\n",
    "4. Which configuration achieved the **best test accuracy**?\n",
    "5. **Bonus experiment:** Change L2 to `l2_lambda=0.1` (10Ã— larger). Re-run, observe, and explain what happens.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Mini-Project C: Optimizer Olympics {#project-c}\n",
    "\n",
    "### ðŸŽ¯ Goal\n",
    "\n",
    "**Implement** SGD with Momentum and Adam optimizers from scratch, then race them on the spiral dataset and a challenging 2D function.\n",
    "\n",
    "**Skills reused:** Gradient descent (Session 5), MLP backprop (Session 6), training pipeline (Project A).  \n",
    "**New skills:** Momentum, adaptive learning rates, optimizer abstraction.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1 â€” Refactor the MLP for External Optimizers\n",
    "\n",
    "**Task:** Modify the MLP to **return gradients** instead of applying updates internally. This decouples the model from the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce464a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexMLP:\n",
    "    \"\"\"MLP that returns gradients â€” optimizer is external.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_classes, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.W1 = np.random.randn(n_hidden, n_input) * np.sqrt(2.0 / n_input)\n",
    "        self.b1 = np.zeros((n_hidden, 1))\n",
    "        self.W2 = np.random.randn(n_classes, n_hidden) * np.sqrt(2.0 / n_hidden)\n",
    "        self.b2 = np.zeros((n_classes, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z1 = self.W1 @ X + self.b1\n",
    "        self.a1 = np.maximum(0, self.z1)\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def loss(self, y_true):\n",
    "        return categorical_cross_entropy(y_true, self.a2)\n",
    "    \n",
    "    def compute_gradients(self, y_true):\n",
    "        \"\"\"\n",
    "        Compute and RETURN gradients â€” do NOT update any weights.\n",
    "        \n",
    "        Returns: [dW1, db1, dW2, db2] â€” same order as self.params\n",
    "        \"\"\"\n",
    "        N = self.X.shape[1]\n",
    "        \n",
    "        # TODO: Backprop math from Session 6 â€” but return gradients instead of updating\n",
    "        delta2 = ___\n",
    "        dW2 = ___\n",
    "        db2 = ___\n",
    "        delta1 = ___\n",
    "        dW1 = ___\n",
    "        db1 = ___\n",
    "        \n",
    "        return [dW1, db1, dW2, db2]\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"Parameter list â€” same order as compute_gradients returns.\"\"\"\n",
    "        return [self.W1, self.b1, self.W2, self.b2]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.a2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bdcd0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59281b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def compute_gradients(self, y_true):\n",
    "    N = self.X.shape[1]\n",
    "    delta2 = (self.a2 - y_true) / N\n",
    "    dW2 = delta2 @ self.a1.T\n",
    "    db2 = np.sum(delta2, axis=1, keepdims=True)\n",
    "    delta1 = (self.W2.T @ delta2) * (self.z1 > 0).astype(float)\n",
    "    dW1 = delta1 @ self.X.T\n",
    "    db1 = np.sum(delta1, axis=1, keepdims=True)\n",
    "    return [dW1, db1, dW2, db2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43594ebd",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 2 â€” Implement Three Optimizers\n",
    "\n",
    "**Vanilla SGD** is given as a reference. **You implement Momentum and Adam.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Vanilla SGD â€” given as reference.\"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"params and grads: lists of arrays, same length and order.\"\"\"\n",
    "        for p, g in zip(params, grads):\n",
    "            p -= self.lr * g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1fa46",
   "metadata": {},
   "source": [
    "**Task:** Implement SGD with Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0531c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum:\n",
    "    \"\"\"\n",
    "    SGD with momentum.\n",
    "    \n",
    "    Formulas:\n",
    "        v_i â† beta * v_i + (1 - beta) * g_i\n",
    "        p_i â† p_i - lr * v_i\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.velocities = None   # Will be initialized on first call\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        # Initialize velocities to zeros on first call\n",
    "        if self.velocities is None:\n",
    "            self.velocities = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        for i, (p, g) in enumerate(zip(params, grads)):\n",
    "            # TODO: Update velocity (exponential moving average of gradients)\n",
    "            self.velocities[i] = ___\n",
    "            \n",
    "            # TODO: Update parameter using velocity\n",
    "            p -= ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6195c4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” Momentum</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acc045",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for i, (p, g) in enumerate(zip(params, grads)):\n",
    "    self.velocities[i] = self.beta * self.velocities[i] + (1 - self.beta) * g\n",
    "    p -= self.lr * self.velocities[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542459a7",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "**Task:** Implement Adam. This is harder â€” you need first moments, second moments, bias correction, and a time step counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aed6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam optimizer.\n",
    "    \n",
    "    Formulas:\n",
    "        m_i â† Î²â‚ * m_i + (1 - Î²â‚) * g_i            (first moment)\n",
    "        v_i â† Î²â‚‚ * v_i + (1 - Î²â‚‚) * g_iÂ²            (second moment)\n",
    "        mÌ‚_i = m_i / (1 - Î²â‚^t)                       (bias correction)\n",
    "        vÌ‚_i = v_i / (1 - Î²â‚‚^t)                       (bias correction)\n",
    "        p_i â† p_i - lr * mÌ‚_i / (âˆšvÌ‚_i + Îµ)\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None      # First moment estimates\n",
    "        self.v = None      # Second moment estimates\n",
    "        self.t = 0         # Time step counter\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        for i, (p, g) in enumerate(zip(params, grads)):\n",
    "            # TODO: Update biased first moment estimate (m)\n",
    "            self.m[i] = ___\n",
    "            \n",
    "            # TODO: Update biased second moment estimate (v)\n",
    "            # Note: element-wise square of gradient\n",
    "            self.v[i] = ___\n",
    "            \n",
    "            # TODO: Compute bias-corrected estimates (m_hat, v_hat)\n",
    "            m_hat = ___\n",
    "            v_hat = ___\n",
    "            \n",
    "            # TODO: Update parameter\n",
    "            p -= ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10b58c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution â€” Adam</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5521f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for i, (p, g) in enumerate(zip(params, grads)):\n",
    "    self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "    self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g ** 2)\n",
    "    \n",
    "    m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "    v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "    \n",
    "    p -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee05af",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 3 â€” Race on the Spiral Dataset\n",
    "\n",
    "**Task:** Write the training loop that trains the **same architecture** with each optimizer. Each optimizer gets a **fresh model from the same seed** for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all_oh, y_all_lbl = generate_spiral(n_per_class=150, noise=0.25)\n",
    "X_tr, y_tr, X_va, y_va, X_te, y_te = train_val_test_split(X_all, y_all_oh)\n",
    "y_te_l = np.argmax(y_te, axis=0)\n",
    "\n",
    "# TODO: Define three optimizer instances\n",
    "# SGD:      lr=1.0\n",
    "# Momentum: lr=1.0, beta=0.9\n",
    "# Adam:     lr=0.01  (Adam typically uses smaller lr)\n",
    "optimizer_configs = [\n",
    "    (\"SGD (lr=1.0)\",      ___),\n",
    "    (\"Momentum (lr=1.0)\", ___),\n",
    "    (\"Adam (lr=0.01)\",    ___),\n",
    "]\n",
    "\n",
    "race_results = {}\n",
    "\n",
    "for name, optimizer in optimizer_configs:\n",
    "    # TODO:\n",
    "    # 1. Create a FRESH FlexMLP (seed=42 for all â€” fair comparison)\n",
    "    # 2. Train for 3000 epochs:\n",
    "    #    a. Forward pass\n",
    "    #    b. Record train loss\n",
    "    #    c. Compute gradients (NOT weight update!)\n",
    "    #    d. optimizer.step(params, grads) â€” this applies the update\n",
    "    #    e. Forward on val, record val loss\n",
    "    # 3. Compute test accuracy\n",
    "    # 4. Store results\n",
    "\n",
    "    model = ___\n",
    "    t_hist, v_hist = [], []\n",
    "    \n",
    "    for epoch in range(3000):\n",
    "        ___\n",
    "    \n",
    "    test_acc = ___\n",
    "    race_results[name] = {\"train\": t_hist, \"val\": v_hist, \"test_acc\": test_acc}\n",
    "    print(f\"{name:>25s}: Test Acc = {test_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef11aa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae620512",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "optimizer_configs = [\n",
    "    (\"SGD (lr=1.0)\",      SGD(lr=1.0)),\n",
    "    (\"Momentum (lr=1.0)\", SGDMomentum(lr=1.0, beta=0.9)),\n",
    "    (\"Adam (lr=0.01)\",    Adam(lr=0.01)),\n",
    "]\n",
    "\n",
    "race_results = {}\n",
    "\n",
    "for name, optimizer in optimizer_configs:\n",
    "    model = FlexMLP(n_input=2, n_hidden=50, n_classes=3, seed=42)\n",
    "    t_hist, v_hist = [], []\n",
    "    \n",
    "    for epoch in range(3000):\n",
    "        model.forward(X_tr)\n",
    "        t_hist.append(model.loss(y_tr))\n",
    "        grads = model.compute_gradients(y_tr)\n",
    "        optimizer.step(model.params, grads)\n",
    "        \n",
    "        model.forward(X_va)\n",
    "        v_hist.append(model.loss(y_va))\n",
    "    \n",
    "    test_acc = np.mean(model.predict(X_te) == y_te_l) * 100\n",
    "    race_results[name] = {\"train\": t_hist, \"val\": v_hist, \"test_acc\": test_acc}\n",
    "    print(f\"{name:>25s}: Test Acc = {test_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ceb23a",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 4 â€” Visualize the Race\n",
    "\n",
    "**Task:** Create a 1Ã—3 figure:\n",
    "1. All training losses overlaid (compare convergence speed)\n",
    "2. Zoom on first 500 epochs (where differences are most visible)\n",
    "3. Bar chart of final test accuracy (with value labels on each bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dedd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# TODO: Panel 1 â€” all training losses, full range\n",
    "ax = axes[0]\n",
    "___\n",
    "\n",
    "# TODO: Panel 2 â€” zoom first 500 epochs\n",
    "ax = axes[1]\n",
    "___\n",
    "\n",
    "# TODO: Panel 3 â€” bar chart with accuracy labels above each bar\n",
    "ax = axes[2]\n",
    "___\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39d584",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06a92b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Panel 1: Full range\n",
    "ax = axes[0]\n",
    "for (name, res), c in zip(race_results.items(), colors):\n",
    "    ax.plot(res[\"train\"], label=name, linewidth=2, color=c, alpha=0.8)\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Train Loss', fontsize=14)\n",
    "ax.set_title('Training Convergence', fontsize=16)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Zoom\n",
    "ax = axes[1]\n",
    "for (name, res), c in zip(race_results.items(), colors):\n",
    "    ax.plot(res[\"train\"][:500], label=name, linewidth=2, color=c, alpha=0.8)\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Train Loss', fontsize=14)\n",
    "ax.set_title('First 500 Epochs', fontsize=16)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Bar chart\n",
    "ax = axes[2]\n",
    "names = list(race_results.keys())\n",
    "accs = [race_results[n][\"test_acc\"] for n in names]\n",
    "bars = ax.bar(range(len(names)), accs, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels([n.split('(')[0].strip() for n in names], fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=14)\n",
    "ax.set_title('Final Test Accuracy', fontsize=16)\n",
    "ax.set_ylim(0, 100)\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{acc:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1f400",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 5 â€” The Gauntlet: Rosenbrock Function\n",
    "\n",
    "The Rosenbrock function has a narrow curved valley â€” vanilla SGD struggles while momentum and Adam navigate it well. This makes the difference between optimizers dramatically visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ffc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(w1, w2):\n",
    "    \"\"\"Famous test function: minimum at (1, 1), narrow curved valley.\"\"\"\n",
    "    return (1 - w1) ** 2 + 100 * (w2 - w1 ** 2) ** 2\n",
    "\n",
    "def rosenbrock_grad(w1, w2):\n",
    "    \"\"\"Analytical gradient of Rosenbrock.\"\"\"\n",
    "    dw1 = -2 * (1 - w1) + 200 * (w2 - w1 ** 2) * (-2 * w1)\n",
    "    dw2 = 200 * (w2 - w1 ** 2)\n",
    "    return np.array([dw1]), np.array([dw2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1587c9",
   "metadata": {},
   "source": [
    "**Task:** Run each optimizer for 2000 steps starting from $(-1, -1)$, record the trajectory, then plot all three on a contour plot of the function.\n",
    "\n",
    "Learning rates: SGD â†’ 0.001, Momentum â†’ 0.001, Adam â†’ 0.05  \n",
    "(Adam can use a much larger lr because it normalizes by gradient variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = (-1.0, -1.0)\n",
    "trajectories = {}\n",
    "\n",
    "for name, opt in [(\"SGD\", SGD(lr=0.001)),\n",
    "                   (\"Momentum\", SGDMomentum(lr=0.001, beta=0.9)),\n",
    "                   (\"Adam\", Adam(lr=0.05))]:\n",
    "    w1, w2 = np.array([start[0]]), np.array([start[1]])\n",
    "    path = [(w1[0], w2[0])]\n",
    "    \n",
    "    for _ in range(2000):\n",
    "        # TODO: Compute gradient and take one optimizer step\n",
    "        # Hint: rosenbrock_grad returns (dw1, dw2) as arrays\n",
    "        # Then opt.step([w1, w2], [dw1, dw2])\n",
    "        ___\n",
    "        \n",
    "        path.append((w1[0], w2[0]))\n",
    "    \n",
    "    trajectories[name] = path\n",
    "    print(f\"{name:>10s}: final = ({w1[0]:.4f}, {w2[0]:.4f}), \"\n",
    "          f\"loss = {rosenbrock(w1[0], w2[0]):.6f}\")\n",
    "\n",
    "# TODO: Create contour plot with all three trajectories\n",
    "# - Compute Z = rosenbrock(W1g, W2g) on a meshgrid\n",
    "# - Use ax.contour with np.log(Z + 1) for level spacing\n",
    "# - Mark minimum (1,1) with a gold star, start (-1,-1) with a black X\n",
    "# - Draw each trajectory as a colored line\n",
    "# - Add legend, labels, title\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "___\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100280f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c20f3d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for name, opt in [(\"SGD\", SGD(lr=0.001)),\n",
    "                   (\"Momentum\", SGDMomentum(lr=0.001, beta=0.9)),\n",
    "                   (\"Adam\", Adam(lr=0.05))]:\n",
    "    w1, w2 = np.array([start[0]]), np.array([start[1]])\n",
    "    path = [(w1[0], w2[0])]\n",
    "    \n",
    "    for _ in range(2000):\n",
    "        g1, g2 = rosenbrock_grad(w1[0], w2[0])\n",
    "        opt.step([w1, w2], [g1, g2])\n",
    "        path.append((w1[0], w2[0]))\n",
    "    \n",
    "    trajectories[name] = path\n",
    "    print(f\"{name:>10s}: final = ({w1[0]:.4f}, {w2[0]:.4f}), \"\n",
    "          f\"loss = {rosenbrock(w1[0], w2[0]):.6f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "w1_range = np.linspace(-2, 2, 300)\n",
    "w2_range = np.linspace(-1.5, 3, 300)\n",
    "W1g, W2g = np.meshgrid(w1_range, w2_range)\n",
    "Z = rosenbrock(W1g, W2g)\n",
    "\n",
    "ax.contour(W1g, W2g, np.log(Z + 1), levels=30, cmap='viridis', alpha=0.5)\n",
    "ax.scatter(1, 1, color='gold', s=200, marker='*', zorder=10, label='Minimum (1,1)')\n",
    "ax.scatter(start[0], start[1], color='black', s=100, marker='x', zorder=10, label='Start')\n",
    "\n",
    "for (name, path), c in zip(trajectories.items(), colors):\n",
    "    xs, ys = zip(*path)\n",
    "    ax.plot(xs, ys, '-', linewidth=1.5, color=c, alpha=0.8, label=name)\n",
    "    ax.scatter(xs[-1], ys[-1], color=c, s=80, edgecolors='black', zorder=8)\n",
    "\n",
    "ax.set_xlabel('$w_1$', fontsize=14)\n",
    "ax.set_ylabel('$w_2$', fontsize=14)\n",
    "ax.set_title('Optimizer Trajectories on Rosenbrock Function', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07ad68",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "### Phase 6 â€” Analysis Report\n",
    "\n",
    "Answer in your notebook:\n",
    "\n",
    "1. **Spiral dataset:** Which optimizer converged fastest (by epoch)? Which reached the best test accuracy?\n",
    "2. **Rosenbrock:** Why does vanilla SGD barely move in 2000 steps? What does momentum do differently?\n",
    "3. **Adam's trade-off:** Adam navigated the valley faster. But compare final loss values â€” which optimizer got closest to the true minimum $(1, 1)$?\n",
    "4. **Practical takeaway:** You're starting a new project tomorrow. Which optimizer do you pick first, and with what learning rate?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "âœ… **Generalization**: Training accuracy â‰  real-world performance  \n",
    "âœ… **Train/Val/Test splits**: The honest way to evaluate models  \n",
    "âœ… **Learning curves**: The diagnostic tool for overfitting and underfitting  \n",
    "âœ… **Early stopping**: Stop when validation loss increases, restore best weights  \n",
    "âœ… **L2 regularization**: Penalize large weights â†’ smoother boundaries  \n",
    "âœ… **Dropout**: Randomly disable neurons â†’ implicit ensemble  \n",
    "âœ… **Modern optimizers**: Momentum smooths oscillations, Adam adapts per-weight\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **The overfitting recipe:**\n",
    "   - Big model + small dataset + long training = memorization\n",
    "   - Detect with learning curves (train-val gap)\n",
    "   - Cure with: early stopping, regularization, more data, simpler model\n",
    "\n",
    "2. **Regularization works by constraining complexity:**\n",
    "   - L2 keeps weights small â†’ smooth decision boundaries\n",
    "   - Dropout prevents co-adaptation â†’ robust features\n",
    "   - They can be combined for stronger effect\n",
    "\n",
    "3. **Optimizer choice matters:**\n",
    "   - Adam is the safe default (fast, adaptive, forgiving)\n",
    "   - SGD + Momentum can generalize better with careful tuning\n",
    "   - Always compare on a validation set\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Session 9: PyTorch Introduction**\n",
    "\n",
    "In the next session, we'll learn:\n",
    "- **Tensors & autograd**: Automatic differentiation (no more manual backprop!)\n",
    "- **nn.Module**: Build networks declaratively\n",
    "- **Training loop**: Optimizers, loss functions, and datasets â€” the PyTorch way\n",
    "- **Rebuild**: Reimplement our MLP in PyTorch and compare\n",
    "\n",
    "**The goal:** Transition from \"understanding the math\" to \"using professional tools\"!\n",
    "\n",
    "### Before Next Session\n",
    "\n",
    "**Think about:**\n",
    "1. We manually implemented backpropagation, gradient descent, L2, dropout, momentum, and Adam. What parts were tedious and error-prone?\n",
    "2. If a library could handle gradients automatically, what would you still need to implement yourself?\n",
    "3. Install PyTorch: `pip install torch` (or see https://pytorch.org)\n",
    "\n",
    "**Optional reading:**\n",
    "- PyTorch \"60 Minute Blitz\" tutorial: https://pytorch.org/tutorials/\n",
    "- \"Why Momentum Really Works\" â€” distill.pub\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 8** ðŸŽ“\n",
    "\n",
    "**You now understand:**\n",
    "- âœ… How to detect and diagnose overfitting\n",
    "- âœ… How regularization prevents memorization\n",
    "- âœ… How modern optimizers improve training\n",
    "\n",
    "**Next up:** PyTorch â€” letting the framework do the heavy lifting! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
